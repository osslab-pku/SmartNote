Changes between 8.0.3 and 8.1.0
================================================================================

COMMITS
--------------------------------------------------------------------------------

Commit: b3d255522c042b579e8c107aa9afe25fd4bd6a68
Author: Vlad Ilyushchenko
Date: 2024-07-16 17:10:46+00:00
Message: build: 8.0.3 (#4786)
----------------------------------------

Commit: e8463fbf4cd88a0c02293be9104178b5ea6c54fd
Author: tris0laris
Date: 2024-07-17 13:42:51+00:00
Message: docs(core): update readme paragraph (#4784)

Co-authored-by: goodroot <9484709+goodroot@users.noreply.github.com>
----------------------------------------

Commit: 9266cba92f4c7ddd54c70659fd0acdd684d0e062
Author: Vlad Ilyushchenko
Date: 2024-07-17 19:09:22+00:00
Message: fix(sql): fix spurious "unsupported operation" errors (#4632)
----------------------------------------

Commit: 1b354ee76fa87c01880b61273022b13acfe638da
Author: Jaromir Hamala
Date: 2024-07-17 19:10:58+00:00
Message: test(core): renable dedup insert fuzz test (#4780)
----------------------------------------

Commit: 4a7763e890e8fe4f031f03cbd6b25fde1b5136b1
Author: Steven Sklar
Date: 2024-07-17 19:13:58+00:00
Message: ci(build): add redhat ubi docker build (#4783)
----------------------------------------

Commit: 5736f6411f1fb424d2457bc1c0144f782a1f1efc
Author: javier ram√≠rez
Date: 2024-07-17 19:14:30+00:00
Message: docs(ilp): changed examples to use trades table (#4764)
----------------------------------------

Commit: 243b95888b46ba3ff6960ed0b56d18201a5e047a
Author: Andrei Pechkurov
Date: 2024-07-17 19:19:12+00:00
Message: chore(sql): hide pre-touch magic number shutdown message (#4767)
----------------------------------------

Commit: b4e55c3b5a7fc8996c7d90f505eec632e498b771
Author: Adam Cimarosti
Date: 2024-07-18 10:14:20+00:00
Message: chore(core): Java code formatting checks with IntelliJ (#4774)
----------------------------------------

Commit: 26b844d1b1b6925a0e83ef5f7b23d291bbc701bf
Author: Alex Pelagenko
Date: 2024-07-19 12:11:15+00:00
Message: perf(core): speedup small transaction writing 50-100% (#4793)
----------------------------------------

Commit: 1b0ed4c3863b81bb989bc03da6a48827d5d0b98f
Author: Steven Sklar
Date: 2024-07-19 13:28:39+00:00
Message: ci(build): fix rhel Docker image build (#4791)
----------------------------------------

Commit: f721343fb3244ae47b1f1925009c906a7e7f91b6
Author: Steven Sklar
Date: 2024-07-19 13:29:31+00:00
Message: ci(build): shrink the size of the QuestDB AMI EBS volume mount (#4795)
----------------------------------------

Commit: 60b30887f162e6e7e8e9902e432142a6a2f318b1
Author: Andrei Pechkurov
Date: 2024-07-19 13:31:23+00:00
Message: fix(sql): fix wrong results returned from parallel WHERE and GROUP BY for some function keys (#4796)
----------------------------------------

Commit: fd3fe645190098e1b36fabbef2ba6340e250d8e6
Author: Andrei Pechkurov
Date: 2024-07-19 13:33:55+00:00
Message: fix(sql): incorrect results returned from parallel GROUP BY with single varchar function key (#4798)
----------------------------------------

Commit: 4b3bb0f3b4dc7f7335e1c75d6663beb20d6330e9
Author: Jaromir Hamala
Date: 2024-07-19 15:35:12+00:00
Message: fix(core): avoid table suspension when under memory pressure (#4745)
----------------------------------------

Commit: 9ee0380e07d91e669093ba6de371be4f3ccdd64f
Author: Andrei Pechkurov
Date: 2024-07-19 16:03:34+00:00
Message: perf(sql): speed up like/ilike operator on symbol column (#4794)
----------------------------------------

Commit: 3ec640a52dc51f745af8767b084dc6fadeb6deb9
Author: Andrei Pechkurov
Date: 2024-07-19 17:59:15+00:00
Message: perf(sql): speed up regexp_replace(varchar) for simple patterns (#4668)
----------------------------------------

Commit: 0557331f592e9c0f627dd341cfbba4acd9626711
Author: Jaromir Hamala
Date: 2024-07-23 09:54:45+00:00
Message: fix(sql): some window functions might double count rows (#4804)
----------------------------------------

Commit: 1f770d0cd47be092c860e646f36310cabba78187
Author: Eugene Lysiuchenko
Date: 2024-07-23 12:02:34+00:00
Message: feat(sql): read_parquet() function (#4460)

Co-authored-by: Marko Topolnik <marko.topolnik@gmail.com>
Co-authored-by: ideoma <2159629+ideoma@users.noreply.github.com>
Co-authored-by: Andrey Pechkurov <apechkurov@gmail.com>
----------------------------------------

Commit: 8b4f6cf727481a6e8dfc5725b3847ee2ed76b931
Author: Nick Woolmer
Date: 2024-07-23 15:02:08+00:00
Message: feat(sql): introduce SAMPLE BY FROM-TO syntax for specifying result ranges (#4733)
----------------------------------------

Commit: 0b0ddfce712cfc98a0aac2c165482c461cb13033
Author: Alex Pelagenko
Date: 2024-07-23 17:46:26+00:00
Message: chore(build): fix sqllogictest binary load to make it work where questdb is a submodule (#4809)
----------------------------------------

Commit: 350f623774a18cbb1574e64f6019268e0e29caf4
Author: jchrys
Date: 2024-07-24 09:08:11+00:00
Message: chore(core): `DirectString#_subSequence` method (#4808)
----------------------------------------

Commit: e384be3b11b8661ae11074932655647321491174
Author: Jaromir Hamala
Date: 2024-07-24 14:07:10+00:00
Message: feat(core): trigger file to initiate snapshot recovery (#4807)
----------------------------------------

Commit: 07ba2e4ad83b32b2fa839398797b9d36c4d5e94d
Author: Vlad Ilyushchenko
Date: 2024-07-24 15:45:46+00:00
Message: fix(ui): fix Charts when using together with OAuth (#4813)
----------------------------------------

Commit: 412f81b337a88478751ce94c95504a6b4b67c29b
Author: Vlad Ilyushchenko
Date: 2024-07-24 15:55:21+00:00
Message: [maven-release-plugin] prepare release 8.1.0
----------------------------------------


PULL REQUESTS
--------------------------------------------------------------------------------

PR #4813: fix(ui): fix Charts when using together with OAuth
Author: bluestreak01
Merged at: 2024-07-24 15:45:46+00:00
URL: https://github.com/questdb/questdb/pull/4813
Description:
updating web console that brings this fix: https://github.com/questdb/ui/pull/310
----------------------------------------

PR #4807: feat(core): trigger file to initiate snapshot recovery
Author: jerrinot
Merged at: 2024-07-24 14:07:10+00:00
URL: https://github.com/questdb/questdb/pull/4807
Description:
Previously, snapshot recovery was driven by the Cairo Snapshot Instance ID property, which had no default value and was generally confusing to use. Snapshot recovery is now triggered by the existence of a trigger file in the QuestDB root directory: If a file named `root/_restore` exists and a snapshot is available, it will be restored on startup.

The old strategy with Cairo Snapshot Instance IDs is left of backwards compatibility, but it's not recommended. 

Additionally, a failure during snapshot recovery aborts QuestDB start. This is technically a breaking change - since before it would just log a failure and continue.  However, a failure during recovery means the system state is undefined and should not be relied on.

TODO:
- [x] fail-fast when a trigger file exists, but there is no snapshot
- [x] warning when cairo id is set - it has no effect and have to communicate this to a user

Docs PR: https://github.com/questdb/documentation/pull/26
----------------------------------------

PR #4808: chore(core): `DirectString#_subSequence` method
Author: jchrys
Merged at: 2024-07-24 09:08:11+00:00
URL: https://github.com/questdb/questdb/pull/4808
Description:
Modified the `_subSequence` to correct the calculation of the `lo`, `hi`, and `len`.
----------------------------------------

PR #4809: chore(build): fix sqllogictest binary load to make it work where questdb is a submodule
Author: ideoma
Merged at: 2024-07-23 17:46:26+00:00
URL: https://github.com/questdb/questdb/pull/4809
Description:
None
----------------------------------------

PR #4733: feat(sql): introduce SAMPLE BY FROM-TO syntax for specifying result ranges
Author: nwoolmer
Merged at: 2024-07-23 15:02:08+00:00
URL: https://github.com/questdb/questdb/pull/4733
Description:
Relies on https://github.com/questdb/questdb/pull/4692

Closes https://github.com/questdb/questdb/issues/3989
Closes https://github.com/questdb/questdb/issues/1417
Closes https://github.com/questdb/questdb/issues/3764

## SAMPLE BY FROM ... TO ...

We will publish more information on this feature soon.

### Headlines

- New syntax for `SAMPLE BY` for specifying output result ranges. 
- Parallel `SAMPLE BY` now supports value fills (`FILL(null)`, `FILL(42)` etc.)
- Bugfix for interval calculations when aligning intervals to calendar.

### Limitations

- This is only for non-keyed SAMPLE BY queries. This means that you may only have designated timestamp and aggregate columns.
- This is only for `FILL(NONE)`, `FILL(VALUE)` and `FILL(NULL)`. The intended behaviour for `FILL(LINEAR)` and `FILL(PREV)` is yet to be determined.
- **Parallel** SAMPLE BY with FROM-TO syntax will not support bind variables, only constant valued FROM-TO.

### FROM-TO syntax

This PR introduces new syntax for SAMPLE BY statements. This syntax is for specifying the output range for the query, which will allow for filling data on either end of a result set. This also defines a lower bound for the data.

Example:

```sql
SAMPLE BY '5d' FROM '2018-01-01' TO '2019-01-01' 
-- fill
-- alignment
-- etc.
```
In simple terms, `WHERE` defines what data will be sampled, and `FROM` defines what shape the output data will have.

### timestamp_floor

The motivation for this change is to fix an issue with how calendar alignment is calculated. When aligning to calendar, the provided stride is rounded with alignment to the unix epoch. Unfortunately, this can give unexpected results when used for intervals.

```sql
select timestamp_floor('5d', '2018-01-01') -- gives '2017-12-30T00:00:00.000000Z'
```

So if somebody runs a query such as this:

```sql
select timestamp_floor('5d', pickup_datetime) t, count 
from trips where pickup_datetime in '2018'
order by 1;
```

You can see that the dataset starts in 2017 - which is not what the user meant!

![image](https://github.com/user-attachments/assets/2820f30f-5fbd-4ec4-b566-63cc73964e4d)

This PR adds a third parameter to `timestamp_floor(sNn)`, which is `offset`, giving an epoch other than `1970-01-01` to calculate the buckets from.

Thus, this query calculates the correct bucket:

```sql
select timestamp_floor('5d', '2018-01-01', '2018-01-01') -- gives '2018-01-01T00:00:00.000000Z'
```

The `FROM` clause has the same function for `SAMPLE BY`. Even if no fill is provided, the clause will be used to orient the buckets.

### With filling

This syntax allows the user to fill outside the data range. For example, this dataset starts at `2018-01-01`.

![image](https://github.com/user-attachments/assets/b0edd5e0-6ea1-4e4c-b44a-e553994f0639)


### Optimising FROM-TO to use interval scans

This PR also includes an optimisation step, to improve `SAMPLE BY` performance when a `WHERE` clause is not provided.

If you have an expression such as this:

```sql
SAMPLE BY '5d' FROM '2018-01-01' TO '2019-01-01' 
```
and the query does not include a `WHERE` clause relating to the designated timestamp, then the `FROM-TO` range will be copied to the `WHERE` clause.

```sql
WHERE ts >= '2018-01-01' AND ts < '2019-01-01'
SAMPLE BY '5d' FROM '2018-01-01' TO '2019-01-01' 
```

This minimises the scanning of data not relevant to the query.



----------------------------------------

PR #4460: feat(sql): read_parquet() function
Author: eugenels
Merged at: 2024-07-23 12:02:35+00:00
URL: https://github.com/questdb/questdb/pull/4460
Description:
Integration of parquet file format reading and writing into QuestDB natively.

This PR includes

- new SQL function `read_parquet('<path to parquet file>')` to query the parquet file located on the server file system in the directory configured under `sql.copy.input.root=`

```SQL
select * from read_parquet('file_name.parquet')
LIMIT 100
```

- experimental SQL to convert table partitions to parquet format `ALTER table table_name CONVERT PARTITION TO PARQUET LIST '2024-07-01'`. Note that this SQL is not ready to be documented since there is no way to read the table with partitions converted to parquet implemented yet.

Also some infrastructure changes:

- Rust libraries to serialize/deserialize parquet format.
- Sqllogictest tests and Rust JNI runner
- Split github actions to separately build CXX, Rust and Rust Test code.
----------------------------------------

PR #4804: fix(sql): some window functions might double count rows
Author: jerrinot
Merged at: 2024-07-23 09:54:45+00:00
URL: https://github.com/questdb/questdb/pull/4804
Description:
This is reproducible when using a negative limit and querying a table without a designated timestamp. then `buildRecordChain()` is called twice: once in `calculateSize()` and then is `hasNext()`

Fixes https://github.com/questdb/questdb/issues/4748

TODO:
- [x] test
----------------------------------------

PR #4668: perf(sql): speed up regexp_replace(varchar) for simple patterns
Author: puzpuzpuz
Merged at: 2024-07-19 17:59:15+00:00
URL: https://github.com/questdb/questdb/pull/4668
Description:
Introduces a new `regexp_replace(varchar)` function class that skips UTF-8 to UTF-16 decoding for patterns that can operate on a UTF-16 view of a UTF-8 string, i.e. only care about ASCII characters present in the varchar argument. With this patch, we fully regain what we have for `regexp_replace(string)`. Mwha-ha-ha-ha (evil laugh)

```sql
-- master 10.75s
-- patch 8.22s
SELECT * FROM (SELECT REGEXP_REPLACE(Referer, '^https?://(?:www\.)?([^/]+)/.*$', '$1') AS k, AVG(length(Referer)) AS l, COUNT(*) AS c, MIN(Referer) FROM hits WHERE Referer IS NOT NULL GROUP BY k) WHERE c > 100000 ORDER BY l DESC LIMIT 25;
```
----------------------------------------

PR #4794: perf(sql): speed up like/ilike operator on symbol column
Author: puzpuzpuz
Merged at: 2024-07-19 16:03:34+00:00
URL: https://github.com/questdb/questdb/pull/4794
Description:
Closes #4790

Also includes the following:
* Fixes operator hard-coded precedences
* Fixes true returned for null values for like/ilike % patterns
* Moves tests from `LikeFunctionFactoryTest` and `ILikeFunctionFactoryTest` into per-column type tests
----------------------------------------

PR #4745: fix(core): avoid table suspension when under memory pressure
Author: jerrinot
Merged at: 2024-07-19 15:35:12+00:00
URL: https://github.com/questdb/questdb/pull/4745
Description:
The WAL Apply job may fail when QuestDB is under memory pressure. If that happens, a table is suspended, requiring user intervention - a highly undesirable situation. 

These changes aim to reduce the frequency of tables being suspended due to memory pressure. When memory pressure is detected while applying data from the WAL, the system retries with reduced internal parallelism instead of suspending the table. There are multiple retry attempts, each with further reduced parallelism. If no progress can be made, even with processing one partition at a time, there are five rounds of backoff - in the hope that the memory pressure will decrease, for example, when a demanding query finishes. If this still does not resolve the issue, the table is suspended.

The concept of "memory pressure" is introduced. It's a per-table value ranging from 0 (no memory pressure at all) to 10 (maximum memory pressure). The SQL function wal_tables() now includes the memory pressure level.

TODO: 
- [x] Fuzz tests
- [x] Perf. regression tests
----------------------------------------

PR #4798: fix(sql): incorrect results returned from parallel GROUP BY with single varchar function key
Author: puzpuzpuz
Merged at: 2024-07-19 13:33:55+00:00
URL: https://github.com/questdb/questdb/pull/4798
Description:
`UnorderedVarcharMapRecord#copyToKey()` was copying pointers from another `UnorderedVarcharMap`, even unstable ones.

Also, reduces memory overhead for such queries by getting rid of redundant key allocations.
----------------------------------------

PR #4796: fix(sql): fix wrong results returned from parallel WHERE and GROUP BY for some function keys
Author: puzpuzpuz
Merged at: 2024-07-19 13:31:23+00:00
URL: https://github.com/questdb/questdb/pull/4796
Description:
The list of problematic SQL functions:
* `to_str()` for timestamp and date
* `lpad()`
* `rpad()`
* `left()`
* `right()`
* `split_part()`
* `to_char()`
* `touch()`
----------------------------------------

PR #4795: ci(build): shrink the size of the QuestDB AMI EBS volume mount
Author: sklarsa
Merged at: 2024-07-19 13:29:31+00:00
URL: https://github.com/questdb/questdb/pull/4795
Description:
None
----------------------------------------

PR #4791: ci(build): fix rhel Docker image build
Author: sklarsa
Merged at: 2024-07-19 13:28:39+00:00
URL: https://github.com/questdb/questdb/pull/4791
Description:
Missed a flag (and an architecture) in my previous command because I was using podman locally. Tested the exact command using docker and it works.

Also overrides 2 additional image tags that were written in the base image by redhat

I tested the docker build (without the `--push` flag) on an x86 ec2 instance 

fixes https://github.com/questdb/questdb/pull/4783


----------------------------------------

PR #4793: perf(core): speedup small transaction writing 50-100%
Author: ideoma
Merged at: 2024-07-19 12:11:15+00:00
URL: https://github.com/questdb/questdb/pull/4793
Description:
Every transaction written to WAL table has a fixed time needed to apply to the table columns. For small 1 line transactions this fixed overhead can make a big impact on overall throughput.

This PR reduces the time spent to apply a transaction and moves overall throughput from 3k transactions per second to 5k transactions per second in a simulated benchmark.

The changes are:
- avoid mmap/munmp WAL column files when data is copied using FDs (no mixed IO)
- optimise batching of rows in the last partition LAG area to reduce small, frequent full commits

Also, this PR reduces the logging by not logging full QuestDB paths to the files but only relative paths to the DB root directory.

----------------------------------------

PR #4774: chore(core): Java code formatting checks with IntelliJ
Author: amunra
Merged at: 2024-07-18 10:14:20+00:00
URL: https://github.com/questdb/questdb/pull/4774
Description:
# Change
Introduced Java auto-code formatting checks in CI.

# Details
* Introduced a new CI job which downloads/installs/runs the IntelliJ IDEA Community Edition code formatter according to our pre-existing project settings.
* Formatted all Java code to pass CI.

# Testing
After setting up the command, testing has been done by formatting around 20 files in the IDE and observing that CI no longer signaled them as badly formatted.
This verifies that the code formatting rules used for the command line match those used by the IDE.

# Caveats
A few files (namely, those related to histogram functionality) have been edited manually.
This is because the code formatter behaves oddly with two consecutive JavaDoc style sections and aims to merge them, corrupting the banner section formatting.

The workaround has been to convert the section following the banner to (`//`) comments.
----------------------------------------

PR #4767: chore(sql): hide pre-touch magic number shutdown message
Author: puzpuzpuz
Merged at: 2024-07-17 19:19:12+00:00
URL: https://github.com/questdb/questdb/pull/4767
Description:
None
----------------------------------------

PR #4764: docs(ilp): changed examples to use trades table
Author: javier
Merged at: 2024-07-17 19:14:30+00:00
URL: https://github.com/questdb/questdb/pull/4764
Description:
Changed examples to use trades data, rather than sensors. These examples are pulled directly to the docs
----------------------------------------

PR #4783: ci(build): add redhat ubi docker build
Author: sklarsa
Merged at: 2024-07-17 19:13:58+00:00
URL: https://github.com/questdb/questdb/pull/4783
Description:
Adds docker builds based on the [redhat ubi](https://www.redhat.com/en/blog/introducing-red-hat-universal-base-image) to become compliant with [preflight checks](https://github.com/redhat-openshift-ecosystem/openshift-preflight) for RedHat certification.

One of the requirements is adding a license file to `/licenses` inside the image. Since I didn't want to expand our Docker build context (for build speed optimization), and we can't add files to an image from outside the context (not even symlinked), I copied the license file to the build context root (`./core`).
----------------------------------------

PR #4780: test(core): renable dedup insert fuzz test
Author: jerrinot
Merged at: 2024-07-17 19:10:59+00:00
URL: https://github.com/questdb/questdb/pull/4780
Description:
The dedup fuzz test was ignored by the Varchar PR: https://github.com/questdb/questdb/pull/4193 I am assuming it was meant to be temporary while
the varchar impl was incomplete. I see no good reason to have it disabled/ignored now.
----------------------------------------

PR #4632: fix(sql): fix spurious "unsupported operation" errors
Author: bluestreak01
Merged at: 2024-07-17 19:09:22+00:00
URL: https://github.com/questdb/questdb/pull/4632
Description:
fixes #4627
fixes #4628
fixes #4772
fixes #4777
fixes #4776
fixes #4778
fixes #4775

### Improve ergonomics of error responses around expression compilation:

```sql
query_activity where not 'blah';
```

demo (the expected type is quite random and is unhelpful on functions with large overload list):
```
unexpected argument for function: not. expected args: (BOOLEAN). actual args: (STRING constant)
```

branch:
```
there is no matching operator `not` with the argument type: STRING constant
```

```sql
query_activity where  'blah'::varchar + true;
```

demo:
```
unexpected argument for function: +. expected args: (INT,INT). actual args: (VARCHAR constant,BOOLEAN constant)
```

branch:
```
there is no matching operator`+` with the argument types: VARCHAR constant + BOOLEAN constant
```

### Changes NULL behaviour

- `NULL` and `NaN` are the same thing, `NaN` should not be used.
- creating table with `NULL` type is disallowed, e.g. this SQL will produce an error:

```sql
create table bb as (select null x);
-- cannot create NULL-type column, please use type cast, e.g. b::type
```

The workaround is to type cast null to a desired type:

```sql
create table bb as (select null::varchar x);
```

Unfortunately in tests, we relied on creating tables with "undefined" type to test the `UNION`. I have to rework these tests.

To-do's:

- [x] remove most manual type overloads in favour of `ColumnType.OVERLOAD_PRIORITY`
- [x] make `ColumnType.OVERLOAD_PRIORITY` the source of truth
- [x] make `timestamp = symbol` work consistently
- [x] make `symbol = long` work consistently (jit was ok with this, java wasn't)
- [x] work out why we default to string,string when timestamp,cursor is not found
- [x] test for new `symbol=long` function
- [ ] test `timestamp = symbol` runtime behaviour (when row in the middle fails to parse timestamp)
----------------------------------------

PR #4784: docs(core): update readme paragraph
Author: tris0laris
Merged at: 2024-07-17 13:42:51+00:00
URL: https://github.com/questdb/questdb/pull/4784
Description:
Readme edit
----------------------------------------

PR #4786: build: 8.0.3
Author: bluestreak01
Merged at: 2024-07-16 17:10:46+00:00
URL: https://github.com/questdb/questdb/pull/4786
Description:
None
----------------------------------------


FILE CHANGES
--------------------------------------------------------------------------------

File: .github/workflows/rebuild_native_libs.yml
Status: modified
Changes: +6 -83
Diff:
@@ -6,39 +6,16 @@ on:
 
 # It builds native libraries for all supported platforms and pushes them to the current branch.
 # It splits the building process into 3 build jobs:
-# 2. build-macos - Builds native libraries for MacOS: both ARM and x64.
+# 1. build-macos - Builds native libraries for MacOS: both ARM and x64.
 #    It uses native runners for each platform, because cross compilation on MacOS is complicated.
-# 3. build-others - Builds native libraries for x64 Linux, ARM Linux and Windows.
+# 2. build-others - Builds native libraries for x64 Linux, ARM Linux and Windows.
 #    It uses cross-compilation for ARM Linux and Windows.
 #
 # Each build job saves the resulting binaries to the cache under a unique key
 # When all build jobs are finished, the collect-commit-and-push job restores the binaries from the cache
 # and pushes them to the current branch.
 
 jobs:
-  build-rust-windows:
-    runs-on: windows-latest
-    steps:
-      - uses: actions/checkout@v4
-        with:
-          submodules: true
-      - name: Build Rust Library
-        run: |
-          cd core/rust/qdbr
-          cargo build --release
-      - name: log git status
-        run: |
-          git status
-      - name: Copy Rust Library to the final directory
-        run: |
-          cp core/rust/qdbr/target/release/questdbr.dll core/src/main/resources/io/questdb/bin/windows-amd64/
-      - name: Save Windows Rust Library to Cache
-        uses: actions/cache/save@v3
-        with:
-          path: |
-            core/src/main/resources/io/questdb/bin/windows-amd64/questdbr.dll
-          key: nativelibs-windows-rust-${{ github.sha }}
-          enableCrossOsArchive: true
   build-all-macos:
     strategy:
       matrix:
@@ -73,28 +50,12 @@ jobs:
           mkdir -p core/src/main/bin/darwin-amd64/
           cp core/target/classes/io/questdb/bin-local/libquestdb.dylib core/src/main/resources/io/questdb/bin/darwin-amd64/
           cp core/target/classes/io/questdb/bin-local/libjemalloc.dylib core/src/main/bin/darwin-amd64/
-      - name: Build Rust Library
-        run: |
-          cd core/rust/qdbr
-          cargo clean
-          cargo build --release
-      - name: Copy darwin-aarch64 Rust Library to the final directory
-        if: ${{ matrix.os == 'macos-14' }}
-        run: |
-          mkdir -p core/src/main/resources/io/questdb/bin/darwin-aarch64/
-          cp core/rust/qdbr/target/release/libquestdbr.dylib core/src/main/resources/io/questdb/bin/darwin-aarch64/
-      - name: Copy darwin-amd64 Rust Library to the final directory
-        if: ${{ matrix.os == 'macos-13' }}
-        run: |
-          mkdir -p core/src/main/resources/io/questdb/bin/darwin-amd64/
-          cp core/rust/qdbr/target/release/libquestdbr.dylib core/src/main/resources/io/questdb/bin/darwin-amd64/
       - name: Save darwin-aarch64 Libraries to Cache
         if: ${{ matrix.os == 'macos-14' }}
         uses: actions/cache/save@v3
         with:
           path: |
             core/src/main/resources/io/questdb/bin/darwin-aarch64/libquestdb.dylib
-            core/src/main/resources/io/questdb/bin/darwin-aarch64/libquestdbr.dylib
             core/src/main/bin/darwin-aarch64/libjemalloc.dylib
           key: nativelibs-armosx-${{ github.sha }}
       - name: Save darwin-amd64 Libraries to Cache
@@ -103,7 +64,6 @@ jobs:
         with:
           path: |
             core/src/main/resources/io/questdb/bin/darwin-amd64/libquestdb.dylib

----------------------------------------

File: .github/workflows/rebuild_rust.yml
Status: modified
Changes: +200 -5
Diff:
@@ -1,13 +1,208 @@
-name: Build and Push Rust libraries
+name: Build Rust libraries
 on:
   workflow_dispatch:
 # This workflow is triggered manually from the Actions tab.
-# It's meant to be run on a PR that changes the Rust code.
+# It's meant to be run on a PR that changes Rust code.
 
+# It builds native libraries for all supported platforms and pushes them to the current branch.
+# It splits the building process into 3 build jobs:
+# 1. build-macos - Builds native libraries for MacOS: both ARM and x64.
+#    It uses native runners for each platform, because cross compilation on MacOS is complicated.
+# 2. build-others - Builds native libraries for x64 Linux, ARM Linux and Windows.
+#    It uses cross-compilation for ARM Linux and Windows.
+#
+# Each build job saves the resulting binaries to the cache under a unique key
+# When all build jobs are finished, the collect-commit-and-push job restores the binaries from the cache
+# and pushes them to the current branch.
 
 jobs:
-  my-job:
+  build-rust-windows:
+    runs-on: windows-latest
+    steps:
+      - uses: actions/checkout@v4
+        with:
+          submodules: false
+      - name: Build Rust Library
+        run: |
+          cd core/rust/qdbr
+          cargo build --release
+      - name: Copy Rust Library to the final directory
+        run: |
+          cp core/rust/qdbr/target/release/questdbr.dll core/src/main/resources/io/questdb/bin/windows-amd64/
+      - name: Save Windows Rust Library to Cache
+        uses: actions/cache/save@v3
+        with:
+          path: |
+            core/src/main/resources/io/questdb/bin/windows-amd64/questdbr.dll
+          key: nativelibs-windows-rust-${{ github.sha }}
+          enableCrossOsArchive: true
+  build-all-macos:
+    strategy:
+      matrix:
+        # macos-14 = ARM M1
+        # macos-13 = x64
+        # if you change OS definitions then you need to change conditions in cache-save steps below
+        os: [ macos-14, macos-13 ]
+    runs-on: ${{ matrix.os }}
+    steps:
+      - uses: actions/checkout@v4
+        with:
+          submodules: false
+      - name: Build Rust Library
+        run: |
+          cd core/rust/qdbr
+          cargo clean
+          cargo build --release
+      - name: Copy darwin-aarch64 Rust Library to the final directory
+        if: ${{ matrix.os == 'macos-14' }}
+        run: |
+          mkdir -p core/src/main/resources/io/questdb/bin/darwin-aarch64/
+          cp core/rust/qdbr/target/release/libquestdbr.dylib core/src/main/resources/io/questdb/bin/darwin-aarch64/
+      - name: Copy darwin-amd64 Rust Library to the final directory
+        if: ${{ matrix.os == 'macos-13' }}
+        run: |
+          mkdir -p core/src/main/resources/io/questdb/bin/darwin-amd64/
+          cp core/rust/qdbr/target/release/libquestdbr.dylib core/src/main/resources/io/questdb/bin/darwin-amd64/
+      - name: Save darwin-aarch64 Libraries to Cache
+        if: ${{ matrix.os == 'macos-14' }}

----------------------------------------

File: .github/workflows/rebuild_rust_test.yml
Status: modified
Changes: +200 -5
Diff:
@@ -1,13 +1,208 @@
-name: Build and Push Rust Test libraries
+name: Build Rust Test libraries
 on:
   workflow_dispatch:
 # This workflow is triggered manually from the Actions tab.
-# It's meant to be run on a PR that changes the Rust Test lib code.
+# It's meant to be run on a PR that changes Rust test libraries code.
 
+# It builds native libraries for all supported platforms and pushes them to the current branch.
+# It splits the building process into 3 build jobs:
+# 1. build-macos - Builds native libraries for MacOS: both ARM and x64.
+#    It uses native runners for each platform, because cross compilation on MacOS is complicated.
+# 2. build-others - Builds native libraries for x64 Linux, ARM Linux and Windows.
+#    It uses cross-compilation for ARM Linux and Windows.
+#
+# Each build job saves the resulting binaries to the cache under a unique key
+# When all build jobs are finished, the collect-commit-and-push j ob restores the binaries from the cache
+# and pushes them to the current branch.
 
 jobs:
-  my-job:
+  build-rust-windows:
+    runs-on: windows-latest
+    steps:
+      - uses: actions/checkout@v4
+        with:
+          submodules: false
+      - name: Build Rust Library
+        run: |
+          cd core/rust/qdb-sqllogictest
+          cargo build --release
+      - name: Copy Rust Library to the final directory
+        run: |
+          cp core/rust/qdb-sqllogictest/target/release/qdbsqllogictest.dll core/src/test/resources/io/questdb/bin/windows-amd64/
+      - name: Save Windows Rust Library to Cache
+        uses: actions/cache/save@v3
+        with:
+          path: |
+            core/src/test/resources/io/questdb/bin/windows-amd64/qdbsqllogictest.dll
+          key: nativelibs-windows-rust-${{ github.sha }}
+          enableCrossOsArchive: true
+  build-all-macos:
+    strategy:
+      matrix:
+        # macos-14 = ARM M1
+        # macos-13 = x64
+        # if you change OS definitions then you need to change conditions in cache-save steps below
+        os: [ macos-14, macos-13 ]
+    runs-on: ${{ matrix.os }}
+    steps:
+      - uses: actions/checkout@v4
+        with:
+          submodules: false
+      - name: Build Rust Library
+        run: |
+          cd core/rust/qdb-sqllogictest
+          cargo clean
+          cargo build --release
+      - name: Copy darwin-aarch64 Rust Library to the final directory
+        if: ${{ matrix.os == 'macos-14' }}
+        run: |
+          mkdir -p core/src/test/resources/io/questdb/bin/darwin-aarch64/
+          cp core/rust/qdb-sqllogictest/target/release/libqdbsqllogictest.dylib core/src/test/resources/io/questdb/bin/darwin-aarch64/
+      - name: Copy darwin-amd64 Rust Library to the final directory
+        if: ${{ matrix.os == 'macos-13' }}
+        run: |
+          mkdir -p core/src/test/resources/io/questdb/bin/darwin-amd64/
+          cp core/rust/qdb-sqllogictest/target/release/libqdbsqllogictest.dylib core/src/test/resources/io/questdb/bin/darwin-amd64/
+      - name: Save darwin-aarch64 Libraries to Cache
+        if: ${{ matrix.os == 'macos-14' }}

----------------------------------------

File: README.md
Status: modified
Changes: +1 -1
Diff:
@@ -19,7 +19,7 @@ English | [ÁÆÄ‰Ωì‰∏≠Êñá](./i18n/README.zh-cn.md) | [ÁπÅÈ´î‰∏≠Êñá](./i18n/README.
 QuestDB is an open-source time-series database for high throughput
 ingestion and fast SQL queries with operational simplicity.
 
-QuestDB is well-suited for financial market data, IoT sensor data, ad-tech and real-time dashboards. It shines for datasets with [high cardinality](https://questdb.io/glossary/high-cardinality/)
+QuestDB is well-suited for financial market data, IoT sensor data and real-time dashboards. It shines for datasets with [high cardinality](https://questdb.io/glossary/high-cardinality/)
 and is a drop-in replacement for InfluxDB via support for the InfluxDB Line Protocol.
 
 QuestDB implements ANSI SQL with native time-series SQL extensions. These SQL extensions make it simple to filter and downsample data,

----------------------------------------

File: artifacts/release/README.md
Status: modified
Changes: +1 -0
Diff:
@@ -15,6 +15,7 @@ guidelines. Releases should not look too dissimilar.
 ```bash
 git fetch
 git checkout master
+git pull
 git checkout -b v_7_1_1
 ```
 

----------------------------------------

File: benchmarks/pom.xml
Status: modified
Changes: +1 -1
Diff:
@@ -27,7 +27,7 @@
 
     <groupId>io.questdb</groupId>
     <artifactId>benchmarks</artifactId>
-    <version>8.0.3-SNAPSHOT</version>
+    <version>8.1.0</version>
     <packaging>jar</packaging>
     <name>JMH benchmarks for QuestDB</name>
 

----------------------------------------

File: ci/docker-release-pipeline.yml
Status: modified
Changes: +4 -2
Diff:
@@ -21,7 +21,7 @@ jobs:
 
     steps:
       - bash: |
-          export hub_tag_name=$(git describe --tags)-$(tag_name)  
+          export hub_tag_name=$(git describe --tags)-$(tag_name)
           echo hub_tag_name=$hub_tag_name
           echo "##vso[task.setvariable variable=hub_tag_name]$hub_tag_name"
         name: check_changes
@@ -40,6 +40,7 @@ jobs:

----------------------------------------

File: ci/rust-test-and-lint.yaml
Status: added
Changes: +44 -0
Diff:
@@ -0,0 +1,44 @@
+jobs:
+  - job: RunOn
+    displayName: "on"
+    strategy:
+      matrix:
+        linux-jdk17:
+          imageName: "ubuntu-latest"
+          poolName: "Azure Pipelines"
+          os: Linux
+          jdk: "1.17"
+          testset: "all"
+    pool:
+      vmImage: $(imageName)
+      name: $(poolName)

----------------------------------------

File: ci/templates/check-changes-job.yml
Status: modified
Changes: +37 -42
Diff:
@@ -5,58 +5,53 @@ jobs:
       vmImage: "ubuntu-latest"
     steps:
       - checkout: none
-      - bash: |
-          echo $PRID
-          if [[ $PRID == ?(-)+([0-9]) ]]
-            then
-              CHANGED_CORE_FILES=$(curl https://api.github.com/repos/questdb/questdb/pulls/$PRID/files -s  | grep -oP 'filename": "(core|benchmarks|compat|utils)/*\/\K[^.]+')
-              if [ -z "$CHANGED_CORE_FILES" ]
-                then
-                  echo "##vso[task.setvariable variable=SOURCE_CODE_CHANGED;isOutput=true]false"
-                else
-                  echo "##vso[task.setvariable variable=SOURCE_CODE_CHANGED;isOutput=true]true"
-              fi
-            else
-              echo "##vso[task.setvariable variable=SOURCE_CODE_CHANGED;isOutput=true]true"
-          fi
         name: check_changes
         env:
           PRID: $(System.PullRequest.PullRequestNumber)
         displayName: "Check changed files"
       - bash: |
           echo $PRID
-          if [[ $PRID == ?(-)+([0-9]) ]]
-            then
-              COV_CLASSES=$(curl https://api.github.com/repos/questdb/questdb/pulls/$PRID/files?per_page=100 -s \
-                  | grep -oP 'filename": "core/src/main/java/io/questdb.*\/\K[^.]+' \
-                  | tr '\n' ',' | sed -e 's/,/,+:*./g' | sed 's/,+:\*\.$//')
-
-              if [ -z "$COV_CLASSES" ]
-              then

----------------------------------------

File: ci/templates/hosted-compat-jobs.yml
Status: modified
Changes: +1 -1
Diff:
@@ -15,7 +15,7 @@ jobs:
       name: $(poolName)
     timeoutInMinutes: 60
     variables:
-      SOURCE_CODE_CHANGED: $[stageDependencies.CheckChanges.CheckChanges.outputs['check_changes.SOURCE_CODE_CHANGED']]
+      SOURCE_CODE_CHANGED: $[stageDependencies.CheckChanges.CheckChanges.outputs['check_coverage.SOURCE_CODE_CHANGED']]
       COVERAGE_DIFF: $[stageDependencies.CheckChanges.CheckChanges.outputs['check_coverage.COVERAGE_DIFF']]
       CODE_COVERAGE_TOOL_OPTION: $[stageDependencies.CheckChanges.CheckChanges.outputs['check_coverage.CODE_COVERAGE_TOOL_OPTION']]
       ARCHIVED_CRASH_LOG: "$(Build.ArtifactStagingDirectory)/questdb-crash-$(Build.SourceBranchName)-$(Build.SourceVersion)-$(System.StageAttempt)-$(Agent.OS)-$(jdk).log"

----------------------------------------

File: ci/templates/hosted-cover-jobs.yml
Status: modified
Changes: +1 -1
Diff:
@@ -35,7 +35,7 @@ jobs:
       name: $(poolName)
     timeoutInMinutes: 60
     variables:
-      SOURCE_CODE_CHANGED: $[stageDependencies.CheckChanges.CheckChanges.outputs['check_changes.SOURCE_CODE_CHANGED']]
+      SOURCE_CODE_CHANGED: $[stageDependencies.CheckChanges.CheckChanges.outputs['check_coverage.SOURCE_CODE_CHANGED']]
       COVERAGE_DIFF: $[stageDependencies.CheckChanges.CheckChanges.outputs['check_coverage.COVERAGE_DIFF']]
       CODE_COVERAGE_TOOL_OPTION: $[stageDependencies.CheckChanges.CheckChanges.outputs['check_coverage.CODE_COVERAGE_TOOL_OPTION']]
       ARCHIVED_CRASH_LOG: "$(Build.ArtifactStagingDirectory)/questdb-crash-$(Build.SourceBranchName)-$(Build.SourceVersion)-$(System.StageAttempt)-$(Agent.OS)-$(jdk).log"

----------------------------------------

File: ci/templates/hosted-jobs.yml
Status: modified
Changes: +1 -1
Diff:
@@ -88,7 +88,7 @@ jobs:
       name: $(poolName)
     timeoutInMinutes: 60
     variables:
-      SOURCE_CODE_CHANGED: $[stageDependencies.CheckChanges.CheckChanges.outputs['check_changes.SOURCE_CODE_CHANGED']]
+      SOURCE_CODE_CHANGED: $[stageDependencies.CheckChanges.CheckChanges.outputs['check_coverage.SOURCE_CODE_CHANGED']]
       COVERAGE_DIFF: $[stageDependencies.CheckChanges.CheckChanges.outputs['check_coverage.COVERAGE_DIFF']]
       CODE_COVERAGE_TOOL_OPTION: $[stageDependencies.CheckChanges.CheckChanges.outputs['check_coverage.CODE_COVERAGE_TOOL_OPTION']]
       ARCHIVED_CRASH_LOG: "$(Build.ArtifactStagingDirectory)/questdb-crash-$(Build.SourceBranchName)-$(Build.SourceVersion)-$(System.StageAttempt)-$(Agent.OS)-$(jdk).log"

----------------------------------------

File: ci/templates/java-code-formatting-intellij.yml
Status: added
Changes: +49 -0
Diff:
@@ -0,0 +1,49 @@
+jobs:
+  - job: RunOn
+    displayName: "on"
+    strategy:
+      matrix:
+        linux-jdk17:
+          imageName: "ubuntu-latest"
+          poolName: "Azure Pipelines"
+          os: Linux
+          jdk: "1.17"
+          testset: "none"
+          javadoc_step: "javadoc:javadoc"
+          javadoc_profile: ",javadoc"
+    pool:
+      vmImage: $(imageName)

----------------------------------------

File: ci/templates/self-hosted-jobs.yml
Status: modified
Changes: +2 -2
Diff:
@@ -11,7 +11,7 @@ jobs:
     timeoutInMinutes: 60
     condition: eq(variables['System.PullRequest.IsFork'], 'false')
     variables:
-      SOURCE_CODE_CHANGED: $[stageDependencies.CheckChanges.CheckChanges.outputs['check_changes.SOURCE_CODE_CHANGED']]
+      SOURCE_CODE_CHANGED: $[stageDependencies.CheckChanges.CheckChanges.outputs['check_coverage.SOURCE_CODE_CHANGED']]
       CODE_COVERAGE_TOOL_OPTION: $[stageDependencies.CheckChanges.CheckChanges.outputs['check_coverage.CODE_COVERAGE_TOOL_OPTION']]
       ARCHIVED_CRASH_LOG: "$(Build.ArtifactStagingDirectory)/questdb-crash-$(Build.SourceBranchName)-$(Build.SourceVersion)-$(System.StageAttempt)-$(Agent.OS)-$(jdk).log"
       testset: "all"
@@ -34,7 +34,7 @@ jobs:

----------------------------------------

File: ci/test-pipeline.yml
Status: modified
Changes: +15 -1
Diff:
@@ -23,6 +23,13 @@ stages:
     jobs:
       - template: templates/check-changes-job.yml
 
+  - stage: RustTestAndLint
+    displayName: "Rust Test and Lint"
+    dependsOn:
+      - CheckChanges
+    jobs:
+      - template: rust-test-and-lint.yaml
+
   - stage: SelfHostedRunGriffin

----------------------------------------

File: compat/pom.xml
Status: modified
Changes: +13 -1
Diff:
@@ -27,7 +27,7 @@
 
     <groupId>io.questdb</groupId>
     <artifactId>compat</artifactId>
-    <version>8.0.3-SNAPSHOT</version>
+    <version>8.1.0</version>
 
     <name>Compatibility tests for QuestDB</name>
 
@@ -72,6 +72,18 @@

----------------------------------------

File: compat/src/test/java/io/questdb/compat/LineOkHttpFuzzTest.java
Status: modified
Changes: +3 -3
Diff:
@@ -170,7 +170,7 @@ public void testFuzzChunkedDataIlpUpload() throws SqlException {
             }
 
             serverMain.awaitTable("m1");
-            assertSql(serverMain.getEngine(),"select count() from m1", "count\n" +
+            assertSql(serverMain.getEngine(), "select count() from m1", "count\n" +
                     totalCount + "\n");
         }
     }
@@ -204,7 +204,7 @@ public void testMultipartDataIlpUpload() throws Exception {

----------------------------------------

File: compat/src/test/java/io/questdb/compat/ParquetTest.java
Status: added
Changes: +463 -0
Diff:
@@ -0,0 +1,463 @@
+/*******************************************************************************
+ *     ___                  _   ____  ____
+ *    / _ \ _   _  ___  ___| |_|  _ \| __ )
+ *   | | | | | | |/ _ \/ __| __| | | |  _ \
+ *   | |_| | |_| |  __/\__ \ |_| |_| | |_) |
+ *    \__\_\\__,_|\___||___/\__|____/|____/
+ *
+ *  Copyright (c) 2014-2019 Appsicle
+ *  Copyright (c) 2019-2024 QuestDB
+ *
+ *  Licensed under the Apache License, Version 2.0 (the "License");
+ *  you may not use this file except in compliance with the License.
+ *  You may obtain a copy of the License at
+ *
+ *  http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *  Unless required by applicable law or agreed to in writing, software
+ *  distributed under the License is distributed on an "AS IS" BASIS,
+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *  See the License for the specific language governing permissions and
+ *  limitations under the License.
+ *
+ ******************************************************************************/
+
+package io.questdb.compat;
+
+import io.questdb.ServerMain;
+import io.questdb.cairo.TableReader;
+import io.questdb.cairo.sql.Record;
+import io.questdb.cairo.sql.RecordCursor;
+import io.questdb.griffin.engine.table.parquet.PartitionDescriptor;
+import io.questdb.griffin.engine.table.parquet.PartitionEncoder;
+import io.questdb.log.Log;
+import io.questdb.log.LogFactory;
+import io.questdb.std.*;
+import io.questdb.std.str.Path;
+import io.questdb.std.str.StringSink;
+import io.questdb.std.str.Utf8Sequence;
+import io.questdb.std.str.Utf8s;
+import org.apache.avro.generic.GenericData;
+import org.apache.avro.generic.GenericRecord;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.parquet.avro.AvroParquetReader;
+import org.apache.parquet.column.ColumnDescriptor;
+import org.apache.parquet.column.statistics.Statistics;
+import org.apache.parquet.hadoop.ParquetFileReader;
+import org.apache.parquet.hadoop.ParquetReader;
+import org.apache.parquet.hadoop.metadata.BlockMetaData;
+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;
+import org.apache.parquet.hadoop.metadata.FileMetaData;
+import org.apache.parquet.hadoop.metadata.ParquetMetadata;
+import org.apache.parquet.hadoop.util.HadoopInputFile;
+import org.apache.parquet.io.InputFile;
+import org.apache.parquet.schema.MessageType;
+import org.apache.parquet.schema.PrimitiveType;
+import org.junit.Assert;
+import org.junit.Test;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.nio.ByteOrder;
+import java.util.List;
+
+public class ParquetTest extends AbstractTest {
+    private final static long DATA_PAGE_SIZE = 128; // bytes
+    private final static Log LOG = LogFactory.getLog(ParquetTest.class);
+    private final static int NUMERIC_MAX = 10;
+    private final static int NUMERIC_MIN = -10;
+    private final static long ROW_GROUP_SIZE = 64;
+    private final static long INITIAL_ROWS = ROW_GROUP_SIZE * 2;
+    private final static long UPDATE_ROWS = ROW_GROUP_SIZE * 4;
+
+    @Test
+    public void testAllTypes() throws Exception {
+        final String tableName = "x";
+        String ddl = "create table " + tableName + " as (select" +
+                " x id," +
+                " rnd_boolean() a_boolean," +
+                " rnd_byte() a_byte," +
+                " rnd_short() a_short," +
+                " rnd_char() a_char," +
+                " rnd_int(" + NUMERIC_MIN + ", " + NUMERIC_MAX + ", 0) an_int," +
+                " rnd_long(" + NUMERIC_MIN + ", " + NUMERIC_MAX + ", 0) a_long," +
+                " rnd_float() a_float," +
+                " rnd_double() a_double," +
+                " rnd_symbol('a','b','c') a_symbol," +
+                " rnd_geohash(4) a_geo_byte," +
+                " rnd_geohash(8) a_geo_short," +
+                " rnd_geohash(16) a_geo_int," +
+                " rnd_geohash(32) a_geo_long," +
+                " rnd_str('hello', 'world', '!') a_string," +
+                " rnd_bin(1, 8, 0) a_bin," +
+                " rnd_varchar('–≥–∞–Ω—å–±–∞','—Å–ª–∞–≤–∞','–¥–æ–±—Ä–∏–π','–≤–µ—á—ñ—Ä') a_varchar," +
+                " rnd_ipv4() a_ip," +
+                " rnd_uuid4() a_uuid," +
+                " rnd_long256() a_long256," +
+                " to_long128(rnd_long(), rnd_long()) a_long128," +
+                " cast(timestamp_sequence(600000000000, 700) as date) a_date," +
+                " timestamp_sequence(500000000000, 600) a_ts," +
+                " timestamp_sequence(400000000000, 500) designated_ts" +
+                " from long_sequence(" + INITIAL_ROWS + ")) timestamp(designated_ts) partition by month";
+
+        try (final ServerMain serverMain = ServerMain.create(root)) {
+            serverMain.start();
+            serverMain.getEngine().compile(ddl); // txn 1
+
+            serverMain.getEngine().compile("alter table " + tableName + " add column an_int_top int");
+            serverMain.getEngine().compile("alter table " + tableName + " add column a_long_top long");
+            serverMain.getEngine().compile("alter table " + tableName + " add column a_float_top float");
+            serverMain.getEngine().compile("alter table " + tableName + " add column a_double_top double");
+            serverMain.getEngine().compile("alter table " + tableName + " add column a_symbol_top symbol");
+            serverMain.getEngine().compile("alter table " + tableName + " add column a_geo_byte_top geohash(4b)");
+            serverMain.getEngine().compile("alter table " + tableName + " add column a_geo_short_top geohash(8b)");
+            serverMain.getEngine().compile("alter table " + tableName + " add column a_geo_int_top geohash(16b)");
+            serverMain.getEngine().compile("alter table " + tableName + " add column a_geo_long_top geohash(32b)");
+            serverMain.getEngine().compile("alter table " + tableName + " add column a_string_top string");
+            serverMain.getEngine().compile("alter table " + tableName + " add column a_bin_top binary");
+            serverMain.getEngine().compile("alter table " + tableName + " add column a_varchar_top varchar");
+            serverMain.getEngine().compile("alter table " + tableName + " add column a_ip_top ipv4");
+            serverMain.getEngine().compile("alter table " + tableName + " add column a_uuid_top uuid");
+            serverMain.getEngine().compile("alter table " + tableName + " add column a_long128_top long128");
+            serverMain.getEngine().compile("alter table " + tableName + " add column a_long256_top long256");
+            serverMain.getEngine().compile("alter table " + tableName + " add column a_date_top date");
+            serverMain.getEngine().compile("alter table " + tableName + " add column a_ts_top timestamp");
+
+            String insert = "insert into " + tableName + "(id, an_int_top, a_long_top, a_float_top, a_double_top,\n" +
+                    " a_symbol_top, a_geo_byte_top, a_geo_short_top, a_geo_int_top, a_geo_long_top,\n" +
+                    " a_string_top, a_bin_top, a_varchar_top, a_ip_top, a_uuid_top, a_long128_top, a_long256_top,\n" +
+                    " a_date_top, a_ts_top, designated_ts) select\n" +
+                    " " + INITIAL_ROWS + " + x," +
+                    " rnd_int(" + NUMERIC_MIN + ", " + NUMERIC_MAX + ", 2)," +
+                    " rnd_long(" + NUMERIC_MIN + ", " + NUMERIC_MAX + ", 2)," +
+                    " rnd_float(2)," +
+                    " rnd_double(2)," +
+                    " rnd_symbol('a','b','c', null)," +
+                    " rnd_geohash(4)," +
+                    " rnd_geohash(8)," +
+                    " rnd_geohash(16)," +
+                    " rnd_geohash(32)," +
+                    " rnd_str('hello', 'world', '!', null)," +
+                    " rnd_bin(1, 8, 2)," +
+                    " rnd_varchar('–≥–∞–Ω—å–±–∞','—Å–ª–∞–≤–∞','–¥–æ–±—Ä–∏–π','–≤–µ—á—ñ—Ä', null)," +
+                    " rnd_ipv4('192.168.88.0/24', 2)," +
+                    " rnd_uuid4()," +
+                    " to_long128(rnd_long(0,10, 2), null)," +
+                    " to_long256(rnd_long(0,10, 2), null, null, null)," +
+                    " rnd_date(" +
+                    "    to_date('2022', 'yyyy')," +
+                    "    to_date('2027', 'yyyy')," +
+                    "    2) a_date_top," +
+                    " rnd_timestamp(" +
+                    "    to_timestamp('2022', 'yyyy')," +
+                    "    to_timestamp('2027', 'yyyy')," +

----------------------------------------

File: core/Dockerfile
Status: modified
Changes: +22 -2
Diff:
@@ -1,11 +1,13 @@
+ARG base_image=debian:bookworm-slim
+
 FROM debian:bookworm
 ARG tag_name
 
 ENV GOSU_VERSION=1.14
 ENV JDK_VERSION=17.0.11.9-1
 
 RUN apt-get update \
-  && apt-get install --no-install-recommends git curl wget gnupg2 ca-certificates lsb-release software-properties-common unzip -y
+    && apt-get install --no-install-recommends git curl wget gnupg2 ca-certificates lsb-release software-properties-common unzip -y
 
 RUN wget -O - https://apt.corretto.aws/corretto.key | gpg --dearmor -o /usr/share/keyrings/corretto-keyring.gpg && \
     echo "deb [signed-by=/usr/share/keyrings/corretto-keyring.gpg] https://apt.corretto.aws stable main" | tee /etc/apt/sources.list.d/corretto.list && \
@@ -42,7 +44,8 @@ RUN dpkgArch="$(dpkg --print-architecture | awk -F- '{ print $NF }')"; \
     ./gosu --version && \
     ./gosu nobody true

----------------------------------------

File: core/LICENSE.txt
Status: added
Changes: +202 -0
Diff:
@@ -0,0 +1,202 @@
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of

----------------------------------------

File: core/pom.xml
Status: modified
Changes: +3 -3
Diff:
@@ -36,12 +36,12 @@
         <argLine>-ea -Dfile.encoding=UTF-8</argLine>
         <test.exclude>None</test.exclude>
         <test.include>%regex[.*[^o].class]</test.include><!-- exclude module-info.class-->
-        <web.console.version>0.5.0</web.console.version>
+        <web.console.version>0.5.1</web.console.version>
         <qdbr.path>rust/qdbr</qdbr.path>
         <qdbr.release>false</qdbr.release>
     </properties>
 

----------------------------------------

File: core/rust/qdb-sqllogictest/.gitignore
Status: added
Changes: +1 -0
Diff:
@@ -0,0 +1 @@
+target/

----------------------------------------

File: core/rust/qdb-sqllogictest/Cargo.lock
Status: added
Changes: +2009 -0
Diff:
@@ -0,0 +1,2009 @@
+# This file is automatically @generated by Cargo.
+# It is not intended for manual editing.
+version = 3
+
+[[package]]
+name = "addr2line"
+version = "0.22.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "6e4503c46a5c0c7844e948c9a4d6acd9f50cccb4de1c48eb9e291ea17470c678"
+dependencies = [
+    "gimli",
+]
+
+[[package]]
+name = "adler"
+version = "1.0.2"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "f26201604c87b1e01bd3d98f8d5d9a8fcbb815e8cedb41ffccbeb4bf593a35fe"
+
+[[package]]
+name = "ahash"
+version = "0.7.8"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "891477e0c6a8957309ee5c45a6368af3ae14bb510732d2684ffa19af310920f9"
+dependencies = [
+    "getrandom",
+    "once_cell",
+    "version_check",
+]
+
+[[package]]
+name = "aho-corasick"
+version = "1.1.3"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "8e60d3430d3a69478ad0993f19238d2df97c507009a52b3c10addcd7f6bcb916"
+dependencies = [
+    "memchr",
+]
+
+[[package]]
+name = "android-tzdata"
+version = "0.1.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "e999941b234f3131b00bc13c22d06e8c5ff726d1b6318ac7eb276997bbb4fef0"
+
+[[package]]
+name = "android_system_properties"
+version = "0.1.5"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "819e7219dbd41043ac279b19830f2efc897156490d7fd6ea916720117ee66311"
+dependencies = [
+    "libc",
+]
+
+[[package]]
+name = "anstream"
+version = "0.6.14"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "418c75fa768af9c03be99d17643f93f79bbba589895012a80e3452a19ddda15b"
+dependencies = [
+    "anstyle",
+    "anstyle-parse",
+    "anstyle-query",
+    "anstyle-wincon",
+    "colorchoice",
+    "is_terminal_polyfill",
+    "utf8parse",
+]
+
+[[package]]
+name = "anstyle"
+version = "1.0.7"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "038dfcf04a5feb68e9c60b21c9625a54c2c0616e79b72b0fd87075a056ae1d1b"
+
+[[package]]
+name = "anstyle-parse"
+version = "0.2.4"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "c03a11a9034d92058ceb6ee011ce58af4a9bf61491aa7e1e59ecd24bd40d22d4"
+dependencies = [
+    "utf8parse",
+]
+
+[[package]]
+name = "anstyle-query"
+version = "1.1.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "ad186efb764318d35165f1758e7dcef3b10628e26d41a44bc5550652e6804391"
+dependencies = [
+    "windows-sys 0.52.0",
+]
+
+[[package]]
+name = "anstyle-wincon"
+version = "3.0.3"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "61a38449feb7068f52bb06c12759005cf459ee52bb4adc1d5a7c4322d716fb19"
+dependencies = [
+    "anstyle",
+    "windows-sys 0.52.0",
+]
+
+[[package]]
+name = "anyhow"
+version = "1.0.86"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "b3d1d046238990b9cf5bcde22a3fb3584ee5cf65fb2765f454ed428c7a0063da"
+
+[[package]]
+name = "arrayvec"
+version = "0.7.4"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "96d30a06541fbafbc7f82ed10c06164cfbd2c401138f6addd8404629c4b16711"
+
+[[package]]
+name = "async-trait"
+version = "0.1.81"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "6e0c28dcc82d7c8ead5cb13beb15405b57b8546e93215673ff8ca0349a028107"
+dependencies = [
+    "proc-macro2",
+    "quote",
+    "syn 2.0.70",
+]
+
+[[package]]
+name = "autocfg"
+version = "1.3.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "0c4b4d0bd25bd0b74681c0ad21497610ce1b7c91b1022cd21c80c6fbdd9476b0"
+
+[[package]]
+name = "backtrace"
+version = "0.3.73"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "5cc23269a4f8976d0a4d2e7109211a419fe30e8d88d677cd60b6bc79c5732e0a"
+dependencies = [
+    "addr2line",
+    "cc",
+    "cfg-if",
+    "libc",
+    "miniz_oxide",
+    "object",
+    "rustc-demangle",
+]
+
+[[package]]
+name = "base64"
+version = "0.21.7"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "9d297deb1925b89f2ccc13d7635fa0714f12c87adce1c75356b39ca9b7178567"
+
+[[package]]
+name = "bitflags"
+version = "1.3.2"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "bef38d45163c2f1dde094a7dfd33ccf595c92905c8f8f4fdc18d06fb1037718a"
+
+[[package]]
+name = "bitflags"
+version = "2.6.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "b048fb63fd8b5923fc5aa7b340d8e156aec7ec02f0c78fa8a6ddc2613f6f71de"
+
+[[package]]
+name = "bitvec"
+version = "1.0.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "1bc2832c24239b0141d5674bb9174f9d68a8b5b3f2753311927c172ca46f7e9c"
+dependencies = [
+    "funty",
+    "radium",
+    "tap",
+    "wyz",
+]
+
+[[package]]
+name = "block-buffer"
+version = "0.10.4"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "3078c7629b62d3f0439517fa394996acacc5cbc91c5a20d8c658e77abd503a71"
+dependencies = [
+    "generic-array",
+]
+
+[[package]]
+name = "borsh"
+version = "1.5.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "a6362ed55def622cddc70a4746a68554d7b687713770de539e59a739b249f8ed"
+dependencies = [
+    "borsh-derive",
+    "cfg_aliases",
+]
+
+[[package]]
+name = "borsh-derive"
+version = "1.5.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "c3ef8005764f53cd4dca619f5bf64cafd4664dada50ece25e4d81de54c80cc0b"
+dependencies = [
+    "once_cell",
+    "proc-macro-crate",
+    "proc-macro2",
+    "quote",
+    "syn 2.0.70",
+    "syn_derive",
+]
+
+[[package]]
+name = "bumpalo"
+version = "3.16.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "79296716171880943b8470b5f8d03aa55eb2e645a4874bdbb28adb49162e012c"
+
+[[package]]
+name = "bytecheck"
+version = "0.6.12"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "23cdc57ce23ac53c931e88a43d06d070a6fd142f2617be5855eb75efc9beb1c2"
+dependencies = [
+    "bytecheck_derive",
+    "ptr_meta",
+    "simdutf8",
+]
+
+[[package]]
+name = "bytecheck_derive"
+version = "0.6.12"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "3db406d29fbcd95542e92559bed4d8ad92636d1ca8b3b72ede10b4bcc010e659"
+dependencies = [
+    "proc-macro2",
+    "quote",
+    "syn 1.0.109",
+]
+
+[[package]]
+name = "byteorder"
+version = "1.5.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "1fd0f2584146f6f2ef48085050886acf353beff7305ebd1ae69500e27c67f64b"
+
+[[package]]
+name = "bytes"
+version = "1.6.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "514de17de45fdb8dc022b1a7975556c53c86f9f0aa5f534b98977b171857c2c9"
+
+[[package]]
+name = "cc"
+version = "1.1.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "eaff6f8ce506b9773fa786672d63fc7a191ffea1be33f72bbd4aeacefca9ffc8"
+
+[[package]]
+name = "cesu8"
+version = "1.1.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "6d43a04d8753f35258c91f8ec639f792891f748a1edbd759cf1dcea3382ad83c"
+
+[[package]]
+name = "cfg-if"
+version = "1.0.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "baf1de4339761588bc0619e3cbc0120ee582ebb74b53b4efbf79117bd2da40fd"
+
+[[package]]
+name = "cfg_aliases"
+version = "0.2.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "613afe47fcd5fac7ccf1db93babcb082c5994d996f20b8b159f2ad1658eb5724"
+
+[[package]]
+name = "chrono"
+version = "0.4.38"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "a21f936df1771bf62b77f047b726c4625ff2e8aa607c01ec06e5a05bd8463401"
+dependencies = [
+    "android-tzdata",
+    "iana-time-zone",
+    "js-sys",
+    "num-traits",
+    "wasm-bindgen",
+    "windows-targets 0.52.6",
+]
+
+[[package]]
+name = "clap"
+version = "4.5.9"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "64acc1846d54c1fe936a78dc189c34e28d3f5afc348403f28ecf53660b9b8462"
+dependencies = [
+    "clap_builder",
+    "clap_derive",
+]
+
+[[package]]
+name = "clap_builder"
+version = "4.5.9"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "6fb8393d67ba2e7bfaf28a23458e4e2b543cc73a99595511eb207fdb8aede942"
+dependencies = [
+    "anstream",
+    "anstyle",
+    "clap_lex",
+    "strsim",
+]
+
+[[package]]
+name = "clap_derive"
+version = "4.5.8"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "2bac35c6dafb060fd4d275d9a4ffae97917c13a6327903a8be2153cd964f7085"
+dependencies = [
+    "heck 0.5.0",
+    "proc-macro2",
+    "quote",
+    "syn 2.0.70",
+]
+
+[[package]]
+name = "clap_lex"
+version = "0.7.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "4b82cf0babdbd58558212896d1a4272303a57bdb245c2bf1147185fb45640e70"
+
+[[package]]
+name = "colorchoice"
+version = "1.0.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "0b6a852b24ab71dffc585bcb46eaf7959d175cb865a7152e35b348d1b2960422"
+
+[[package]]
+name = "combine"
+version = "4.6.7"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "ba5a308b75df32fe02788e748662718f03fde005016435c444eea572398219fd"
+dependencies = [
+    "bytes",
+    "memchr",
+]
+
+[[package]]
+name = "core-foundation-sys"
+version = "0.8.6"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "06ea2b9bc92be3c2baa9334a323ebca2d6f074ff852cd1d7b11064035cd3868f"
+
+[[package]]
+name = "cpufeatures"
+version = "0.2.12"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "53fe5e26ff1b7aef8bca9c6080520cfb8d9333c7568e1829cef191a9723e5504"
+dependencies = [
+    "libc",
+]
+
+[[package]]
+name = "crypto-common"
+version = "0.1.6"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "1bfb12502f3fc46cca1bb51ac28df9d618d813cdc3d2f25b9fe775a34af26bb3"
+dependencies = [
+    "generic-array",
+    "typenum",
+]
+
+[[package]]
+name = "digest"
+version = "0.10.7"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "9ed9a281f7bc9b7576e61468ba615a66a5c8cfdff42420a70aa82701a3b1e292"
+dependencies = [
+    "block-buffer",
+    "crypto-common",
+    "subtle",
+]
+
+[[package]]
+name = "educe"
+version = "0.6.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "1d7bc049e1bd8cdeb31b68bbd586a9464ecf9f3944af3958a7a9d0f8b9799417"
+dependencies = [
+    "enum-ordinalize",
+    "proc-macro2",
+    "quote",
+    "syn 2.0.70",
+]
+
+[[package]]
+name = "either"
+version = "1.13.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "60b1af1c220855b6ceac025d3f6ecdd2b7c4894bfe9cd9bda4fbb4bc7c0d4cf0"
+
+[[package]]
+name = "enum-ordinalize"
+version = "4.3.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "fea0dcfa4e54eeb516fe454635a95753ddd39acda650ce703031c6973e315dd5"
+dependencies = [
+    "enum-ordinalize-derive",
+]
+
+[[package]]
+name = "enum-ordinalize-derive"
+version = "4.3.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "0d28318a75d4aead5c4db25382e8ef717932d0346600cacae6357eb5941bc5ff"
+dependencies = [
+    "proc-macro2",
+    "quote",
+    "syn 2.0.70",
+]
+
+[[package]]
+name = "equivalent"
+version = "1.0.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "5443807d6dff69373d433ab9ef5378ad8df50ca6298caf15de6e52e24aaf54d5"
+
+[[package]]
+name = "errno"
+version = "0.3.9"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "534c5cf6194dfab3db3242765c03bbe257cf92f22b38f6bc0c58d59108a820ba"
+dependencies = [
+    "libc",
+    "windows-sys 0.52.0",
+]
+
+[[package]]
+name = "escape8259"
+version = "0.5.3"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "5692dd7b5a1978a5aeb0ce83b7655c58ca8efdcb79d21036ea249da95afec2c6"
+
+[[package]]
+name = "fallible-iterator"
+version = "0.2.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "4443176a9f2c162692bd3d352d745ef9413eec5782a80d8fd6f8a1ac692a07f7"
+
+[[package]]
+name = "fastrand"
+version = "2.1.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "9fc0510504f03c51ada170672ac806f1f105a88aa97a5281117e1ddc3368e51a"
+
+[[package]]
+name = "fs-err"
+version = "2.11.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "88a41f105fe1d5b6b34b2055e3dc59bb79b46b48b2040b9e6c7b4b5de097aa41"
+dependencies = [
+    "autocfg",
+]
+
+[[package]]
+name = "funty"
+version = "2.0.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "e6d5a32815ae3f33302d95fdcb2ce17862f8c65363dcfd29360480ba1001fc9c"
+
+[[package]]
+name = "futures"
+version = "0.3.30"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "645c6916888f6cb6350d2550b80fb63e734897a8498abe35cfb732b6487804b0"
+dependencies = [
+    "futures-channel",
+    "futures-core",
+    "futures-executor",
+    "futures-io",
+    "futures-sink",
+    "futures-task",
+    "futures-util",
+]
+
+[[package]]
+name = "futures-channel"
+version = "0.3.30"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "eac8f7d7865dcb88bd4373ab671c8cf4508703796caa2b1985a9ca867b3fcb78"
+dependencies = [
+    "futures-core",
+    "futures-sink",
+]
+
+[[package]]
+name = "futures-core"
+version = "0.3.30"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "dfc6580bb841c5a68e9ef15c77ccc837b40a7504914d52e47b8b0e9bbda25a1d"
+
+[[package]]
+name = "futures-executor"
+version = "0.3.30"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "a576fc72ae164fca6b9db127eaa9a9dda0d61316034f33a0a0d4eda41f02b01d"
+dependencies = [
+    "futures-core",
+    "futures-task",
+    "futures-util",
+]
+
+[[package]]
+name = "futures-io"
+version = "0.3.30"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "a44623e20b9681a318efdd71c299b6b222ed6f231972bfe2f224ebad6311f0c1"
+
+[[package]]
+name = "futures-macro"
+version = "0.3.30"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "87750cf4b7a4c0625b1529e4c543c2182106e4dedc60a2a6455e00d212c489ac"
+dependencies = [
+    "proc-macro2",
+    "quote",
+    "syn 2.0.70",
+]
+
+[[package]]
+name = "futures-sink"
+version = "0.3.30"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "9fb8e00e87438d937621c1c6269e53f536c14d3fbd6a042bb24879e57d474fb5"
+
+[[package]]
+name = "futures-task"
+version = "0.3.30"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "38d84fa142264698cdce1a9f9172cf383a0c82de1bddcf3092901442c4097004"
+
+[[package]]
+name = "futures-util"
+version = "0.3.30"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "3d6401deb83407ab3da39eba7e33987a73c3df0c82b4bb5813ee871c19c41d48"
+dependencies = [
+    "futures-channel",
+    "futures-core",
+    "futures-io",
+    "futures-macro",
+    "futures-sink",
+    "futures-task",
+    "memchr",
+    "pin-project-lite",
+    "pin-utils",
+    "slab",
+]
+
+[[package]]
+name = "generic-array"
+version = "0.14.7"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "85649ca51fd72272d7821adaf274ad91c288277713d9c18820d8499a7ff69e9a"
+dependencies = [
+    "typenum",
+    "version_check",
+]
+
+[[package]]
+name = "getrandom"
+version = "0.2.15"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "c4567c8db10ae91089c99af84c68c38da3ec2f087c3f82960bcdbf3656b6f4d7"
+dependencies = [
+    "cfg-if",
+    "libc",
+    "wasi",
+]
+
+[[package]]
+name = "gimli"
+version = "0.29.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "40ecd4077b5ae9fd2e9e169b102c6c330d0605168eb0e8bf79952b256dbefffd"
+
+[[package]]
+name = "glob"
+version = "0.3.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "d2fabcfbdc87f4758337ca535fb41a6d701b65693ce38287d856d1674551ec9b"
+
+[[package]]
+name = "hashbrown"
+version = "0.12.3"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "8a9ee70c43aaf417c914396645a0fa852624801b24ebb7ae78fe8272889ac888"
+dependencies = [
+    "ahash",
+]
+
+[[package]]
+name = "hashbrown"
+version = "0.14.5"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "e5274423e17b7c9fc20b6e7e208532f9b19825d82dfd615708b70edd83df41f1"
+
+[[package]]
+name = "heck"
+version = "0.4.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "95505c38b4572b2d910cecb0281560f54b440a19336cbbcb27bf6ce6adc6f5a8"
+
+[[package]]
+name = "heck"
+version = "0.5.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "2304e00983f87ffb38b55b444b5e3b60a884b5d30c0fca7d82fe33449bbe55ea"
+
+[[package]]
+name = "hermit-abi"
+version = "0.3.9"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "d231dfb89cfffdbc30e7fc41579ed6066ad03abda9e567ccafae602b97ec5024"
+
+[[package]]
+name = "hmac"
+version = "0.12.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "6c49c37c09c17a53d937dfbb742eb3a961d65a994e6bcdcf37e7399d0cc8ab5e"
+dependencies = [
+    "digest",
+]
+
+[[package]]
+name = "humantime"
+version = "2.1.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "9a3a5bfb195931eeb336b2a7b4d761daec841b97f947d34394601737a7bba5e4"
+
+[[package]]
+name = "iana-time-zone"
+version = "0.1.60"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "e7ffbb5a1b541ea2561f8c41c087286cc091e21e556a4f09a8f6cbf17b69b141"
+dependencies = [
+    "android_system_properties",
+    "core-foundation-sys",
+    "iana-time-zone-haiku",
+    "js-sys",
+    "wasm-bindgen",
+    "windows-core",
+]
+
+[[package]]
+name = "iana-time-zone-haiku"
+version = "0.1.2"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "f31827a206f56af32e590ba56d5d2d085f558508192593743f16b2306495269f"
+dependencies = [
+    "cc",
+]
+
+[[package]]
+name = "indexmap"
+version = "2.2.6"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "168fb715dda47215e360912c096649d23d58bf392ac62f73919e831745e40f26"
+dependencies = [
+    "equivalent",
+    "hashbrown 0.14.5",
+]

----------------------------------------

File: core/rust/qdb-sqllogictest/Cargo.toml
Status: added
Changes: +18 -0
Diff:
@@ -0,0 +1,18 @@
+[package]
+name = "qdbr-sqllogictest"
+version = "0.1.0"
+edition = "2021"
+
+[lib]
+name = "qdbsqllogictest"
+crate-type = ["dylib"]
+

----------------------------------------

File: core/rust/qdb-sqllogictest/README.md
Status: added
Changes: +27 -0
Diff:
@@ -0,0 +1,27 @@
+# SqlLogicTest QuestDB Rust JNI bindings
+
+This project contains modified sources of [sqllogictest-rs](https://github.com/risinglightdb/sqllogictest-rs) and JNI bindings
+to execute sqllogic tests format files as Java unit tests.
+
+### Usage
+
+The output of this project is used in Java unit tests, see `io.questdb.test.sqllogictest.parquet`.
+

----------------------------------------

File: core/rust/qdb-sqllogictest/sqllogictest-engines/Cargo.toml
Status: added
Changes: +29 -0
Diff:
@@ -0,0 +1,29 @@
+[package]
+name = "sqllogictest-engines"
+version = "0.21.0"
+edition = "2021"
+description = "Sqllogictest built-in engines."
+
+[dependencies]
+async-trait = "0.1"
+bytes = "1"

----------------------------------------

File: core/rust/qdb-sqllogictest/sqllogictest-engines/LICENSE-APACHE
Status: added
Changes: +176 -0
Diff:
@@ -0,0 +1,176 @@
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but

----------------------------------------

File: core/rust/qdb-sqllogictest/sqllogictest-engines/LICENSE-MIT
Status: added
Changes: +17 -0
Diff:
@@ -0,0 +1,17 @@
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.

----------------------------------------

File: core/rust/qdb-sqllogictest/sqllogictest-engines/src/external.rs
Status: added
Changes: +157 -0
Diff:
@@ -0,0 +1,157 @@
+use std::io;
+use std::marker::PhantomData;
+use std::process::Stdio;
+use std::time::Duration;
+
+use async_trait::async_trait;
+use bytes::{Buf, BytesMut};
+use futures::StreamExt;
+use serde::{Deserialize, Serialize};
+use sqllogictest::{AsyncDB, DBOutput, DefaultColumnType};
+use thiserror::Error;
+use tokio::io::AsyncWriteExt;
+use tokio::process::{Child, ChildStdin, ChildStdout, Command};
+use tokio_util::codec::{Decoder, FramedRead};
+
+/// Communicates with a subprocess via its stdin/stdout.
+///
+/// # Protocol
+///
+/// Sends JSON stream:
+/// ```json
+/// {"sql":"SELECT 1,2"}
+/// ```
+///
+/// Receives JSON stream:
+///
+/// If the query succeeds:
+/// ```json
+/// {"result":[["1","2"]]}
+/// ```
+///
+/// If the query fails:
+/// ```json
+/// {"err":"..."}
+/// ```
+pub struct ExternalDriver {
+    child: Child,
+    stdin: ChildStdin,
+    stdout: FramedRead<ChildStdout, JsonDecoder<Output>>,
+}
+
+#[derive(Serialize)]
+struct Input {
+    sql: String,
+}
+
+#[derive(Deserialize)]
+#[serde(untagged)]
+enum Output {
+    Success { result: Vec<Vec<String>> },
+    Failed { err: String },

----------------------------------------

File: core/rust/qdb-sqllogictest/sqllogictest-engines/src/lib.rs
Status: added
Changes: +2 -0
Diff:
@@ -0,0 +1,2 @@
+pub mod external;
+pub mod postgres;

----------------------------------------

File: core/rust/qdb-sqllogictest/sqllogictest-engines/src/postgres.rs
Status: added
Changes: +60 -0
Diff:
@@ -0,0 +1,60 @@
+mod extended;
+mod simple;
+
+use std::marker::PhantomData;
+use std::sync::Arc;
+
+use tokio::task::JoinHandle;
+
+type Result<T> = std::result::Result<T, tokio_postgres::Error>;
+
+/// Marker type for the Postgres simple query protocol.
+pub struct Simple;
+/// Marker type for the Postgres extended query protocol.
+pub struct Extended;
+
+/// Generic Postgres engine based on the client from [`tokio_postgres`]. The protocol `P` can be
+/// either [`Simple`] or [`Extended`].
+pub struct Postgres<P> {
+    client: Arc<tokio_postgres::Client>,

----------------------------------------

File: core/rust/qdb-sqllogictest/sqllogictest-engines/src/postgres/extended.rs
Status: added
Changes: +375 -0
Diff:
@@ -0,0 +1,375 @@
+use std::fmt::Write;
+use std::process::Command;
+use std::time::Duration;
+
+use async_trait::async_trait;
+use chrono::{DateTime, NaiveDate, NaiveDateTime, NaiveTime};
+use futures::{pin_mut, StreamExt};
+use pg_interval::Interval;
+use postgres_types::{ToSql, Type};
+use rust_decimal::Decimal;
+use sqllogictest::{DBOutput, DefaultColumnType};
+
+use super::{Extended, Postgres, Result};
+
+macro_rules! array_process {
+    ($row:ident, $row_vec:ident, $idx:ident, $t:ty) => {
+        let value: Option<Vec<Option<$t>>> = $row.get($idx);
+        match value {
+            Some(value) => {
+                let mut output = String::new();
+                write!(output, "{{").unwrap();
+                for (i, v) in value.iter().enumerate() {
+                    match v {
+                        Some(v) => {
+                            write!(output, "{}", v).unwrap();
+                        }
+                        None => {
+                            write!(output, "NULL").unwrap();
+                        }
+                    }
+                    if i < value.len() - 1 {
+                        write!(output, ",").unwrap();
+                    }
+                }
+                write!(output, "}}").unwrap();
+                $row_vec.push(output);
+            }
+            None => {
+                $row_vec.push("NULL".to_string());
+            }
+        }
+    };
+    ($row:ident, $row_vec:ident, $idx:ident, $t:ty, $convert:ident) => {
+        let value: Option<Vec<Option<$t>>> = $row.get($idx);
+        match value {
+            Some(value) => {
+                let mut output = String::new();
+                write!(output, "{{").unwrap();
+                for (i, v) in value.iter().enumerate() {
+                    match v {
+                        Some(v) => {
+                            write!(output, "{}", $convert(v)).unwrap();
+                        }
+                        None => {
+                            write!(output, "NULL").unwrap();
+                        }
+                    }
+                    if i < value.len() - 1 {
+                        write!(output, ",").unwrap();
+                    }
+                }
+                write!(output, "}}").unwrap();
+                $row_vec.push(output);
+            }
+            None => {
+                $row_vec.push("NULL".to_string());
+            }
+        }
+    };
+    ($self:ident, $row:ident, $row_vec:ident, $idx:ident, $t:ty, $ty_name:expr) => {
+        let value: Option<Vec<Option<$t>>> = $row.get($idx);
+        match value {
+            Some(value) => {
+                let mut output = String::new();
+                write!(output, "{{").unwrap();
+                for (i, v) in value.iter().enumerate() {
+                    match v {
+                        Some(v) => {
+                            let sql = format!("select ($1::{})::varchar", stringify!($ty_name));
+                            let tmp_rows = $self.client.query(&sql, &[&v]).await.unwrap();
+                            let value: &str = tmp_rows.get(0).unwrap().get(0);
+                            assert!(value.len() > 0);
+                            write!(output, "{}", value).unwrap();
+                        }
+                        None => {
+                            write!(output, "NULL").unwrap();
+                        }
+                    }
+                    if i < value.len() - 1 {
+                        write!(output, ",").unwrap();
+                    }
+                }
+                write!(output, "}}").unwrap();
+                $row_vec.push(output);
+            }
+            None => {
+                $row_vec.push("NULL".to_string());
+            }
+        }
+    };
+}
+
+macro_rules! single_process {
+    ($row:ident, $row_vec:ident, $idx:ident, $t:ty) => {
+        let value: Option<$t> = $row.get($idx);
+        match value {
+            Some(value) => {
+                $row_vec.push(value.to_string());
+            }
+            None => {
+                $row_vec.push("NULL".to_string());
+            }
+        }
+    };
+    ($row:ident, $row_vec:ident, $idx:ident, $t:ty, $convert:ident) => {
+        let value: Option<$t> = $row.get($idx);
+        match value {
+            Some(value) => {
+                $row_vec.push($convert(&value).to_string());
+            }
+            None => {
+                $row_vec.push("NULL".to_string());
+            }
+        }

----------------------------------------

File: core/rust/qdb-sqllogictest/sqllogictest-engines/src/postgres/postgres_extended_test.slt
Status: added
Changes: +107 -0
Diff:
@@ -0,0 +1,107 @@
+# postgres_extended engine support following type: 
+# NOTE: array only support one dimension
+# int2
+# int2 array
+# int4 
+# int4 array
+# int8
+# int8 array 
+# float4 
+# float4 array
+# float8
+# float8 array
+# numeric (not include: NaN,+Inf,-Inf)
+# numeric array
+# varchar 
+# varchar array
+# date 
+# date array
+# time 
+# time array
+# timestamp
+# timestamp array
+# timestamptz
+# timestamptz array
+# interval
+# interval array
+# bool
+# bool array
+
+# int2 && int2 array 
+query I 
+select 1::int2, '{1,2,3}'::int2[]
+----
+1 {1,2,3}
+

----------------------------------------

File: core/rust/qdb-sqllogictest/sqllogictest-engines/src/postgres/simple.rs
Status: added
Changes: +72 -0
Diff:
@@ -0,0 +1,72 @@
+use std::process::Command;
+use std::time::Duration;
+
+use async_trait::async_trait;
+use sqllogictest::{DBOutput, DefaultColumnType};
+
+use super::{Postgres, Result, Simple};
+
+#[async_trait]
+impl sqllogictest::AsyncDB for Postgres<Simple> {
+    type Error = tokio_postgres::error::Error;
+    type ColumnType = DefaultColumnType;
+
+    async fn run(&mut self, sql: &str) -> Result<DBOutput<Self::ColumnType>> {
+        let mut output = vec![];
+
+        // NOTE:
+        // We use `simple_query` API which returns the query results as strings.
+        // This means that we can not reformat values based on their type,
+        // and we have to follow the format given by the specific database (pg).
+        // For example, postgres will output `t` as true and `f` as false,
+        // thus we have to write `t`/`f` in the expected results.
+        let rows = self.client.simple_query(sql).await?;

----------------------------------------

File: core/rust/qdb-sqllogictest/sqllogictest/Cargo.lock
Status: added
Changes: +786 -0
Diff:
@@ -0,0 +1,786 @@
+# This file is automatically @generated by Cargo.
+# It is not intended for manual editing.
+version = 3
+
+[[package]]
+name = "aho-corasick"
+version = "1.1.3"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "8e60d3430d3a69478ad0993f19238d2df97c507009a52b3c10addcd7f6bcb916"
+dependencies = [
+ "memchr",
+]
+
+[[package]]
+name = "anstream"
+version = "0.6.14"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "418c75fa768af9c03be99d17643f93f79bbba589895012a80e3452a19ddda15b"
+dependencies = [
+ "anstyle",
+ "anstyle-parse",
+ "anstyle-query",
+ "anstyle-wincon",
+ "colorchoice",
+ "is_terminal_polyfill",
+ "utf8parse",
+]
+
+[[package]]
+name = "anstyle"
+version = "1.0.7"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "038dfcf04a5feb68e9c60b21c9625a54c2c0616e79b72b0fd87075a056ae1d1b"
+
+[[package]]
+name = "anstyle-parse"
+version = "0.2.4"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "c03a11a9034d92058ceb6ee011ce58af4a9bf61491aa7e1e59ecd24bd40d22d4"
+dependencies = [
+ "utf8parse",
+]
+
+[[package]]
+name = "anstyle-query"
+version = "1.1.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "ad186efb764318d35165f1758e7dcef3b10628e26d41a44bc5550652e6804391"
+dependencies = [
+ "windows-sys",
+]
+
+[[package]]
+name = "anstyle-wincon"
+version = "3.0.3"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "61a38449feb7068f52bb06c12759005cf459ee52bb4adc1d5a7c4322d716fb19"
+dependencies = [
+ "anstyle",
+ "windows-sys",
+]
+
+[[package]]
+name = "async-trait"
+version = "0.1.81"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "6e0c28dcc82d7c8ead5cb13beb15405b57b8546e93215673ff8ca0349a028107"
+dependencies = [
+ "proc-macro2",
+ "quote",
+ "syn",
+]
+
+[[package]]
+name = "autocfg"
+version = "1.3.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "0c4b4d0bd25bd0b74681c0ad21497610ce1b7c91b1022cd21c80c6fbdd9476b0"
+
+[[package]]
+name = "bitflags"
+version = "2.6.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "b048fb63fd8b5923fc5aa7b340d8e156aec7ec02f0c78fa8a6ddc2613f6f71de"
+
+[[package]]
+name = "block-buffer"
+version = "0.10.4"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "3078c7629b62d3f0439517fa394996acacc5cbc91c5a20d8c658e77abd503a71"
+dependencies = [
+ "generic-array",
+]
+
+[[package]]
+name = "cfg-if"
+version = "1.0.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "baf1de4339761588bc0619e3cbc0120ee582ebb74b53b4efbf79117bd2da40fd"
+
+[[package]]
+name = "clap"
+version = "4.5.9"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "64acc1846d54c1fe936a78dc189c34e28d3f5afc348403f28ecf53660b9b8462"
+dependencies = [
+ "clap_builder",
+ "clap_derive",
+]
+
+[[package]]
+name = "clap_builder"
+version = "4.5.9"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "6fb8393d67ba2e7bfaf28a23458e4e2b543cc73a99595511eb207fdb8aede942"
+dependencies = [
+ "anstream",
+ "anstyle",
+ "clap_lex",
+ "strsim",
+]
+
+[[package]]
+name = "clap_derive"
+version = "4.5.8"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "2bac35c6dafb060fd4d275d9a4ffae97917c13a6327903a8be2153cd964f7085"
+dependencies = [
+ "heck",
+ "proc-macro2",
+ "quote",
+ "syn",
+]
+
+[[package]]
+name = "clap_lex"
+version = "0.7.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "4b82cf0babdbd58558212896d1a4272303a57bdb245c2bf1147185fb45640e70"
+
+[[package]]
+name = "colorchoice"
+version = "1.0.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "0b6a852b24ab71dffc585bcb46eaf7959d175cb865a7152e35b348d1b2960422"
+
+[[package]]
+name = "crypto-common"
+version = "0.1.6"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "1bfb12502f3fc46cca1bb51ac28df9d618d813cdc3d2f25b9fe775a34af26bb3"
+dependencies = [
+ "generic-array",
+ "typenum",
+]
+
+[[package]]
+name = "diff"
+version = "0.1.13"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "56254986775e3233ffa9c4d7d3faaf6d36a2c09d30b20687e9f88bc8bafc16c8"
+
+[[package]]
+name = "digest"
+version = "0.10.7"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "9ed9a281f7bc9b7576e61468ba615a66a5c8cfdff42420a70aa82701a3b1e292"
+dependencies = [
+ "block-buffer",
+ "crypto-common",
+]
+
+[[package]]
+name = "educe"
+version = "0.6.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "1d7bc049e1bd8cdeb31b68bbd586a9464ecf9f3944af3958a7a9d0f8b9799417"
+dependencies = [
+ "enum-ordinalize",
+ "proc-macro2",
+ "quote",
+ "syn",
+]
+
+[[package]]
+name = "either"
+version = "1.13.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "60b1af1c220855b6ceac025d3f6ecdd2b7c4894bfe9cd9bda4fbb4bc7c0d4cf0"
+
+[[package]]
+name = "enum-ordinalize"
+version = "4.3.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "fea0dcfa4e54eeb516fe454635a95753ddd39acda650ce703031c6973e315dd5"
+dependencies = [
+ "enum-ordinalize-derive",
+]
+
+[[package]]
+name = "enum-ordinalize-derive"
+version = "4.3.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "0d28318a75d4aead5c4db25382e8ef717932d0346600cacae6357eb5941bc5ff"
+dependencies = [
+ "proc-macro2",
+ "quote",
+ "syn",
+]
+
+[[package]]
+name = "errno"
+version = "0.3.9"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "534c5cf6194dfab3db3242765c03bbe257cf92f22b38f6bc0c58d59108a820ba"
+dependencies = [
+ "libc",
+ "windows-sys",
+]
+
+[[package]]
+name = "escape8259"
+version = "0.5.3"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "5692dd7b5a1978a5aeb0ce83b7655c58ca8efdcb79d21036ea249da95afec2c6"
+
+[[package]]
+name = "fastrand"
+version = "2.1.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "9fc0510504f03c51ada170672ac806f1f105a88aa97a5281117e1ddc3368e51a"
+
+[[package]]
+name = "fs-err"
+version = "2.11.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "88a41f105fe1d5b6b34b2055e3dc59bb79b46b48b2040b9e6c7b4b5de097aa41"
+dependencies = [
+ "autocfg",
+]
+
+[[package]]
+name = "futures"
+version = "0.3.30"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "645c6916888f6cb6350d2550b80fb63e734897a8498abe35cfb732b6487804b0"
+dependencies = [
+ "futures-channel",
+ "futures-core",
+ "futures-executor",
+ "futures-io",
+ "futures-sink",
+ "futures-task",
+ "futures-util",
+]
+
+[[package]]
+name = "futures-channel"
+version = "0.3.30"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "eac8f7d7865dcb88bd4373ab671c8cf4508703796caa2b1985a9ca867b3fcb78"

----------------------------------------

File: core/rust/qdb-sqllogictest/sqllogictest/Cargo.toml
Status: added
Changes: +30 -0
Diff:
@@ -0,0 +1,30 @@
+[package]
+name = "sqllogictest"
+version = "0.21.0"
+description = "Sqllogictest parser and runner."
+edition = "2021"
+
+[lib]
+name = "sqllogictest"
+bench = false

----------------------------------------

File: core/rust/qdb-sqllogictest/sqllogictest/README.md
Status: added
Changes: +8 -0
Diff:
@@ -0,0 +1,8 @@
+# Sqllogictest
+
+[Sqllogictest][Sqllogictest] is a testing framework to verify the correctness of an SQL database.
+See [GitHub Homepage](https://github.com/risinglightdb/sqllogictest-rs/) for more information.
+
+This crate implements a sqllogictest parser and runner library in Rust.
+
+[Sqllogictest]: https://www.sqlite.org/sqllogictest/doc/trunk/about.wiki

----------------------------------------

File: core/rust/qdb-sqllogictest/sqllogictest/src/column_type.rs
Status: added
Changes: +43 -0
Diff:
@@ -0,0 +1,43 @@
+use std::fmt::Debug;
+
+/// This trait represents an Sqllogictest column type.
+/// An Sqllogictest column is represented with a single character.
+/// The type has to be serializable to a character.
+pub trait ColumnType: Debug + PartialEq + Eq + Clone + Send + Sync {
+    fn from_char(value: char) -> Option<Self>;
+    fn to_char(&self) -> char;
+}
+
+/// The default Sqllogictest type.
+/// The valid types are:
+/// - 'T' - text, varchar results

----------------------------------------

File: core/rust/qdb-sqllogictest/sqllogictest/src/connection.rs
Status: added
Changes: +71 -0
Diff:
@@ -0,0 +1,71 @@
+use std::collections::HashMap;
+use std::future::IntoFuture;
+
+use futures::Future;
+
+use crate::{AsyncDB, Connection as ConnectionName, DBOutput};
+
+/// Trait for making connections to an [`AsyncDB`].
+///
+/// This is introduced to allow querying the database with different connections
+/// (then generally different sessions) in a single test file with `connection` records.
+pub trait MakeConnection {
+    /// The database type.
+    type Conn: AsyncDB;
+    /// The future returned by [`MakeConnection::make`].
+    type MakeFuture: Future<Output = Result<Self::Conn, <Self::Conn as AsyncDB>::Error>>;
+
+    /// Creates a new connection to the database.
+    fn make(&mut self) -> Self::MakeFuture;
+}
+
+/// Make connections directly from a closure returning a future.
+impl<D: AsyncDB, F, Fut> MakeConnection for F

----------------------------------------

File: core/rust/qdb-sqllogictest/sqllogictest/src/harness.rs
Status: added
Changes: +39 -0
Diff:
@@ -0,0 +1,39 @@
+use std::path::Path;
+
+pub use glob::glob;
+pub use libtest_mimic::{run, Arguments, Failed, Trial};
+
+use crate::{MakeConnection, Runner};
+
+/// * `db_fn`: `fn() -> sqllogictest::AsyncDB`
+/// * `pattern`: The glob used to match against and select each file to be tested. It is relative to
+///   the root of the crate.
+#[macro_export]
+macro_rules! harness {

----------------------------------------

File: core/rust/qdb-sqllogictest/sqllogictest/src/lib.rs
Status: added
Changes: +63 -0
Diff:
@@ -0,0 +1,63 @@
+//! [Sqllogictest][Sqllogictest] parser and runner.
+//!
+//! This crate supports multiple extensions beyond the original sqllogictest format.
+//! See the [README](https://github.com/risinglightdb/sqllogictest-rs#slt-test-file-format-cookbook) for more information.
+//!
+//! [Sqllogictest]: https://www.sqlite.org/sqllogictest/doc/trunk/about.wiki
+//!
+//! # Usage
+//!
+//! For how to use the CLI tool backed by this library, see the [README](https://github.com/risinglightdb/sqllogictest-rs#use-the-cli-tool).
+//!
+//! For using the crate as a lib, and implement your custom driver, see below.
+//!
+//! Implement [`DB`] trait for your database structure:
+//!
+//! ```
+//! struct MyDatabase {
+//!     // fields
+//! }
+//!

----------------------------------------

File: core/rust/qdb-sqllogictest/sqllogictest/src/parser.rs
Status: added
Changes: +1147 -0
Diff:
@@ -0,0 +1,1147 @@
+//! Sqllogictest parser.
+
+use std::fmt;
+use std::iter::Peekable;
+use std::path::Path;
+use std::sync::Arc;
+use std::time::Duration;
+
+use itertools::Itertools;
+use regex::Regex;
+
+use crate::ColumnType;
+
+const RESULTS_DELIMITER: &str = "----";
+
+/// The location in source file.
+#[derive(Debug, PartialEq, Eq, Clone)]
+pub struct Location {
+    file: Arc<str>,
+    line: u32,
+    upper: Option<Arc<Location>>,
+}
+
+impl fmt::Display for Location {
+    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
+        write!(f, "{}:{}", self.file, self.line)?;
+        if let Some(upper) = &self.upper {
+            write!(f, "\nat {upper}")?;
+        }
+        Ok(())
+    }
+}
+
+impl Location {
+    /// File path.
+    pub fn file(&self) -> &str {
+        &self.file
+    }
+
+    /// Line number.
+    pub fn line(&self) -> u32 {
+        self.line
+    }
+
+    fn new(file: impl Into<Arc<str>>, line: u32) -> Self {
+        Self {
+            file: file.into(),
+            line,
+            upper: None,
+        }
+    }
+
+    /// Returns the location of next line.
+    #[must_use]
+    fn next_line(mut self) -> Self {
+        self.line += 1;
+        self
+    }
+
+    /// Returns the location of next level file.
+    fn include(&self, file: &str) -> Self {
+        Self {
+            file: file.into(),
+            line: 0,
+            upper: Some(Arc::new(self.clone())),
+        }
+    }
+}
+
+/// Expectation for a statement.
+#[derive(Debug, Clone, PartialEq)]
+pub enum StatementExpect {
+    /// Statement should succeed.
+    Ok,
+    /// Statement should succeed and affect the given number of rows.
+    Count(u64),
+    /// Statement should fail with the given error message.
+    Error(ExpectedError),
+}
+
+/// Expectation for a query.
+#[derive(Debug, Clone, PartialEq)]
+pub enum QueryExpect<T: ColumnType> {
+    /// Query should succeed and return the given results.
+    Results {
+        types: Vec<T>,
+        sort_mode: Option<SortMode>,
+        label: Option<String>,
+        results: Vec<String>,
+    },
+    /// Query should fail with the given error message.
+    Error(ExpectedError),
+}
+
+impl<T: ColumnType> QueryExpect<T> {
+    /// Creates a new [`QueryExpect`] with empty results.
+    fn empty_results() -> Self {
+        Self::Results {
+            types: Vec::new(),
+            sort_mode: None,
+            label: None,
+            results: Vec::new(),
+        }
+    }
+}
+
+/// A single directive in a sqllogictest file.
+#[derive(Debug, Clone, PartialEq)]
+#[non_exhaustive]
+pub enum Record<T: ColumnType> {
+    /// An include copies all records from another files.
+    Include {
+        loc: Location,
+        /// A glob pattern
+        filename: String,
+    },
+    /// A statement is an SQL command that is to be evaluated but from which we do not expect to
+    /// get results (other than success or failure).
+    Statement {
+        loc: Location,
+        conditions: Vec<Condition>,
+        connection: Connection,
+        /// The SQL command.
+        sql: String,
+        expected: StatementExpect,
+    },
+    /// A query is an SQL command from which we expect to receive results. The result set might be
+    /// empty.
+    Query {
+        loc: Location,
+        conditions: Vec<Condition>,
+        connection: Connection,
+        /// The SQL command.
+        sql: String,
+        expected: QueryExpect<T>,
+    },
+    /// A system command is an external command that is to be executed by the shell. Currently it
+    /// must succeed and the output is ignored.
+    #[non_exhaustive]
+    System {
+        loc: Location,
+        conditions: Vec<Condition>,
+        /// The external command.
+        command: String,
+        stdout: Option<String>,
+    },
+    /// A sleep period.
+    Sleep {
+        loc: Location,
+        duration: Duration,
+    },
+    /// Subtest.
+    Subtest {
+        loc: Location,
+        name: String,
+    },
+    /// A halt record merely causes sqllogictest to ignore the rest of the test script.
+    /// For debugging use only.
+    Halt {
+        loc: Location,
+    },
+    /// Control statements.
+    Control(Control),
+    /// Set the maximum number of result values that will be accepted
+    /// for a query.  If the number of result values exceeds this number,
+    /// then an MD5 hash is computed of all values, and the resulting hash
+    /// is the only result.
+    ///
+    /// If the threshold is 0, then hashing is never used.
+    HashThreshold {
+        loc: Location,
+        threshold: u64,
+    },
+    /// Condition statements, including `onlyif` and `skipif`.
+    Condition(Condition),
+    /// Connection statements to specify the connection to use for the following statement.
+    Connection(Connection),
+    Comment(Vec<String>),
+    Newline,
+    /// Internally injected record which should not occur in the test file.
+    Injected(Injected),
+}
+
+impl<T: ColumnType> Record<T> {
+    /// Unparses the record to its string representation in the test file.
+    ///
+    /// # Panics
+    /// If the record is an internally injected record which should not occur in the test file.
+    pub fn unparse(&self, w: &mut impl std::io::Write) -> std::io::Result<()> {
+        write!(w, "{self}")
+    }
+}
+
+/// As is the standard for Display, does not print any trailing
+/// newline except for records that always end with a blank line such
+/// as Query and Statement.
+impl<T: ColumnType> std::fmt::Display for Record<T> {
+    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
+        match self {
+            Record::Include { loc: _, filename } => {
+                write!(f, "include {filename}")
+            }
+            Record::Statement {
+                loc: _,
+                conditions: _,
+                connection: _,
+                sql,
+                expected,
+            } => {
+                write!(f, "statement ")?;
+                match expected {
+                    StatementExpect::Ok => write!(f, "ok")?,
+                    StatementExpect::Count(cnt) => write!(f, "count {cnt}")?,
+                    StatementExpect::Error(err) => err.fmt_inline(f)?,
+                }
+                writeln!(f)?;
+                // statement always end with a blank line
+                writeln!(f, "{sql}")?;
+
+                if let StatementExpect::Error(err) = expected {
+                    err.fmt_multiline(f)?;
+                }
+                Ok(())
+            }
+            Record::Query {
+                loc: _,
+                conditions: _,
+                connection: _,
+                sql,
+                expected,
+            } => {
+                write!(f, "query ")?;
+                match expected {
+                    QueryExpect::Results {
+                        types,
+                        sort_mode,
+                        label,
+                        ..
+                    } => {
+                        write!(f, "{}", types.iter().map(|c| c.to_char()).join(""))?;
+                        if let Some(sort_mode) = sort_mode {
+                            write!(f, " {}", sort_mode.as_str())?;
+                        }
+                        if let Some(label) = label {
+                            write!(f, " {label}")?;
+                        }
+                    }
+                    QueryExpect::Error(err) => err.fmt_inline(f)?,
+                }
+                writeln!(f)?;
+                writeln!(f, "{sql}")?;
+
+                match expected {
+                    QueryExpect::Results { results, .. } => {
+                        write!(f, "{}", RESULTS_DELIMITER)?;
+
+                        for result in results {
+                            write!(f, "\n{result}")?;
+                        }
+                        // query always ends with a blank line
+                        writeln!(f)?
+                    }
+                    QueryExpect::Error(err) => err.fmt_multiline(f)?,
+                }
+                Ok(())
+            }
+            Record::System {
+                loc: _,
+                conditions: _,
+                command,
+                stdout,
+            } => {
+                writeln!(f, "system ok\n{command}")?;
+                if let Some(stdout) = stdout {
+                    writeln!(f, "----\n{}\n", stdout.trim())?;
+                }
+                Ok(())
+            }
+            Record::Sleep { loc: _, duration } => {
+                write!(f, "sleep {}", humantime::format_duration(*duration))
+            }
+            Record::Subtest { loc: _, name } => {
+                write!(f, "subtest {name}")
+            }
+            Record::Halt { loc: _ } => {
+                write!(f, "halt")
+            }
+            Record::Control(c) => match c {
+                Control::SortMode(m) => write!(f, "control sortmode {}", m.as_str()),
+                Control::Substitution(s) => write!(f, "control substitution {}", s.as_str()),
+            },
+            Record::Condition(cond) => match cond {
+                Condition::OnlyIf { label } => write!(f, "onlyif {label}"),
+                Condition::SkipIf { label } => write!(f, "skipif {label}"),
+            },
+            Record::Connection(conn) => {
+                if let Connection::Named(conn) = conn {
+                    write!(f, "connection {}", conn)?;
+                }
+                Ok(())
+            }
+            Record::HashThreshold { loc: _, threshold } => {
+                write!(f, "hash-threshold {threshold}")
+            }
+            Record::Comment(comment) => {
+                let mut iter = comment.iter();
+                write!(f, "#{}", iter.next().unwrap().trim_end())?;
+                for line in iter {
+                    write!(f, "\n#{}", line.trim_end())?;
+                }
+                Ok(())
+            }
+            Record::Newline => Ok(()), // Display doesn't end with newline
+            Record::Injected(p) => panic!("unexpected injected record: {p:?}"),
+        }
+    }
+}
+
+/// Expected error message after `error` or under `----`.
+#[derive(Debug, Clone)]
+pub enum ExpectedError {
+    /// No expected error message.
+    ///
+    /// Any error message is considered as a match.
+    Empty,
+    /// An inline regular expression after `error`.
+    ///
+    /// The actual error message that matches the regex is considered as a match.
+    Inline(Regex),
+    /// A multiline error message under `----`, ends with 2 consecutive empty lines.
+    ///
+    /// The actual error message that's exactly the same as the expected one is considered as a
+    /// match.
+    Multiline(String),
+}
+
+impl ExpectedError {
+    /// Parses an inline regex variant from tokens.
+    fn parse_inline_tokens(tokens: &[&str]) -> Result<Self, ParseErrorKind> {
+        Self::new_inline(tokens.join(" "))
+    }
+
+    /// Creates an inline expected error message from a regex string.
+    ///
+    /// If the regex is empty, it's considered as [`ExpectedError::Empty`].
+    fn new_inline(regex: String) -> Result<Self, ParseErrorKind> {
+        if regex.is_empty() {
+            Ok(Self::Empty)
+        } else {
+            let regex =
+                Regex::new(&regex).map_err(|_| ParseErrorKind::InvalidErrorMessage(regex))?;
+            Ok(Self::Inline(regex))
+        }
+    }
+
+    /// Returns whether it's an empty match.
+    fn is_empty(&self) -> bool {
+        matches!(self, Self::Empty)
+    }
+
+    /// Unparses the expected message after `statement`.
+    fn fmt_inline(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
+        write!(f, "error")?;
+        if let Self::Inline(regex) = self {
+            write!(f, " {regex}")?;
+        }
+        Ok(())
+    }
+
+    /// Unparses the expected message with `----`, if it's multiline.
+    fn fmt_multiline(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
+        if let Self::Multiline(results) = self {
+            writeln!(f, "{}", RESULTS_DELIMITER)?;
+            writeln!(f, "{}", results.trim())?;
+            writeln!(f)?; // another empty line to indicate the end of multiline message
+        }
+        Ok(())
+    }
+
+    /// Returns whether the given error message matches the expected one.
+    pub fn is_match(&self, err: &str) -> bool {

----------------------------------------

File: core/rust/qdb-sqllogictest/sqllogictest/src/runner.rs
Status: added
Changes: +2059 -0
Diff:
@@ -0,0 +1,2059 @@
+//! Sqllogictest runner.
+
+use std::collections::HashSet;
+use std::fmt::{Debug, Display};
+use std::path::Path;
+use std::process::{Command, ExitStatus, Output};
+use std::sync::Arc;
+use std::time::Duration;
+use std::vec;
+
+use async_trait::async_trait;
+use futures::executor::block_on;
+use futures::{stream, Future, FutureExt, StreamExt};
+use itertools::Itertools;
+use md5::Digest;
+use owo_colors::OwoColorize;
+use similar::{Change, ChangeTag, TextDiff};
+
+use crate::parser::*;
+use crate::substitution::Substitution;
+use crate::{ColumnType, Connections, MakeConnection};
+
+/// Type-erased error type.
+type AnyError = Arc<dyn std::error::Error + Send + Sync>;
+
+/// Output of a record.
+#[derive(Debug, Clone)]
+#[non_exhaustive]
+pub enum RecordOutput<T: ColumnType> {
+    /// No output. Occurs when the record is skipped or not a `query`, `statement`, or `system`
+    /// command.
+    Nothing,
+    /// The output of a `query`.
+    Query {
+        types: Vec<T>,
+        rows: Vec<Vec<String>>,
+        error: Option<AnyError>,
+    },
+    /// The output of a `statement`.
+    Statement { count: u64, error: Option<AnyError> },
+    /// The output of a `system` command.
+    #[non_exhaustive]
+    System {
+        stdout: Option<String>,
+        error: Option<AnyError>,
+    },
+}
+
+#[non_exhaustive]
+pub enum DBOutput<T: ColumnType> {
+    Rows {
+        types: Vec<T>,
+        rows: Vec<Vec<String>>,
+    },
+    /// A statement in the query has completed.
+    ///
+    /// The number of rows modified or selected is returned.
+    ///
+    /// If the test case doesn't specify `statement count <n>`, the number is simply ignored.
+    StatementComplete(u64),
+}
+
+/// The async database to be tested.
+#[async_trait]
+pub trait AsyncDB {
+    /// The error type of SQL execution.
+    type Error: std::error::Error + Send + Sync + 'static;
+    /// The type of result columns
+    type ColumnType: ColumnType;
+
+    /// Async run a SQL query and return the output.
+    async fn run(&mut self, sql: &str) -> Result<DBOutput<Self::ColumnType>, Self::Error>;
+
+    /// Engine name of current database.
+    fn engine_name(&self) -> &str {
+        ""
+    }
+
+    /// [`Runner`] calls this function to perform sleep.
+    ///
+    /// The default implementation is `std::thread::sleep`, which is universal to any async runtime
+    /// but would block the current thread. If you are running in tokio runtime, you should override
+    /// this by `tokio::time::sleep`.
+    async fn sleep(dur: Duration) {
+        std::thread::sleep(dur);
+    }
+
+    /// [`Runner`] calls this function to run a system command.
+    ///
+    /// The default implementation is `std::process::Command::output`, which is universal to any
+    /// async runtime but would block the current thread. If you are running in tokio runtime, you
+    /// should override this by `tokio::process::Command::output`.
+    async fn run_command(mut command: Command) -> std::io::Result<std::process::Output> {
+        command.output()
+    }
+}
+
+/// The database to be tested.
+pub trait DB {
+    /// The error type of SQL execution.
+    type Error: std::error::Error + Send + Sync + 'static;
+    /// The type of result columns
+    type ColumnType: ColumnType;
+
+    /// Run a SQL query and return the output.
+    fn run(&mut self, sql: &str) -> Result<DBOutput<Self::ColumnType>, Self::Error>;
+
+    /// Engine name of current database.
+    fn engine_name(&self) -> &str {
+        ""
+    }
+}
+
+/// Compat-layer for the new AsyncDB and DB trait
+#[async_trait]
+impl<D> AsyncDB for D
+where
+    D: DB + Send,
+{
+    type Error = D::Error;
+    type ColumnType = D::ColumnType;
+
+    async fn run(&mut self, sql: &str) -> Result<DBOutput<Self::ColumnType>, Self::Error> {
+        D::run(self, sql)
+    }
+
+    fn engine_name(&self) -> &str {
+        D::engine_name(self)
+    }
+}
+
+/// The error type for running sqllogictest.
+///
+/// For colored error message, use `self.display()`.
+#[derive(thiserror::Error, Clone)]
+#[error("{kind}\nat {loc}\n")]
+pub struct TestError {
+    kind: TestErrorKind,
+    loc: Location,
+}
+
+impl TestError {
+    pub fn display(&self, colorize: bool) -> TestErrorDisplay<'_> {
+        TestErrorDisplay {
+            err: self,
+            colorize,
+        }
+    }
+}
+
+/// Overrides the `Display` implementation of [`TestError`] to support controlling colorization.
+pub struct TestErrorDisplay<'a> {
+    err: &'a TestError,
+    colorize: bool,
+}
+
+impl<'a> Display for TestErrorDisplay<'a> {
+    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
+        write!(
+            f,
+            "{}\nat {}\n",
+            self.err.kind.display(self.colorize),
+            self.err.loc
+        )
+    }
+}
+
+/// For colored error message, use `self.display()`.
+#[derive(Clone, Debug, thiserror::Error)]
+pub struct ParallelTestError {
+    errors: Vec<TestError>,
+}
+
+impl Display for ParallelTestError {
+    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
+        writeln!(f, "parallel test failed")?;
+        write!(f, "Caused by:")?;
+        for i in &self.errors {
+            writeln!(f, "{i}")?;
+        }
+        Ok(())
+    }
+}
+
+/// Overrides the `Display` implementation of [`ParallelTestError`] to support controlling
+/// colorization.
+pub struct ParallelTestErrorDisplay<'a> {
+    err: &'a ParallelTestError,
+    colorize: bool,
+}
+
+impl<'a> Display for ParallelTestErrorDisplay<'a> {
+    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
+        writeln!(f, "parallel test failed")?;
+        write!(f, "Caused by:")?;
+        for i in &self.err.errors {
+            writeln!(f, "{}", i.display(self.colorize))?;
+        }
+        Ok(())
+    }
+}
+
+impl ParallelTestError {
+    pub fn display(&self, colorize: bool) -> ParallelTestErrorDisplay<'_> {
+        ParallelTestErrorDisplay {
+            err: self,
+            colorize,
+        }
+    }
+}
+
+impl std::fmt::Debug for TestError {
+    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
+        write!(f, "{self}")
+    }
+}
+
+impl TestError {
+    /// Returns the corresponding [`TestErrorKind`] for this error.
+    pub fn kind(&self) -> TestErrorKind {
+        self.kind.clone()
+    }
+
+    /// Returns the location from which the error originated.
+    pub fn location(&self) -> Location {
+        self.loc.clone()
+    }
+}
+
+#[derive(Debug, Clone)]
+pub enum RecordKind {
+    Statement,
+    Query,
+}
+
+impl std::fmt::Display for RecordKind {
+    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
+        match self {
+            RecordKind::Statement => write!(f, "statement"),
+            RecordKind::Query => write!(f, "query"),
+        }
+    }
+}
+
+/// The error kind for running sqllogictest.
+///
+/// For colored error message, use `self.display()`.
+#[derive(thiserror::Error, Debug, Clone)]
+#[non_exhaustive]
+pub enum TestErrorKind {
+    #[error("parse error: {0}")]
+    ParseError(ParseErrorKind),
+    #[error("{kind} is expected to fail, but actually succeed:\n[SQL] {sql}")]
+    Ok { sql: String, kind: RecordKind },
+    #[error("{kind} failed: {err}\n[SQL] {sql}")]
+    Fail {
+        sql: String,
+        err: AnyError,
+        kind: RecordKind,
+    },
+    #[error("system command failed: {err}\n[CMD] {command}")]
+    SystemFail { command: String, err: AnyError },
+    #[error(
+        "system command stdout mismatch:\n[command] {command}\n[Diff] (-expected|+actual)\n{}",
+        TextDiff::from_lines(.expected_stdout, .actual_stdout).iter_all_changes().format_with("\n", |diff, f| format_diff(&diff, f, false))
+    )]
+    SystemStdoutMismatch {
+        command: String,
+        expected_stdout: String,
+        actual_stdout: String,
+    },
+    // Remember to also update [`TestErrorKindDisplay`] if this message is changed.
+    #[error("{kind} is expected to fail with error:\n\t{expected_err}\nbut got error:\n\t{err}\n[SQL] {sql}")]
+    ErrorMismatch {
+        sql: String,
+        err: AnyError,
+        expected_err: String,
+        kind: RecordKind,
+    },
+    #[error("statement is expected to affect {expected} rows, but actually {actual}\n[SQL] {sql}")]
+    StatementResultMismatch {
+        sql: String,
+        expected: u64,
+        actual: String,
+    },
+    // Remember to also update [`TestErrorKindDisplay`] if this message is changed.
+    #[error(
+        "query result mismatch:\n[SQL] {sql}\n[Diff] (-expected|+actual)\n{}",
+        TextDiff::from_lines(.expected, .actual).iter_all_changes().format_with("\n", |diff, f| format_diff(&diff, f, false))
+    )]
+    QueryResultMismatch {
+        sql: String,
+        expected: String,
+        actual: String,
+    },
+    #[error(
+        "query columns mismatch:\n[SQL] {sql}\n{}",
+        format_column_diff(expected, actual, false)
+    )]
+    QueryResultColumnsMismatch {
+        sql: String,
+        expected: String,
+        actual: String,
+    },
+}
+
+impl From<ParseError> for TestError {
+    fn from(e: ParseError) -> Self {
+        TestError {
+            kind: TestErrorKind::ParseError(e.kind()),
+            loc: e.location(),
+        }
+    }
+}
+
+impl TestErrorKind {
+    fn at(self, loc: Location) -> TestError {
+        TestError { kind: self, loc }
+    }
+
+    pub fn display(&self, colorize: bool) -> TestErrorKindDisplay<'_> {
+        TestErrorKindDisplay {
+            error: self,
+            colorize,
+        }
+    }
+}
+
+/// Overrides the `Display` implementation of [`TestErrorKind`] to support controlling colorization.
+pub struct TestErrorKindDisplay<'a> {
+    error: &'a TestErrorKind,
+    colorize: bool,
+}
+
+impl<'a> Display for TestErrorKindDisplay<'a> {
+    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
+        if !self.colorize {
+            return write!(f, "{}", self.error);
+        }
+        match self.error {
+            TestErrorKind::ErrorMismatch {
+                sql,
+                err,
+                expected_err,
+                kind,
+            } => write!(
+                f,
+                "{kind} is expected to fail with error:\n\t{}\nbut got error:\n\t{}\n[SQL] {sql}",
+                expected_err.bright_green(),
+                err.bright_red(),
+            ),
+            TestErrorKind::QueryResultMismatch {
+                sql,
+                expected,
+                actual,
+            } => write!(
+                f,
+                "query result mismatch:\n[SQL] {sql}\n[Diff] ({}|{})\n{}",
+                "-expected".bright_red(),
+                "+actual".bright_green(),
+                TextDiff::from_lines(expected, actual)
+                    .iter_all_changes()
+                    .format_with("\n", |diff, f| format_diff(&diff, f, true))
+            ),
+            TestErrorKind::QueryResultColumnsMismatch {
+                sql,
+                expected,
+                actual,
+            } => {
+                write!(
+                    f,
+                    "query columns mismatch:\n[SQL] {sql}\n{}",
+                    format_column_diff(expected, actual, true)
+                )
+            }
+            TestErrorKind::SystemStdoutMismatch {
+                command,
+                expected_stdout,
+                actual_stdout,
+            } => {
+                write!(
+                    f,
+                    "system command stdout mismatch:\n[command] {command}\n[Diff] (-expected|+actual)\n{}",
+                    TextDiff::from_lines(expected_stdout, actual_stdout)
+                        .iter_all_changes()
+                        .format_with("\n", |diff, f|{ format_diff(&diff, f, true)})
+                )
+            }
+            _ => write!(f, "{}", self.error),
+        }
+    }
+}
+
+fn format_diff(
+    diff: &Change<&str>,
+    f: &mut dyn FnMut(&dyn std::fmt::Display) -> std::fmt::Result,
+    colorize: bool,
+) -> std::fmt::Result {
+    match diff.tag() {
+        ChangeTag::Equal => f(&diff
+            .value()
+            .lines()
+            .format_with("\n", |line, f| f(&format_args!("    {line}")))),
+        ChangeTag::Insert => f(&diff.value().lines().format_with("\n", |line, f| {
+            if colorize {
+                f(&format_args!("+   {line}").bright_green())
+            } else {
+                f(&format_args!("+   {line}"))
+            }
+        })),
+        ChangeTag::Delete => f(&diff.value().lines().format_with("\n", |line, f| {
+            if colorize {
+                f(&format_args!("-   {line}").bright_red())
+            } else {
+                f(&format_args!("-   {line}"))
+            }
+        })),
+    }
+}
+
+fn format_column_diff(expected: &str, actual: &str, colorize: bool) -> String {
+    let (expected, actual) = TextDiff::from_chars(expected, actual)
+        .iter_all_changes()
+        .fold(
+            ("".to_string(), "".to_string()),
+            |(expected, actual), change| match change.tag() {
+                ChangeTag::Equal => (
+                    format!("{}{}", expected, change.value()),
+                    format!("{}{}", actual, change.value()),
+                ),
+                ChangeTag::Delete => (
+                    if colorize {
+                        format!("{}[{}]", expected, change.value().bright_red())
+                    } else {
+                        format!("{}[{}]", expected, change.value())
+                    },
+                    actual,
+                ),
+                ChangeTag::Insert => (
+                    expected,
+                    if colorize {
+                        format!("{}[{}]", actual, change.value().bright_green())
+                    } else {
+                        format!("{}[{}]", actual, change.value())
+                    },
+                ),
+            },
+        );
+    format!("[Expected] {expected}\n[Actual  ] {actual}")
+}
+
+/// Trim and replace multiple whitespaces with one.
+#[allow(clippy::ptr_arg)]
+fn normalize_string(s: &String) -> String {
+    s.trim().split_ascii_whitespace().join(" ")
+}
+
+/// Validator will be used by [`Runner`] to validate the output.
+///
+/// # Default
+///
+/// By default ([`default_validator`]), we will use compare normalized results.
+pub type Validator = fn(actual: &[Vec<String>], expected: &[String]) -> bool;
+
+pub fn default_validator(actual: &[Vec<String>], expected: &[String]) -> bool {
+    let expected_results = expected.iter().map(normalize_string).collect_vec();
+    // Default, we compare normalized results. Whitespace characters are ignored.
+    let normalized_rows = actual
+        .iter()
+        .map(|strs| strs.iter().map(normalize_string).join(" "))
+        .collect_vec();
+    normalized_rows == expected_results
+}
+
+/// [`Runner`] uses this validator to check that the expected column types match an actual output.
+///
+/// # Default
+///
+/// By default ([`default_column_validator`]), columns are not validated.
+pub type ColumnTypeValidator<T> = fn(actual: &Vec<T>, expected: &Vec<T>) -> bool;
+
+/// The default validator always returns success for any inputs of expected and actual sets of
+/// columns.
+pub fn default_column_validator<T: ColumnType>(_: &Vec<T>, _: &Vec<T>) -> bool {
+    true
+}
+
+/// The strict validator checks:
+/// - the number of columns is as expected
+/// - each column has the same type as expected
+#[allow(clippy::ptr_arg)]
+pub fn strict_column_validator<T: ColumnType>(actual: &Vec<T>, expected: &Vec<T>) -> bool {
+    actual.len() == expected.len()
+        && !actual
+            .iter()
+            .zip(expected.iter())
+            .any(|(actual_column, expected_column)| actual_column != expected_column)
+}
+
+/// Sqllogictest runner.
+pub struct Runner<D: AsyncDB, M: MakeConnection> {
+    conn: Connections<D, M>,
+    // validator is used for validate if the result of query equals to expected.
+    validator: Validator,
+    column_type_validator: ColumnTypeValidator<D::ColumnType>,
+    substitution: Option<Substitution>,
+    sort_mode: Option<SortMode>,
+    /// 0 means never hashing
+    hash_threshold: usize,
+    /// Labels for condition `skipif` and `onlyif`.
+    labels: HashSet<String>,
+}
+
+impl<D: AsyncDB, M: MakeConnection<Conn = D>> Runner<D, M> {
+    /// Create a new test runner on the database, with the given connection maker.
+    ///
+    /// See [`MakeConnection`] for more details.
+    pub fn new(make_conn: M) -> Self {
+        Runner {
+            validator: default_validator,
+            column_type_validator: default_column_validator,
+            substitution: None,
+            sort_mode: None,
+            hash_threshold: 0,
+            labels: HashSet::new(),
+            conn: Connections::new(make_conn),
+        }
+    }
+
+    /// Add a label for condition `skipif` and `onlyif`.
+    pub fn add_label(&mut self, label: &str) {
+        self.labels.insert(label.to_string());
+    }
+
+    pub fn with_validator(&mut self, validator: Validator) {
+        self.validator = validator;
+    }
+
+    pub fn with_column_validator(&mut self, validator: ColumnTypeValidator<D::ColumnType>) {
+        self.column_type_validator = validator;
+    }
+
+    pub fn with_hash_threshold(&mut self, hash_threshold: usize) {
+        self.hash_threshold = hash_threshold;
+    }
+
+    pub async fn apply_record(
+        &mut self,
+        record: Record<D::ColumnType>,
+    ) -> RecordOutput<D::ColumnType> {
+        tracing::debug!(?record, "testing");
+        /// Returns whether we should skip this record, according to given `conditions`.
+        fn should_skip(
+            labels: &HashSet<String>,
+            engine_name: &str,
+            conditions: &[Condition],
+        ) -> bool {
+            conditions.iter().any(|c| {
+                c.should_skip(
+                    labels
+                        .iter()
+                        .map(|l| l.as_str())
+                        // attach the engine name to the labels
+                        .chain(Some(engine_name).filter(|n| !n.is_empty())),
+                )
+            })
+        }
+
+        match record {
+            Record::Statement {
+                conditions,
+                connection,
+                sql,
+
+                // compare result in run_async
+                expected: _,
+                loc: _,
+            } => {
+                let sql = match self.may_substitute(sql, true) {
+                    Ok(sql) => sql,
+                    Err(error) => {
+                        return RecordOutput::Statement {
+                            count: 0,
+                            error: Some(error),
+                        }
+                    }
+                };
+
+                let conn = match self.conn.get(connection).await {
+                    Ok(conn) => conn,
+                    Err(e) => {
+                        return RecordOutput::Statement {
+                            count: 0,
+                            error: Some(Arc::new(e)),
+                        }
+                    }
+                };
+                if should_skip(&self.labels, conn.engine_name(), &conditions) {
+                    return RecordOutput::Nothing;
+                }
+
+                let ret = conn.run(&sql).await;
+                match ret {
+                    Ok(out) => match out {
+                        DBOutput::Rows { types, rows } => RecordOutput::Query {
+                            types,
+                            rows,
+                            error: None,
+                        },
+                        DBOutput::StatementComplete(count) => {
+                            RecordOutput::Statement { count, error: None }
+                        }
+                    },
+                    Err(e) => RecordOutput::Statement {
+                        count: 0,
+                        error: Some(Arc::new(e)),
+                    },
+                }
+            }
+            Record::System {
+                conditions,
+                command,
+                loc: _,
+                stdout: expected_stdout,
+            } => {
+                if should_skip(&self.labels, "", &conditions) {
+                    return RecordOutput::Nothing;
+                }
+
+                let mut command = match self.may_substitute(command, false) {
+                    Ok(command) => command,
+                    Err(error) => {
+                        return RecordOutput::System {
+                            stdout: None,
+                            error: Some(error),
+                        }
+                    }
+                };
+
+                let is_background = command.trim().ends_with('&');
+                if is_background {
+                    command = command.trim_end_matches('&').trim().to_string();
+                }
+
+                let mut cmd = if cfg!(target_os = "windows") {
+                    let mut cmd = std::process::Command::new("cmd");
+                    cmd.arg("/C").arg(&command);
+                    cmd
+                } else {
+                    let mut cmd = std::process::Command::new("bash");
+                    cmd.arg("-c").arg(&command);
+                    cmd
+                };
+
+                if is_background {
+                    // Spawn a new process, but don't wait for stdout, otherwise it will block until
+                    // the process exits.
+                    let error: Option<AnyError> = match cmd.spawn() {
+                        Ok(_) => None,
+                        Err(e) => Some(Arc::new(e)),
+                    };
+                    tracing::info!(target:"sqllogictest::system_command", command, "background system command spawned");
+                    return RecordOutput::System {
+                        error,
+                        stdout: None,
+                    };
+                }
+
+                cmd.stdout(std::process::Stdio::piped());
+                cmd.stderr(std::process::Stdio::piped());
+
+                let result = D::run_command(cmd).await;
+                #[derive(thiserror::Error, Debug)]
+                #[error(
+                    "process exited unsuccessfully: {status}\nstdout: {stdout}\nstderr: {stderr}"
+                )]
+                struct SystemError {
+                    status: ExitStatus,
+                    stdout: String,
+                    stderr: String,
+                }
+
+                let mut actual_stdout = None;
+                let error: Option<AnyError> = match result {
+                    Ok(Output {

----------------------------------------

File: core/rust/qdb-sqllogictest/sqllogictest/src/substitution.rs
Status: added
Changes: +55 -0
Diff:
@@ -0,0 +1,55 @@
+use std::sync::{Arc, OnceLock};
+
+use subst::Env;
+use tempfile::{tempdir, TempDir};
+
+/// Substitute environment variables and special variables like `__TEST_DIR__` in SQL.
+#[derive(Default, Clone)]
+pub(crate) struct Substitution {
+    /// The temporary directory for `__TEST_DIR__`.
+    /// Lazily initialized and cleaned up when dropped.
+    test_dir: Arc<OnceLock<TempDir>>,
+}
+
+#[derive(thiserror::Error, Debug)]
+#[error("substitution failed: {0}")]
+pub(crate) struct SubstError(subst::Error);
+

----------------------------------------

File: core/rust/qdb-sqllogictest/src/lib.rs
Status: added
Changes: +28 -0
Diff:
@@ -0,0 +1,28 @@
+/*******************************************************************************
+ *     ___                  _   ____  ____
+ *    / _ \ _   _  ___  ___| |_|  _ \| __ )
+ *   | | | | | | |/ _ \/ __| __| | | |  _ \
+ *   | |_| | |_| |  __/\__ \ |_| |_| | |_) |
+ *    \__\_\\__,_|\___||___/\__|____/|____/
+ *
+ *  Copyright (c) 2014-2019 Appsicle
+ *  Copyright (c) 2019-2024 QuestDB

----------------------------------------

File: core/rust/qdb-sqllogictest/src/rustfmt.toml
Status: added
Changes: +1 -0
Diff:
@@ -0,0 +1 @@
+struct_lit_width = 50

----------------------------------------

File: core/rust/qdb-sqllogictest/src/sqllogictest/mod.rs
Status: added
Changes: +100 -0
Diff:
@@ -0,0 +1,100 @@
+use jni::objects::JClass;
+use jni::JNIEnv;
+use sqllogictest::{Record, Runner};
+use sqllogictest_engines::postgres::{PostgresConfig, PostgresExtended};
+use std::ffi::CStr;
+use std::os::raw::c_char;
+use std::path::Path;
+use tokio::runtime::Runtime;
+
+#[no_mangle]
+pub extern "system" fn Java_io_questdb_test_Sqllogictest_setEnvVar(
+    _env: JNIEnv,
+    _class: JClass,
+    var_name: *const c_char,
+    var_value: *const c_char,
+) {
+    let var_name = unsafe { CStr::from_ptr(var_name).to_str().expect("Invalid UTF-8") };
+    let var_value = unsafe { CStr::from_ptr(var_value).to_str().expect("Invalid UTF-8") };
+    std::env::set_var(var_name, var_value);
+}
+
+#[no_mangle]
+pub extern "system" fn Java_io_questdb_test_Sqllogictest_run(
+    mut env: JNIEnv,
+    _class: JClass,
+    port: i16,
+    path_ptr: *const c_char,
+) {
+    let file_path = Path::new(unsafe { CStr::from_ptr(path_ptr).to_str().expect("Invalid UTF-8") });
+    if !file_path.exists() {
+        let error_msg = format!("file not found: {:?}", file_path);
+        eprintln!("{}", &error_msg);

----------------------------------------

File: core/rust/qdbr/Cargo.lock
Status: modified
Changes: +1517 -61
Diff:
@@ -2,12 +2,453 @@
 # It is not intended for manual editing.
 version = 3
 
+[[package]]
+name = "addr2line"
+version = "0.22.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "6e4503c46a5c0c7844e948c9a4d6acd9f50cccb4de1c48eb9e291ea17470c678"
+dependencies = [
+ "gimli",
+]
+
+[[package]]
+name = "adler"
+version = "1.0.2"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "f26201604c87b1e01bd3d98f8d5d9a8fcbb815e8cedb41ffccbeb4bf593a35fe"
+
+[[package]]
+name = "ahash"
+version = "0.8.11"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "e89da841a80418a9b391ebaea17f5c112ffaaa96f621d2c285b5174da76b9011"
+dependencies = [
+ "cfg-if",
+ "const-random",
+ "getrandom",
+ "once_cell",
+ "version_check",
+ "zerocopy 0.7.35",
+]
+
+[[package]]
+name = "aho-corasick"
+version = "1.1.3"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "8e60d3430d3a69478ad0993f19238d2df97c507009a52b3c10addcd7f6bcb916"
+dependencies = [
+ "memchr",
+]
+
+[[package]]
+name = "alloc-no-stdlib"
+version = "2.0.4"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "cc7bb162ec39d46ab1ca8c77bf72e890535becd1751bb45f64c597edb4c8c6b3"
+
+[[package]]
+name = "alloc-stdlib"
+version = "0.2.2"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "94fb8275041c72129eb51b7d0322c29b8387a0386127718b096429201a5d6ece"
+dependencies = [
+ "alloc-no-stdlib",
+]
+
+[[package]]
+name = "android-tzdata"
+version = "0.1.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "e999941b234f3131b00bc13c22d06e8c5ff726d1b6318ac7eb276997bbb4fef0"
+
+[[package]]
+name = "android_system_properties"
+version = "0.1.5"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "819e7219dbd41043ac279b19830f2efc897156490d7fd6ea916720117ee66311"
+dependencies = [
+ "libc",
+]
+
+[[package]]
+name = "anstream"
+version = "0.6.14"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "418c75fa768af9c03be99d17643f93f79bbba589895012a80e3452a19ddda15b"
+dependencies = [
+ "anstyle",
+ "anstyle-parse",
+ "anstyle-query",
+ "anstyle-wincon",
+ "colorchoice",
+ "is_terminal_polyfill",
+ "utf8parse",
+]
+
+[[package]]
+name = "anstyle"
+version = "1.0.7"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "038dfcf04a5feb68e9c60b21c9625a54c2c0616e79b72b0fd87075a056ae1d1b"
+
+[[package]]
+name = "anstyle-parse"
+version = "0.2.4"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "c03a11a9034d92058ceb6ee011ce58af4a9bf61491aa7e1e59ecd24bd40d22d4"
+dependencies = [
+ "utf8parse",
+]
+
+[[package]]
+name = "anstyle-query"
+version = "1.1.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "ad186efb764318d35165f1758e7dcef3b10628e26d41a44bc5550652e6804391"
+dependencies = [
+ "windows-sys 0.52.0",
+]
+
+[[package]]
+name = "anstyle-wincon"
+version = "3.0.3"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "61a38449feb7068f52bb06c12759005cf459ee52bb4adc1d5a7c4322d716fb19"
+dependencies = [
+ "anstyle",
+ "windows-sys 0.52.0",
+]
+
+[[package]]
+name = "anyhow"
+version = "1.0.86"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "b3d1d046238990b9cf5bcde22a3fb3584ee5cf65fb2765f454ed428c7a0063da"
+
+[[package]]
+name = "arrow"
+version = "51.0.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "219d05930b81663fd3b32e3bde8ce5bff3c4d23052a99f11a8fa50a3b47b2658"
+dependencies = [
+ "arrow-arith",
+ "arrow-array",
+ "arrow-buffer",
+ "arrow-cast",
+ "arrow-csv",
+ "arrow-data",
+ "arrow-ipc",
+ "arrow-json",
+ "arrow-ord",
+ "arrow-row",
+ "arrow-schema",
+ "arrow-select",
+ "arrow-string",
+]
+
+[[package]]
+name = "arrow-arith"
+version = "51.0.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "0272150200c07a86a390be651abdd320a2d12e84535f0837566ca87ecd8f95e0"
+dependencies = [
+ "arrow-array",
+ "arrow-buffer",
+ "arrow-data",
+ "arrow-schema",
+ "chrono",
+ "half",
+ "num",
+]
+
+[[package]]
+name = "arrow-array"
+version = "51.0.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "8010572cf8c745e242d1b632bd97bd6d4f40fefed5ed1290a8f433abaa686fea"
+dependencies = [
+ "ahash",
+ "arrow-buffer",
+ "arrow-data",
+ "arrow-schema",
+ "chrono",
+ "half",
+ "hashbrown",
+ "num",
+]
+
+[[package]]
+name = "arrow-buffer"
+version = "51.0.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "0d0a2432f0cba5692bf4cb757469c66791394bac9ec7ce63c1afe74744c37b27"
+dependencies = [
+ "bytes",
+ "half",
+ "num",
+]
+
+[[package]]
+name = "arrow-cast"
+version = "51.0.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "9abc10cd7995e83505cc290df9384d6e5412b207b79ce6bdff89a10505ed2cba"
+dependencies = [
+ "arrow-array",
+ "arrow-buffer",
+ "arrow-data",
+ "arrow-schema",
+ "arrow-select",
+ "atoi",
+ "base64",
+ "chrono",
+ "half",
+ "lexical-core",
+ "num",
+ "ryu",
+]
+
+[[package]]
+name = "arrow-csv"
+version = "51.0.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "95cbcba196b862270bf2a5edb75927380a7f3a163622c61d40cbba416a6305f2"
+dependencies = [
+ "arrow-array",
+ "arrow-buffer",
+ "arrow-cast",
+ "arrow-data",
+ "arrow-schema",
+ "chrono",
+ "csv",
+ "csv-core",
+ "lazy_static",
+ "lexical-core",
+ "regex",
+]
+
+[[package]]
+name = "arrow-data"
+version = "51.0.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "2742ac1f6650696ab08c88f6dd3f0eb68ce10f8c253958a18c943a68cd04aec5"
+dependencies = [
+ "arrow-buffer",
+ "arrow-schema",
+ "half",
+ "num",
+]
+
+[[package]]
+name = "arrow-ipc"
+version = "51.0.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "a42ea853130f7e78b9b9d178cb4cd01dee0f78e64d96c2949dc0a915d6d9e19d"
+dependencies = [
+ "arrow-array",
+ "arrow-buffer",
+ "arrow-cast",
+ "arrow-data",
+ "arrow-schema",
+ "flatbuffers",
+]
+
+[[package]]
+name = "arrow-json"
+version = "51.0.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "eaafb5714d4e59feae964714d724f880511500e3569cc2a94d02456b403a2a49"
+dependencies = [
+ "arrow-array",
+ "arrow-buffer",
+ "arrow-cast",
+ "arrow-data",
+ "arrow-schema",
+ "chrono",
+ "half",
+ "indexmap",
+ "lexical-core",
+ "num",
+ "serde",
+ "serde_json",
+]
+
+[[package]]
+name = "arrow-ord"
+version = "51.0.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "e3e6b61e3dc468f503181dccc2fc705bdcc5f2f146755fa5b56d0a6c5943f412"
+dependencies = [
+ "arrow-array",
+ "arrow-buffer",
+ "arrow-data",
+ "arrow-schema",
+ "arrow-select",
+ "half",
+ "num",
+]
+
+[[package]]
+name = "arrow-row"
+version = "51.0.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "848ee52bb92eb459b811fb471175ea3afcf620157674c8794f539838920f9228"
+dependencies = [
+ "ahash",
+ "arrow-array",
+ "arrow-buffer",
+ "arrow-data",
+ "arrow-schema",
+ "half",
+ "hashbrown",
+]
+
+[[package]]
+name = "arrow-schema"
+version = "51.0.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "02d9483aaabe910c4781153ae1b6ae0393f72d9ef757d38d09d450070cf2e528"
+
+[[package]]
+name = "arrow-select"
+version = "51.0.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "849524fa70e0e3c5ab58394c770cb8f514d0122d20de08475f7b472ed8075830"
+dependencies = [
+ "ahash",
+ "arrow-array",
+ "arrow-buffer",
+ "arrow-data",
+ "arrow-schema",
+ "num",
+]
+
+[[package]]
+name = "arrow-string"
+version = "51.0.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "9373cb5a021aee58863498c37eb484998ef13377f69989c6c5ccfbd258236cdb"
+dependencies = [
+ "arrow-array",
+ "arrow-buffer",
+ "arrow-data",
+ "arrow-schema",
+ "arrow-select",
+ "memchr",
+ "num",
+ "regex",
+ "regex-syntax",
+]
+
+[[package]]
+name = "async-trait"
+version = "0.1.81"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "6e0c28dcc82d7c8ead5cb13beb15405b57b8546e93215673ff8ca0349a028107"
+dependencies = [
+ "proc-macro2",
+ "quote",
+ "syn",
+]
+
+[[package]]
+name = "atoi"
+version = "2.0.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "f28d99ec8bfea296261ca1af174f24225171fea9664ba9003cbebee704810528"
+dependencies = [
+ "num-traits",
+]
+
+[[package]]
+name = "autocfg"
+version = "1.3.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "0c4b4d0bd25bd0b74681c0ad21497610ce1b7c91b1022cd21c80c6fbdd9476b0"
+
+[[package]]
+name = "backtrace"
+version = "0.3.73"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "5cc23269a4f8976d0a4d2e7109211a419fe30e8d88d677cd60b6bc79c5732e0a"
+dependencies = [
+ "addr2line",
+ "cc",
+ "cfg-if",
+ "libc",
+ "miniz_oxide",
+ "object",
+ "rustc-demangle",
+]
+
+[[package]]
+name = "base64"
+version = "0.22.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "72b3254f16251a8381aa12e40e3c4d2f0199f8c6508fbecb9d91f575e0fbb8c6"
+
+[[package]]
+name = "bitflags"
+version = "1.3.2"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "bef38d45163c2f1dde094a7dfd33ccf595c92905c8f8f4fdc18d06fb1037718a"
+
+[[package]]
+name = "bitflags"
+version = "2.6.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "b048fb63fd8b5923fc5aa7b340d8e156aec7ec02f0c78fa8a6ddc2613f6f71de"
+
+[[package]]
+name = "brotli"
+version = "3.5.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "d640d25bc63c50fb1f0b545ffd80207d2e10a4c965530809b40ba3386825c391"
+dependencies = [
+ "alloc-no-stdlib",
+ "alloc-stdlib",
+ "brotli-decompressor",
+]
+
+[[package]]
+name = "brotli-decompressor"
+version = "2.5.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "4e2e4afe60d7dd600fdd3de8d0f08c2b7ec039712e3b6137ff98b7004e82de4f"
+dependencies = [
+ "alloc-no-stdlib",
+ "alloc-stdlib",
+]
+
+[[package]]
+name = "bumpalo"
+version = "3.16.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "79296716171880943b8470b5f8d03aa55eb2e645a4874bdbb28adb49162e012c"
+
+[[package]]
+name = "byteorder"
+version = "1.5.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "1fd0f2584146f6f2ef48085050886acf353beff7305ebd1ae69500e27c67f64b"
+
 [[package]]
 name = "bytes"
 version = "1.6.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "514de17de45fdb8dc022b1a7975556c53c86f9f0aa5f534b98977b171857c2c9"
 
+[[package]]
+name = "cc"
+version = "1.1.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "eaff6f8ce506b9773fa786672d63fc7a191ffea1be33f72bbd4aeacefca9ffc8"
+dependencies = [
+ "jobserver",
+ "libc",
+ "once_cell",
+]
+
 [[package]]
 name = "cesu8"
 version = "1.1.0"
@@ -18,57 +459,647 @@ checksum = "6d43a04d8753f35258c91f8ec639f792891f748a1edbd759cf1dcea3382ad83c"
 name = "cfg-if"
 version = "1.0.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "baf1de4339761588bc0619e3cbc0120ee582ebb74b53b4efbf79117bd2da40fd"
+checksum = "baf1de4339761588bc0619e3cbc0120ee582ebb74b53b4efbf79117bd2da40fd"
+
+[[package]]
+name = "chrono"
+version = "0.4.38"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "a21f936df1771bf62b77f047b726c4625ff2e8aa607c01ec06e5a05bd8463401"
+dependencies = [
+ "android-tzdata",
+ "iana-time-zone",
+ "num-traits",
+ "windows-targets 0.52.6",
+]
+
+[[package]]
+name = "clap"
+version = "4.5.9"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "64acc1846d54c1fe936a78dc189c34e28d3f5afc348403f28ecf53660b9b8462"
+dependencies = [
+ "clap_builder",
+]
+
+[[package]]
+name = "clap_builder"
+version = "4.5.9"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "6fb8393d67ba2e7bfaf28a23458e4e2b543cc73a99595511eb207fdb8aede942"
+dependencies = [
+ "anstream",
+ "anstyle",
+ "clap_lex",
+ "strsim",
+]
+
+[[package]]
+name = "clap_lex"
+version = "0.7.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "4b82cf0babdbd58558212896d1a4272303a57bdb245c2bf1147185fb45640e70"
+
+[[package]]
+name = "colorchoice"
+version = "1.0.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "0b6a852b24ab71dffc585bcb46eaf7959d175cb865a7152e35b348d1b2960422"
+
+[[package]]
+name = "combine"
+version = "4.6.7"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "ba5a308b75df32fe02788e748662718f03fde005016435c444eea572398219fd"
+dependencies = [
+ "bytes",
+ "memchr",
+]
+
+[[package]]
+name = "const-random"
+version = "0.1.18"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "87e00182fe74b066627d63b85fd550ac2998d4b0bd86bfed477a0ae4c7c71359"
+dependencies = [
+ "const-random-macro",
+]
+
+[[package]]
+name = "const-random-macro"
+version = "0.1.16"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "f9d839f2a20b0aee515dc581a6172f2321f96cab76c1a38a4c584a194955390e"
+dependencies = [
+ "getrandom",
+ "once_cell",
+ "tiny-keccak",
+]
+
+[[package]]
+name = "core-foundation-sys"
+version = "0.8.6"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "06ea2b9bc92be3c2baa9334a323ebca2d6f074ff852cd1d7b11064035cd3868f"
+
+[[package]]
+name = "crc32fast"
+version = "1.4.2"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "a97769d94ddab943e4510d138150169a2758b5ef3eb191a9ee688de3e23ef7b3"
+dependencies = [
+ "cfg-if",
+]
+
+[[package]]
+name = "crossbeam-deque"
+version = "0.8.5"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "613f8cc01fe9cf1a3eb3d7f488fd2fa8388403e97039e2f73692932e291a770d"
+dependencies = [
+ "crossbeam-epoch",
+ "crossbeam-utils",
+]
+
+[[package]]
+name = "crossbeam-epoch"
+version = "0.9.18"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "5b82ac4a3c2ca9c3460964f020e1402edd5753411d7737aa39c3714ad1b5420e"
+dependencies = [
+ "crossbeam-utils",
+]
+
+[[package]]
+name = "crossbeam-utils"
+version = "0.8.20"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "22ec99545bb0ed0ea7bb9b8e1e9122ea386ff8a48c0922e43f36d45ab09e0e80"
+
+[[package]]
+name = "crunchy"
+version = "0.2.2"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "7a81dae078cea95a014a339291cec439d2f232ebe854a9d672b796c6afafa9b7"
+
+[[package]]
+name = "csv"
+version = "1.3.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "ac574ff4d437a7b5ad237ef331c17ccca63c46479e5b5453eb8e10bb99a759fe"

----------------------------------------

File: core/rust/qdbr/Cargo.toml
Status: modified
Changes: +17 -0
Diff:
@@ -9,3 +9,20 @@ crate-type = ["dylib"]
 
 [dependencies]
 jni = "0.21.1"
+num-traits = "0.2.18"
+parquet2 = { path = "parquet2" }
+libc = "0.2.155"
+anyhow = "1.0.86"
+once_cell = "1.19.0"
+rayon = "1.10.0"

----------------------------------------

File: core/rust/qdbr/parquet2/Cargo.toml
Status: added
Changes: +60 -0
Diff:
@@ -0,0 +1,60 @@
+[package]
+name = "parquet2"
+version = "0.17.2"
+license = "Apache-2.0"
+description = "Safe implementation of parquet IO."
+authors = ["Jorge C. Leitao <jorgecarleitao@gmail.com", "Apache Arrow <dev@arrow.apache.org>"]
+keywords = [ "analytics", "parquet" ]
+homepage = "https://github.com/jorgecarleitao/parquet2"
+repository = "https://github.com/jorgecarleitao/parquet2"
+readme = "README.md"
+edition = "2021"
+
+[lib]
+name = "parquet2"
+bench = false
+
+[package.metadata.docs.rs]
+features = ["full"]
+rustdoc-args = ["--cfg", "docsrs"]

----------------------------------------

File: core/rust/qdbr/parquet2/LICENSE
Status: added
Changes: +13 -0
Diff:
@@ -0,0 +1,13 @@
+Copyright [2021] [Jorge C Leitao]
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software

----------------------------------------

File: core/rust/qdbr/parquet2/src/bloom_filter/hash.rs
Status: added
Changes: +17 -0
Diff:
@@ -0,0 +1,17 @@
+use xxhash_rust::xxh64::xxh64;
+
+use crate::types::NativeType;
+
+const SEED: u64 = 0;
+
+/// (xxh64) hash of a [`NativeType`].
+#[inline]
+pub fn hash_native<T: NativeType>(value: T) -> u64 {

----------------------------------------

File: core/rust/qdbr/parquet2/src/bloom_filter/mod.rs
Status: added
Changes: +71 -0
Diff:
@@ -0,0 +1,71 @@
+//! API to read and use bloom filters
+mod hash;
+mod read;
+mod split_block;
+
+pub use hash::{hash_byte, hash_native};
+pub use read::read;
+pub use split_block::{insert, is_in_set};
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn basics() {
+        let mut bitset = vec![0; 32];
+
+        // insert
+        for a in 0..10i64 {
+            let hash = hash_native(a);
+            insert(&mut bitset, hash);
+        }
+

----------------------------------------

File: core/rust/qdbr/parquet2/src/bloom_filter/read.rs
Status: added
Changes: +49 -0
Diff:
@@ -0,0 +1,49 @@
+use std::io::{Read, Seek, SeekFrom};
+
+use parquet_format_safe::{
+    thrift::protocol::TCompactInputProtocol, BloomFilterAlgorithm, BloomFilterCompression,
+    BloomFilterHeader, SplitBlockAlgorithm, Uncompressed,
+};
+
+use crate::{error::Error, metadata::ColumnChunkMetaData};
+
+/// Reads the bloom filter associated to [`ColumnChunkMetaData`] into `bitset`.
+/// Results in an empty `bitset` if there is no associated bloom filter or the algorithm is not supported.
+/// # Error
+/// Errors if the column contains no metadata or the filter can't be read or deserialized.
+pub fn read<R: Read + Seek>(
+    column_metadata: &ColumnChunkMetaData,

----------------------------------------

File: core/rust/qdbr/parquet2/src/bloom_filter/split_block.rs
Status: added
Changes: +82 -0
Diff:
@@ -0,0 +1,82 @@
+use std::convert::TryInto;
+
+/// magic numbers taken from https://github.com/apache/parquet-format/blob/master/BloomFilter.md
+const SALT: [u32; 8] = [
+    1203114875, 1150766481, 2284105051, 2729912477, 1884591559, 770785867, 2667333959, 1550580529,
+];
+
+fn hash_to_block_index(hash: u64, len: usize) -> usize {
+    let number_of_blocks = len as u64 / 32;
+    let low_hash = hash >> 32;
+    let block_index = ((low_hash * number_of_blocks) >> 32) as u32;
+    block_index as usize
+}
+
+fn new_mask(x: u32) -> [u32; 8] {
+    let mut a = [0u32; 8];
+    for i in 0..8 {
+        let mask = x.wrapping_mul(SALT[i]);
+        let mask = mask >> 27;
+        let mask = 0x1 << mask;
+        a[i] = mask;
+    }
+    a
+}
+
+/// loads a block from the bitset to the stack

----------------------------------------

File: core/rust/qdbr/parquet2/src/compression.rs
Status: added
Changes: +407 -0
Diff:
@@ -0,0 +1,407 @@
+//! Functionality to compress and decompress data according to the parquet specification
+pub use super::parquet_bridge::{
+    BrotliLevel, Compression, CompressionOptions, GzipLevel, ZstdLevel,
+};
+
+use crate::error::{Error, Result};
+
+fn inner_compress<G: Fn(usize) -> Result<usize>, F: Fn(&[u8], &mut [u8]) -> Result<usize>>(
+    input: &[u8],
+    output: &mut Vec<u8>,
+    get_length: G,
+    compress: F,
+) -> Result<()> {
+    let original_length = output.len();
+    let max_required_length = get_length(input.len())?;
+
+    output.resize(original_length + max_required_length, 0);
+    let compressed_size = compress(input, &mut output[original_length..])?;
+
+    output.truncate(original_length + compressed_size);
+    Ok(())
+}
+
+/// Compresses data stored in slice `input_buf` and writes the compressed result
+/// to `output_buf`.
+/// Note that you'll need to call `clear()` before reusing the same `output_buf`
+/// across different `compress` calls.
+pub fn compress(
+    compression: CompressionOptions,
+    input_buf: &[u8],
+    output_buf: &mut Vec<u8>,
+) -> Result<()> {
+    match compression {
+        #[cfg(feature = "brotli")]
+        CompressionOptions::Brotli(level) => {
+            use std::io::Write;
+            const BROTLI_DEFAULT_BUFFER_SIZE: usize = 4096;
+            const BROTLI_DEFAULT_LG_WINDOW_SIZE: u32 = 22; // recommended between 20-22
+
+            let q = level.unwrap_or_default();
+            let mut encoder = brotli::CompressorWriter::new(
+                output_buf,
+                BROTLI_DEFAULT_BUFFER_SIZE,
+                q.compression_level(),
+                BROTLI_DEFAULT_LG_WINDOW_SIZE,
+            );
+            encoder.write_all(input_buf)?;
+            encoder.flush().map_err(|e| e.into())
+        }
+        #[cfg(not(feature = "brotli"))]
+        CompressionOptions::Brotli(_) => Err(Error::FeatureNotActive(
+            crate::error::Feature::Brotli,
+            "compress to brotli".to_string(),
+        )),
+        #[cfg(feature = "gzip")]
+        CompressionOptions::Gzip(level) => {
+            use std::io::Write;
+            let level = level.unwrap_or_default();
+            let mut encoder = flate2::write::GzEncoder::new(output_buf, level.into());
+            encoder.write_all(input_buf)?;
+            encoder.try_finish().map_err(|e| e.into())
+        }
+        #[cfg(not(feature = "gzip"))]
+        CompressionOptions::Gzip(_) => Err(Error::FeatureNotActive(
+            crate::error::Feature::Gzip,
+            "compress to gzip".to_string(),
+        )),
+        #[cfg(feature = "snappy")]
+        CompressionOptions::Snappy => inner_compress(
+            input_buf,
+            output_buf,
+            |len| Ok(snap::raw::max_compress_len(len)),
+            |input, output| Ok(snap::raw::Encoder::new().compress(input, output)?),
+        ),
+        #[cfg(not(feature = "snappy"))]
+        CompressionOptions::Snappy => Err(Error::FeatureNotActive(
+            crate::error::Feature::Snappy,
+            "compress to snappy".to_string(),
+        )),
+        #[cfg(all(feature = "lz4_flex", not(feature = "lz4")))]
+        CompressionOptions::Lz4Raw => inner_compress(
+            input_buf,
+            output_buf,
+            |len| Ok(lz4_flex::block::get_maximum_output_size(len)),
+            |input, output| {
+                let compressed_size = lz4_flex::block::compress_into(input, output)?;
+                Ok(compressed_size)
+            },
+        ),
+        #[cfg(feature = "lz4")]
+        CompressionOptions::Lz4Raw => inner_compress(
+            input_buf,
+            output_buf,
+            |len| Ok(lz4::block::compress_bound(len)?),
+            |input, output| {
+                let compressed_size = lz4::block::compress_to_buffer(input, None, false, output)?;
+                Ok(compressed_size)
+            },
+        ),
+        #[cfg(all(not(feature = "lz4"), not(feature = "lz4_flex")))]
+        CompressionOptions::Lz4Raw => Err(Error::FeatureNotActive(
+            crate::error::Feature::Lz4,
+            "compress to lz4".to_string(),
+        )),
+        #[cfg(feature = "zstd")]
+        CompressionOptions::Zstd(level) => {
+            use std::io::Write;
+            let level = level.map(|v| v.compression_level()).unwrap_or_default();
+
+            let mut encoder = zstd::Encoder::new(output_buf, level)?;
+            encoder.write_all(input_buf)?;
+            match encoder.finish() {
+                Ok(_) => Ok(()),
+                Err(e) => Err(e.into()),
+            }
+        }
+        #[cfg(not(feature = "zstd"))]
+        CompressionOptions::Zstd(_) => Err(Error::FeatureNotActive(
+            crate::error::Feature::Zstd,
+            "compress to zstd".to_string(),
+        )),
+        CompressionOptions::Uncompressed => Err(Error::InvalidParameter(
+            "Compressing uncompressed".to_string(),
+        )),
+        _ => Err(Error::FeatureNotSupported(format!(
+            "Compression {:?} is not supported",
+            compression,
+        ))),
+    }
+}
+
+/// Decompresses data stored in slice `input_buf` and writes output to `output_buf`.
+/// Returns the total number of bytes written.
+pub fn decompress(compression: Compression, input_buf: &[u8], output_buf: &mut [u8]) -> Result<()> {
+    match compression {

----------------------------------------

File: core/rust/qdbr/parquet2/src/deserialize/binary.rs
Status: added
Changes: +72 -0
Diff:
@@ -0,0 +1,72 @@
+use crate::{
+    encoding::{hybrid_rle, plain_byte_array::BinaryIter},
+    error::Error,
+    page::{split_buffer, DataPage},
+    parquet_bridge::{Encoding, Repetition},
+};
+
+use super::utils;
+
+#[derive(Debug)]
+pub struct Dictionary<'a, P> {
+    pub indexes: hybrid_rle::HybridRleDecoder<'a>,
+    pub dict: P,
+}
+
+impl<'a, P> Dictionary<'a, P> {
+    pub fn try_new(page: &'a DataPage, dict: P) -> Result<Self, Error> {
+        let indexes = utils::dict_indices_decoder(page)?;
+
+        Ok(Self { indexes, dict })
+    }
+
+    #[inline]

----------------------------------------

File: core/rust/qdbr/parquet2/src/deserialize/boolean.rs
Status: added
Changes: +42 -0
Diff:
@@ -0,0 +1,42 @@
+use crate::{
+    encoding::hybrid_rle::BitmapIter,
+    error::Error,
+    page::{split_buffer, DataPage},
+    parquet_bridge::{Encoding, Repetition},
+};
+
+use super::utils;
+
+// The state of a `DataPage` of `Boolean` parquet boolean type
+#[derive(Debug)]
+#[allow(clippy::large_enum_variant)]
+pub enum BooleanPageState<'a> {

----------------------------------------

File: core/rust/qdbr/parquet2/src/deserialize/filtered_rle.rs
Status: added
Changes: +274 -0
Diff:
@@ -0,0 +1,274 @@
+use std::collections::VecDeque;
+
+use crate::error::Error;
+use crate::{encoding::hybrid_rle::BitmapIter, indexes::Interval};
+
+use super::{HybridDecoderBitmapIter, HybridEncoded};
+
+/// Type definition of a [`FilteredHybridBitmapIter`] of [`HybridDecoderBitmapIter`].
+pub type FilteredHybridRleDecoderIter<'a> =
+    FilteredHybridBitmapIter<'a, HybridDecoderBitmapIter<'a>>;
+
+/// The decoding state of the hybrid-RLE decoder with a maximum definition level of 1
+/// that can supports skipped runs
+#[derive(Debug, Clone, Copy, PartialEq, Eq)]
+pub enum FilteredHybridEncoded<'a> {
+    /// a bitmap (values, offset, length, skipped_set)
+    Bitmap {
+        values: &'a [u8],
+        offset: usize,
+        length: usize,
+    },
+    Repeated {
+        is_set: bool,
+        length: usize,
+    },
+    /// When the run was skipped - contains the number of set values on the skipped run
+    Skipped(usize),
+}
+
+fn is_set_count(values: &[u8], offset: usize, length: usize) -> usize {
+    BitmapIter::new(values, offset, length)
+        .filter(|x| *x)
+        .count()
+}
+
+impl<'a> FilteredHybridEncoded<'a> {
+    /// Returns the length of the run in number of items
+    #[inline]
+    pub fn len(&self) -> usize {
+        match self {
+            FilteredHybridEncoded::Bitmap { length, .. } => *length,
+            FilteredHybridEncoded::Repeated { length, .. } => *length,
+            FilteredHybridEncoded::Skipped(_) => 0,
+        }
+    }
+
+    #[must_use]
+    pub fn is_empty(&self) -> bool {
+        self.len() == 0
+    }
+}
+
+/// An [`Iterator`] adapter over [`HybridEncoded`] that yields [`FilteredHybridEncoded`].
+///
+/// This iterator adapter is used in combination with
+#[derive(Debug, Clone, PartialEq, Eq)]
+pub struct FilteredHybridBitmapIter<'a, I: Iterator<Item = Result<HybridEncoded<'a>, Error>>> {
+    iter: I,
+    current: Option<(HybridEncoded<'a>, usize)>,
+    // a run may end in the middle of an interval, in which case we must
+    // split the interval in parts. This tracks the current interval being computed
+    current_interval: Option<Interval>,
+    selected_rows: VecDeque<Interval>,
+    current_items_in_runs: usize,
+
+    total_items: usize,
+}
+
+impl<'a, I: Iterator<Item = Result<HybridEncoded<'a>, Error>>> FilteredHybridBitmapIter<'a, I> {
+    pub fn new(iter: I, selected_rows: VecDeque<Interval>) -> Self {
+        let total_items = selected_rows.iter().map(|x| x.length).sum();
+        Self {
+            iter,
+            current: None,
+            current_interval: None,
+            selected_rows,
+            current_items_in_runs: 0,
+            total_items,
+        }
+    }
+
+    fn advance_current_interval(&mut self, length: usize) {
+        if let Some(interval) = &mut self.current_interval {
+            interval.start += length;
+            interval.length -= length;
+            self.total_items -= length;
+        }
+    }
+
+    /// Returns the number of elements remaining. Note that each run

----------------------------------------

File: core/rust/qdbr/parquet2/src/deserialize/fixed_len.rs
Status: added
Changes: +110 -0
Diff:
@@ -0,0 +1,110 @@
+use crate::{
+    encoding::hybrid_rle,
+    error::Error,
+    page::{split_buffer, DataPage},
+    parquet_bridge::{Encoding, Repetition},
+    schema::types::PhysicalType,
+};
+
+use super::utils;
+
+#[derive(Debug)]
+pub struct FixexBinaryIter<'a> {
+    values: std::slice::ChunksExact<'a, u8>,
+}
+
+impl<'a> FixexBinaryIter<'a> {
+    pub fn new(values: &'a [u8], size: usize) -> Self {
+        let values = values.chunks_exact(size);
+        Self { values }
+    }
+}
+
+impl<'a> Iterator for FixexBinaryIter<'a> {
+    type Item = &'a [u8];
+
+    #[inline]
+    fn next(&mut self) -> Option<Self::Item> {
+        self.values.next()
+    }
+
+    #[inline]
+    fn size_hint(&self) -> (usize, Option<usize>) {
+        self.values.size_hint()
+    }
+}
+

----------------------------------------

File: core/rust/qdbr/parquet2/src/deserialize/hybrid_rle.rs
Status: added
Changes: +205 -0
Diff:
@@ -0,0 +1,205 @@
+use crate::error::Error;
+
+use crate::encoding::hybrid_rle::{self, BitmapIter};
+
+/// The decoding state of the hybrid-RLE decoder with a maximum definition level of 1
+#[derive(Debug, Clone, Copy, PartialEq, Eq)]
+pub enum HybridEncoded<'a> {
+    /// a bitmap
+    Bitmap(&'a [u8], usize),
+    /// A repeated item. The first attribute corresponds to whether the value is set
+    /// the second attribute corresponds to the number of repetitions.
+    Repeated(bool, usize),
+}
+
+impl<'a> HybridEncoded<'a> {
+    /// Returns the length of the run in number of items
+    #[inline]
+    pub fn len(&self) -> usize {
+        match self {
+            HybridEncoded::Bitmap(_, length) => *length,
+            HybridEncoded::Repeated(_, length) => *length,
+        }
+    }
+
+    #[must_use]
+    pub fn is_empty(&self) -> bool {
+        self.len() == 0
+    }
+}
+
+pub trait HybridRleRunsIterator<'a>: Iterator<Item = Result<HybridEncoded<'a>, Error>> {
+    /// Number of elements remaining. This may not be the items of the iterator - an item
+    /// of the iterator may contain more than one element.
+    fn number_of_elements(&self) -> usize;
+}
+
+/// An iterator of [`HybridEncoded`], adapter over [`hybrid_rle::HybridEncoded`].
+#[derive(Debug, Clone)]
+pub struct HybridRleIter<'a, I>
+where
+    I: Iterator<Item = Result<hybrid_rle::HybridEncoded<'a>, Error>>,
+{
+    iter: I,
+    length: usize,
+    consumed: usize,
+}
+
+impl<'a, I> HybridRleIter<'a, I>
+where
+    I: Iterator<Item = Result<hybrid_rle::HybridEncoded<'a>, Error>>,
+{
+    /// Returns a new [`HybridRleIter`]
+    #[inline]
+    pub fn new(iter: I, length: usize) -> Self {
+        Self {
+            iter,
+            length,
+            consumed: 0,
+        }
+    }
+
+    /// the number of elements in the iterator. Note that this _is not_ the number of runs.
+    #[inline]
+    pub fn len(&self) -> usize {
+        self.length - self.consumed
+    }
+

----------------------------------------

File: core/rust/qdbr/parquet2/src/deserialize/mod.rs
Status: added
Changes: +16 -0
Diff:
@@ -0,0 +1,16 @@
+#![allow(ambiguous_glob_reexports)]
+mod binary;
+mod boolean;
+mod filtered_rle;
+mod fixed_len;
+mod hybrid_rle;
+mod native;
+mod utils;
+

----------------------------------------

File: core/rust/qdbr/parquet2/src/deserialize/native.rs
Status: added
Changes: +100 -0
Diff:
@@ -0,0 +1,100 @@
+use crate::{
+    encoding::hybrid_rle,
+    error::Error,
+    page::{split_buffer, DataPage},
+    parquet_bridge::{Encoding, Repetition},
+    types::{decode, NativeType},
+};
+
+use super::utils;
+
+/// Typedef of an iterator over PLAIN page values
+pub type Casted<'a, T> = std::iter::Map<std::slice::ChunksExact<'a, u8>, fn(&'a [u8]) -> T>;
+
+/// Views the values of the data page as [`Casted`] to [`NativeType`].
+pub fn native_cast<T: NativeType>(page: &DataPage) -> Result<Casted<T>, Error> {
+    let (_, _, values) = split_buffer(page)?;
+    if values.len() % std::mem::size_of::<T>() != 0 {
+        return Err(Error::oos(
+            "A primitive page data's len must be a multiple of the type",
+        ));
+    }
+
+    Ok(values
+        .chunks_exact(std::mem::size_of::<T>())
+        .map(decode::<T>))
+}
+
+#[derive(Debug)]
+pub struct Dictionary<'a, P> {
+    pub indexes: hybrid_rle::HybridRleDecoder<'a>,
+    pub dict: P,
+}

----------------------------------------

File: core/rust/qdbr/parquet2/src/deserialize/utils.rs
Status: added
Changes: +177 -0
Diff:
@@ -0,0 +1,177 @@
+use std::collections::VecDeque;
+
+use crate::{
+    encoding::hybrid_rle::{self, HybridRleDecoder},
+    error::Error,
+    indexes::Interval,
+    page::{split_buffer, DataPage},
+    read::levels::get_bit_width,
+};
+
+use super::hybrid_rle::{HybridDecoderBitmapIter, HybridRleIter};
+
+pub(super) fn dict_indices_decoder(page: &DataPage) -> Result<hybrid_rle::HybridRleDecoder, Error> {
+    let (_, _, indices_buffer) = split_buffer(page)?;
+
+    // SPEC: Data page format: the bit width used to encode the entry ids stored as 1 byte (max bit width = 32),
+    // SPEC: followed by the values encoded using RLE/Bit packed described above (with the given bit width).
+    let bit_width = indices_buffer[0];
+    if bit_width > 32 {
+        return Err(Error::oos(
+            "Bit width of dictionary pages cannot be larger than 32",
+        ));
+    }
+    let indices_buffer = &indices_buffer[1..];
+
+    hybrid_rle::HybridRleDecoder::try_new(indices_buffer, bit_width as u32, page.num_values())
+}
+
+/// Decoder of definition levels.
+#[derive(Debug)]
+pub enum DefLevelsDecoder<'a> {
+    /// When the maximum definition level is 1, the definition levels are RLE-encoded and
+    /// the bitpacked runs are bitmaps. This variant contains [`HybridDecoderBitmapIter`]
+    /// that decodes the runs, but not the individual values
+    Bitmap(HybridDecoderBitmapIter<'a>),
+    /// When the maximum definition level is larger than 1
+    Levels(HybridRleDecoder<'a>, u32),
+}
+
+impl<'a> DefLevelsDecoder<'a> {
+    pub fn try_new(page: &'a DataPage) -> Result<Self, Error> {
+        let (_, def_levels, _) = split_buffer(page)?;
+
+        let max_def_level = page.descriptor.max_def_level;
+        Ok(if max_def_level == 1 {
+            let iter = hybrid_rle::Decoder::new(def_levels, 1);
+            let iter = HybridRleIter::new(iter, page.num_values());
+            Self::Bitmap(iter)
+        } else {
+            let iter = HybridRleDecoder::try_new(
+                def_levels,
+                get_bit_width(max_def_level),
+                page.num_values(),
+            )?;
+            Self::Levels(iter, max_def_level as u32)
+        })
+    }
+}

----------------------------------------

File: core/rust/qdbr/parquet2/src/encoding/bitpacked/decode.rs
Status: added
Changes: +212 -0
Diff:
@@ -0,0 +1,212 @@
+use crate::error::Error;
+
+use super::{Packed, Unpackable, Unpacked};
+
+/// An [`Iterator`] of [`Unpackable`] unpacked from a bitpacked slice of bytes.
+/// # Implementation
+/// This iterator unpacks bytes in chunks and does not allocate.
+#[derive(Debug, Clone)]
+pub struct Decoder<'a, T: Unpackable> {
+    packed: std::slice::Chunks<'a, u8>,
+    num_bits: usize,
+    remaining: usize,          // in number of items
+    current_pack_index: usize, // invariant: < T::PACK_LENGTH
+    unpacked: T::Unpacked,     // has the current unpacked values.
+}
+
+#[inline]
+fn decode_pack<T: Unpackable>(packed: &[u8], num_bits: usize, unpacked: &mut T::Unpacked) {
+    if packed.len() < T::Unpacked::LENGTH * num_bits / 8 {
+        let mut buf = T::Packed::zero();
+        buf.as_mut()[..packed.len()].copy_from_slice(packed);
+        T::unpack(buf.as_ref(), num_bits, unpacked)
+    } else {
+        T::unpack(packed, num_bits, unpacked)
+    }
+}
+
+impl<'a, T: Unpackable> Decoder<'a, T> {
+    /// Returns a [`Decoder`] with `T` encoded in `packed` with `num_bits`.
+    pub fn try_new(packed: &'a [u8], num_bits: usize, mut length: usize) -> Result<Self, Error> {
+        let block_size = std::mem::size_of::<T>() * num_bits;
+
+        if num_bits == 0 {
+            return Err(Error::oos("Bitpacking requires num_bits > 0"));
+        }
+
+        if packed.len() * 8 < length * num_bits {
+            return Err(Error::oos(format!(
+                "Unpacking {length} items with a number of bits {num_bits} requires at least {} bytes.",
+                length * num_bits / 8
+            )));
+        }
+
+        let mut packed = packed.chunks(block_size);
+        let mut unpacked = T::Unpacked::zero();
+        if let Some(chunk) = packed.next() {
+            decode_pack::<T>(chunk, num_bits, &mut unpacked);
+        } else {
+            length = 0
+        };
+
+        Ok(Self {
+            remaining: length,
+            packed,
+            num_bits,
+            unpacked,
+            current_pack_index: 0,
+        })
+    }
+}
+
+impl<'a, T: Unpackable> Iterator for Decoder<'a, T> {
+    type Item = T;
+
+    #[inline] // -71% improvement in bench
+    fn next(&mut self) -> Option<Self::Item> {
+        if self.remaining == 0 {
+            return None;
+        }
+        let result = self.unpacked[self.current_pack_index];

----------------------------------------

File: core/rust/qdbr/parquet2/src/encoding/bitpacked/encode.rs
Status: added
Changes: +54 -0
Diff:
@@ -0,0 +1,54 @@
+use std::convert::TryInto;
+
+use super::{Packed, Unpackable, Unpacked};
+
+/// Encodes (packs) a slice of [`Unpackable`] into bitpacked bytes `packed`, using `num_bits` per value.
+///
+/// This function assumes that the maximum value in `unpacked` fits in `num_bits` bits
+/// and saturates higher values.
+///
+/// Only the first `ceil8(unpacked.len() * num_bits)` of `packed` are populated.
+pub fn encode<T: Unpackable>(unpacked: &[T], num_bits: usize, packed: &mut [u8]) {
+    let chunks = unpacked.chunks_exact(T::Unpacked::LENGTH);
+
+    let remainder = chunks.remainder();
+
+    let packed_size = (T::Unpacked::LENGTH * num_bits + 7) / 8;
+    if !remainder.is_empty() {

----------------------------------------

File: core/rust/qdbr/parquet2/src/encoding/bitpacked/mod.rs
Status: added
Changes: +220 -0
Diff:
@@ -0,0 +1,220 @@
+mod decode;
+mod encode;
+mod pack;
+mod unpack;
+
+pub use decode::Decoder;
+pub use encode::{encode, encode_pack};
+
+/// A byte slice (e.g. `[u8; 8]`) denoting types that represent complete packs.
+pub trait Packed:
+    Copy
+    + Sized
+    + AsRef<[u8]>
+    + AsMut<[u8]>
+    + std::ops::IndexMut<usize, Output = u8>
+    + for<'a> TryFrom<&'a [u8]>
+{
+    const LENGTH: usize;
+    fn zero() -> Self;
+}
+
+impl Packed for [u8; 8] {
+    const LENGTH: usize = 8;
+    #[inline]
+    fn zero() -> Self {
+        [0; 8]
+    }
+}
+
+impl Packed for [u8; 16 * 2] {
+    const LENGTH: usize = 16 * 2;
+    #[inline]
+    fn zero() -> Self {
+        [0; 16 * 2]
+    }
+}
+
+impl Packed for [u8; 32 * 4] {
+    const LENGTH: usize = 32 * 4;
+    #[inline]
+    fn zero() -> Self {
+        [0; 32 * 4]
+    }
+}
+
+impl Packed for [u8; 64 * 64] {
+    const LENGTH: usize = 64 * 64;
+    #[inline]
+    fn zero() -> Self {
+        [0; 64 * 64]
+    }
+}
+
+/// A byte slice of [`Unpackable`] denoting complete unpacked arrays.
+pub trait Unpacked<T>:
+    Copy
+    + Sized
+    + AsRef<[T]>
+    + AsMut<[T]>
+    + std::ops::Index<usize, Output = T>
+    + std::ops::IndexMut<usize, Output = T>
+    + for<'a> TryFrom<&'a [T], Error = std::array::TryFromSliceError>
+{
+    const LENGTH: usize;
+    fn zero() -> Self;
+}
+
+impl Unpacked<u8> for [u8; 8] {
+    const LENGTH: usize = 8;
+    #[inline]
+    fn zero() -> Self {
+        [0; 8]

----------------------------------------

File: core/rust/qdbr/parquet2/src/encoding/bitpacked/pack.rs
Status: added
Changes: +108 -0
Diff:
@@ -0,0 +1,108 @@
+/// Macro that generates a packing function taking the number of bits as a const generic
+macro_rules! pack_impl {
+    ($t:ty, $bytes:literal, $bits:tt) => {
+        pub fn pack<const NUM_BITS: usize>(input: &[$t; $bits], output: &mut [u8]) {
+            if NUM_BITS == 0 {
+                for out in output {
+                    *out = 0;
+                }
+                return;
+            }
+            assert!(NUM_BITS <= $bytes * 8);
+            assert!(output.len() >= NUM_BITS * $bytes);
+
+            let mask = match NUM_BITS {
+                $bits => <$t>::MAX,
+                _ => ((1 << NUM_BITS) - 1),
+            };
+
+            for i in 0..$bits {
+                let start_bit = i * NUM_BITS;
+                let end_bit = start_bit + NUM_BITS;
+
+                let start_bit_offset = start_bit % $bits;
+                let end_bit_offset = end_bit % $bits;
+                let start_byte = start_bit / $bits;
+                let end_byte = end_bit / $bits;
+                if start_byte != end_byte && end_bit_offset != 0 {
+                    let a = input[i] << start_bit_offset;
+                    let val_a = <$t>::to_le_bytes(a);
+                    for i in 0..$bytes {
+                        output[start_byte * $bytes + i] |= val_a[i]
+                    }
+
+                    let b = (input[i] >> (NUM_BITS - end_bit_offset)) & mask;
+                    let val_b = <$t>::to_le_bytes(b);

----------------------------------------

File: core/rust/qdbr/parquet2/src/encoding/bitpacked/unpack.rs
Status: added
Changes: +137 -0
Diff:
@@ -0,0 +1,137 @@
+// Licensed to the Apache Software Foundation (ASF) under one
+// or more contributor license agreements.  See the NOTICE file
+// distributed with this work for additional information
+// regarding copyright ownership.  The ASF licenses this file
+// to you under the Apache License, Version 2.0 (the
+// "License"); you may not use this file except in compliance
+// with the License.  You may obtain a copy of the License at
+//
+//   http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing,
+// software distributed under the License is distributed on an
+// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+// KIND, either express or implied.  See the License for the
+// specific language governing permissions and limitations
+// under the License.
+
+// Copied from https://github.com/apache/arrow-rs/blob/6859efa690d4c9530cf8a24053bc6ed81025a164/parquet/src/util/bit_pack.rs
+
+/// Macro that generates an unpack function taking the number of bits as a const generic
+macro_rules! unpack_impl {
+    ($t:ty, $bytes:literal, $bits:tt) => {
+        pub fn unpack<const NUM_BITS: usize>(input: &[u8], output: &mut [$t; $bits]) {
+            if NUM_BITS == 0 {
+                for out in output {
+                    *out = 0;
+                }
+                return;
+            }
+
+            assert!(NUM_BITS <= $bytes * 8);
+
+            let mask = match NUM_BITS {
+                $bits => <$t>::MAX,
+                _ => ((1 << NUM_BITS) - 1),
+            };
+
+            assert!(input.len() >= NUM_BITS * $bytes);
+
+            let r = |output_idx: usize| {
+                <$t>::from_le_bytes(
+                    input[output_idx * $bytes..output_idx * $bytes + $bytes]
+                        .try_into()
+                        .unwrap(),
+                )

----------------------------------------

File: core/rust/qdbr/parquet2/src/encoding/delta_bitpacked/decoder.rs
Status: added
Changes: +365 -0
Diff:
@@ -0,0 +1,365 @@
+use crate::encoding::ceil8;
+use crate::error::Error;
+
+use super::super::bitpacked;
+use super::super::uleb128;
+use super::super::zigzag_leb128;
+
+/// An [`Iterator`] of [`i64`]
+#[derive(Debug)]
+struct Block<'a> {
+    // this is the minimum delta that must be added to every value.
+    min_delta: i64,
+    _num_mini_blocks: usize,
+    /// Number of values that each mini block has.
+    values_per_mini_block: usize,
+    bitwidths: std::slice::Iter<'a, u8>,
+    values: &'a [u8],
+    remaining: usize,     // number of elements
+    current_index: usize, // invariant: < values_per_mini_block
+    // None represents a relative delta of zero, in which case there is no miniblock.
+    current_miniblock: Option<bitpacked::Decoder<'a, u64>>,
+    // number of bytes consumed.
+    consumed_bytes: usize,
+}
+
+impl<'a> Block<'a> {
+    pub fn try_new(
+        mut values: &'a [u8],
+        num_mini_blocks: usize,
+        values_per_mini_block: usize,
+        length: usize,
+    ) -> Result<Self, Error> {
+        let length = std::cmp::min(length, num_mini_blocks * values_per_mini_block);
+
+        let mut consumed_bytes = 0;
+        let (min_delta, consumed) = zigzag_leb128::decode(values)?;
+        consumed_bytes += consumed;
+        values = &values[consumed..];
+
+        if num_mini_blocks > values.len() {
+            return Err(Error::oos(
+                "Block must contain at least num_mini_blocks bytes (the bitwidths)",
+            ));
+        }
+        let (bitwidths, remaining) = values.split_at(num_mini_blocks);
+        consumed_bytes += num_mini_blocks;
+        values = remaining;
+
+        let mut block = Block {
+            min_delta,
+            _num_mini_blocks: num_mini_blocks,
+            values_per_mini_block,
+            bitwidths: bitwidths.iter(),
+            remaining: length,
+            values,
+            current_index: 0,
+            current_miniblock: None,
+            consumed_bytes,
+        };
+
+        // Set up first mini-block
+        block.advance_miniblock()?;
+
+        Ok(block)
+    }
+
+    fn advance_miniblock(&mut self) -> Result<(), Error> {
+        // unwrap is ok: we sliced it by num_mini_blocks in try_new
+        let num_bits = self.bitwidths.next().copied().unwrap() as usize;
+
+        self.current_miniblock = if num_bits > 0 {
+            let length = std::cmp::min(self.remaining, self.values_per_mini_block);
+
+            let miniblock_length = ceil8(self.values_per_mini_block * num_bits);
+            if miniblock_length > self.values.len() {
+                return Err(Error::oos(
+                    "block must contain at least miniblock_length bytes (the mini block)",
+                ));
+            }
+            let (miniblock, remainder) = self.values.split_at(miniblock_length);
+
+            self.values = remainder;
+            self.consumed_bytes += miniblock_length;
+
+            Some(bitpacked::Decoder::try_new(miniblock, num_bits, length).unwrap())
+        } else {
+            None
+        };
+        self.current_index = 0;
+
+        Ok(())
+    }
+}
+
+impl<'a> Iterator for Block<'a> {
+    type Item = Result<i64, Error>;
+
+    fn next(&mut self) -> Option<Self::Item> {
+        if self.remaining == 0 {
+            return None;
+        }
+        let result = self.min_delta
+            + self
+                .current_miniblock
+                .as_mut()
+                .map(|x| x.next().unwrap_or_default())
+                .unwrap_or(0) as i64;
+        self.current_index += 1;
+        self.remaining -= 1;
+
+        if self.remaining > 0 && self.current_index == self.values_per_mini_block {
+            if let Err(e) = self.advance_miniblock() {
+                return Some(Err(e));
+            }
+        }
+
+        Some(Ok(result))
+    }
+}
+
+/// Decoder of parquets' `DELTA_BINARY_PACKED`. Implements `Iterator<Item = i64>`.

----------------------------------------

File: core/rust/qdbr/parquet2/src/encoding/delta_bitpacked/encoder.rs
Status: added
Changes: +125 -0
Diff:
@@ -0,0 +1,125 @@
+use crate::encoding::ceil8;
+
+use super::super::bitpacked;
+use super::super::uleb128;
+use super::super::zigzag_leb128;
+
+/// Encodes an iterator of `i64` according to parquet's `DELTA_BINARY_PACKED`.
+/// # Implementation
+/// * This function does not allocate on the heap.
+/// * The number of mini-blocks is always 1. This may change in the future.
+pub fn encode<I: Iterator<Item = i64>>(mut iterator: I, buffer: &mut Vec<u8>) {
+    let block_size = 128;
+    let mini_blocks = 1;
+
+    let mut container = [0u8; 10];
+    let encoded_len = uleb128::encode(block_size, &mut container);
+    buffer.extend_from_slice(&container[..encoded_len]);
+
+    let encoded_len = uleb128::encode(mini_blocks, &mut container);
+    buffer.extend_from_slice(&container[..encoded_len]);
+
+    let length = iterator.size_hint().1.unwrap();
+    let encoded_len = uleb128::encode(length as u64, &mut container);
+    buffer.extend_from_slice(&container[..encoded_len]);
+
+    let mut values = [0i64; 128];
+    let mut deltas = [0u64; 128];
+
+    let first_value = iterator.next().unwrap_or_default();
+    let (container, encoded_len) = zigzag_leb128::encode(first_value);
+    buffer.extend_from_slice(&container[..encoded_len]);
+
+    let mut prev = first_value;
+    let mut length = iterator.size_hint().1.unwrap();
+    while length != 0 {
+        let mut min_delta = i64::MAX;
+        let mut max_delta = i64::MIN;
+        let mut num_bits = 0;
+        for (i, integer) in (0..128).zip(&mut iterator) {
+            let delta = integer - prev;
+            min_delta = min_delta.min(delta);

----------------------------------------

File: core/rust/qdbr/parquet2/src/encoding/delta_bitpacked/mod.rs
Status: added
Changes: +91 -0
Diff:
@@ -0,0 +1,91 @@
+mod decoder;
+mod encoder;
+
+pub use decoder::Decoder;
+pub use encoder::encode;
+
+#[cfg(test)]
+mod tests {
+    use crate::error::Error;
+
+    use super::*;
+
+    #[test]
+    fn basic() -> Result<(), Error> {
+        let data = vec![1, 3, 1, 2, 3];
+
+        let mut buffer = vec![];
+        encode(data.clone().into_iter(), &mut buffer);
+        let iter = Decoder::try_new(&buffer)?;
+
+        let result = iter.collect::<Result<Vec<_>, _>>()?;
+        assert_eq!(result, data);
+        Ok(())
+    }
+
+    #[test]
+    fn negative_value() -> Result<(), Error> {
+        let data = vec![1, 3, -1, 2, 3];
+

----------------------------------------

File: core/rust/qdbr/parquet2/src/encoding/delta_byte_array/decoder.rs
Status: added
Changes: +108 -0
Diff:
@@ -0,0 +1,108 @@
+use crate::error::Error;
+
+use super::super::delta_bitpacked;
+use super::super::delta_length_byte_array;
+
+/// Decodes according to [Delta strings](https://github.com/apache/parquet-format/blob/master/Encodings.md#delta-strings-delta_byte_array--7),
+/// prefixes, lengths and values
+/// # Implementation
+/// This struct does not allocate on the heap.
+#[derive(Debug)]
+pub struct Decoder<'a> {
+    values: &'a [u8],
+    prefix_lengths: delta_bitpacked::Decoder<'a>,
+}
+
+impl<'a> Decoder<'a> {
+    pub fn try_new(values: &'a [u8]) -> Result<Self, Error> {
+        let prefix_lengths = delta_bitpacked::Decoder::try_new(values)?;
+        Ok(Self {
+            values,
+            prefix_lengths,
+        })
+    }
+
+    pub fn into_lengths(self) -> Result<delta_length_byte_array::Decoder<'a>, Error> {
+        assert_eq!(self.prefix_lengths.size_hint().0, 0);
+        delta_length_byte_array::Decoder::try_new(
+            &self.values[self.prefix_lengths.consumed_bytes()..],
+        )
+    }
+}
+
+impl<'a> Iterator for Decoder<'a> {
+    type Item = Result<u32, Error>;
+

----------------------------------------

File: core/rust/qdbr/parquet2/src/encoding/delta_byte_array/encoder.rs
Status: added
Changes: +33 -0
Diff:
@@ -0,0 +1,33 @@
+use crate::encoding::delta_length_byte_array;
+
+use super::super::delta_bitpacked;
+
+/// Encodes an iterator of according to DELTA_BYTE_ARRAY
+pub fn encode<'a, I: Iterator<Item = &'a [u8]> + Clone>(iterator: I, buffer: &mut Vec<u8>) {
+    let mut previous = b"".as_ref();
+
+    let mut sum_lengths = 0;
+    let prefixes = iterator

----------------------------------------

File: core/rust/qdbr/parquet2/src/encoding/delta_byte_array/mod.rs
Status: added
Changes: +33 -0
Diff:
@@ -0,0 +1,33 @@
+mod decoder;
+mod encoder;
+
+pub use decoder::Decoder;
+pub use encoder::encode;
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+    use crate::error::Error;

----------------------------------------

File: core/rust/qdbr/parquet2/src/encoding/delta_length_byte_array/decoder.rs
Status: added
Changes: +81 -0
Diff:
@@ -0,0 +1,81 @@
+use crate::error::Error;
+
+use super::super::delta_bitpacked;
+
+/// Decodes [Delta-length byte array](https://github.com/apache/parquet-format/blob/master/Encodings.md#delta-length-byte-array-delta_length_byte_array--6)
+/// lengths and values.
+/// # Implementation
+/// This struct does not allocate on the heap.
+/// # Example
+/// ```
+/// use parquet2::encoding::delta_length_byte_array::Decoder;
+///
+/// let expected = &["Hello", "World"];
+/// let expected_lengths = expected.iter().map(|x| x.len() as i32).collect::<Vec<_>>();
+/// let expected_values = expected.join("");
+/// let expected_values = expected_values.as_bytes();
+/// let data = &[
+///     128, 1, 4, 2, 10, 0, 0, 0, 0, 0, 72, 101, 108, 108, 111, 87, 111, 114, 108, 100,
+/// ];
+///
+/// let mut decoder = Decoder::try_new(data).unwrap();
+///
+/// // Extract the lengths
+/// let lengths = decoder.by_ref().collect::<Result<Vec<_>, _>>().unwrap();
+/// assert_eq!(lengths, expected_lengths);
+///

----------------------------------------

File: core/rust/qdbr/parquet2/src/encoding/delta_length_byte_array/encoder.rs
Status: added
Changes: +19 -0
Diff:
@@ -0,0 +1,19 @@
+use crate::encoding::delta_bitpacked;
+
+/// Encodes a clonable iterator of `&[u8]` into `buffer`. This does not allocated on the heap.
+/// # Implementation
+/// This encoding is equivalent to call [`delta_bitpacked::encode`] on the lengths of the items
+/// of the iterator followed by extending the buffer from each item of the iterator.
+pub fn encode<A: AsRef<[u8]>, I: Iterator<Item = A> + Clone>(iterator: I, buffer: &mut Vec<u8>) {
+    let mut total_length = 0;
+    delta_bitpacked::encode(

----------------------------------------

File: core/rust/qdbr/parquet2/src/encoding/delta_length_byte_array/mod.rs
Status: added
Changes: +51 -0
Diff:
@@ -0,0 +1,51 @@
+mod decoder;
+mod encoder;
+
+pub use decoder::Decoder;
+pub use encoder::encode;
+
+#[cfg(test)]
+mod tests {
+    use crate::error::Error;
+
+    use super::*;
+
+    #[test]
+    fn basic() -> Result<(), Error> {
+        let data = vec!["aa", "bbb", "a", "aa", "b"];
+

----------------------------------------

File: core/rust/qdbr/parquet2/src/encoding/hybrid_rle/bitmap.rs
Status: added
Changes: +102 -0
Diff:
@@ -0,0 +1,102 @@
+use std::io::Write;
+
+const BIT_MASK: [u8; 8] = [1, 2, 4, 8, 16, 32, 64, 128];
+
+/// Sets bit at position `i` in `byte`
+#[inline]
+pub fn set(byte: u8, i: usize) -> u8 {
+    byte | BIT_MASK[i]
+}
+
+/// An [`Iterator`] of bool that decodes a bitmap.
+/// This is a specialization of [`super::super::bitpacked::Decoder`] for `num_bits == 1`.
+#[derive(Debug)]
+pub struct BitmapIter<'a> {
+    iter: std::slice::Iter<'a, u8>,
+    current_byte: &'a u8,
+    remaining: usize,
+    mask: u8,
+}
+
+impl<'a> BitmapIter<'a> {
+    /// Returns a new [`BitmapIter`].
+    /// # Panics
+    /// This function panics iff `offset / 8 > slice.len()`
+    #[inline]
+    pub fn new(slice: &'a [u8], offset: usize, len: usize) -> Self {
+        let bytes = &slice[offset / 8..];
+
+        let mut iter = bytes.iter();
+
+        let current_byte = iter.next().unwrap_or(&0);
+
+        Self {

----------------------------------------

File: core/rust/qdbr/parquet2/src/encoding/hybrid_rle/decoder.rs
Status: added
Changes: +144 -0
Diff:
@@ -0,0 +1,144 @@
+use crate::error::Error;
+
+use super::super::uleb128;
+use super::{super::ceil8, HybridEncoded};
+
+/// An [`Iterator`] of [`HybridEncoded`].
+#[derive(Debug, Clone)]
+pub struct Decoder<'a> {
+    values: &'a [u8],
+    num_bits: usize,
+}
+
+impl<'a> Decoder<'a> {
+    /// Returns a new [`Decoder`]
+    pub fn new(values: &'a [u8], num_bits: usize) -> Self {
+        Self { values, num_bits }
+    }
+
+    /// Returns the number of bits being used by this decoder.
+    #[inline]
+    pub fn num_bits(&self) -> usize {
+        self.num_bits
+    }
+}
+
+impl<'a> Iterator for Decoder<'a> {
+    type Item = Result<HybridEncoded<'a>, Error>;
+
+    #[inline] // -18% improvement in bench
+    fn next(&mut self) -> Option<Self::Item> {
+        if self.num_bits == 0 {
+            return None;
+        }
+
+        if self.values.is_empty() {
+            return None;
+        }
+
+        let (indicator, consumed) = match uleb128::decode(self.values) {
+            Ok((indicator, consumed)) => (indicator, consumed),
+            Err(e) => return Some(Err(e)),
+        };
+        self.values = &self.values[consumed..];
+        if self.values.is_empty() {
+            return None;
+        };
+

----------------------------------------

File: core/rust/qdbr/parquet2/src/encoding/hybrid_rle/encoder.rs
Status: added
Changes: +161 -0
Diff:
@@ -0,0 +1,161 @@
+use crate::encoding::bitpacked;
+use crate::encoding::{ceil8, uleb128};
+
+use std::io::Write;
+
+use super::bitpacked_encode;
+
+/// RLE-hybrid encoding of `u32`. This currently only yields bitpacked values.
+pub fn encode_u32<W: Write, I: Iterator<Item = u32>>(
+    writer: &mut W,
+    iterator: I,
+    num_bits: u32,
+) -> std::io::Result<()> {
+    let num_bits = num_bits as u8;
+    encode_header(writer, &iterator)?;
+    bitpacked_encode_u32(writer, iterator, num_bits as usize)?;
+    Ok(())
+}
+
+fn encode_header<W: Write, T, I: Iterator<Item=T>>(writer: &mut W, iterator: &I) -> std::io::Result<()> {
+    // the length of the iterator.
+    let length = iterator.size_hint().1.unwrap();
+
+    // write the length + indicator
+    let mut header = ceil8(length) as u64;
+    header <<= 1;
+    header |= 1; // it is bitpacked => first bit is set
+    let mut container = [0; 10];
+    let used = uleb128::encode(header, &mut container);
+    writer.write_all(&container[..used])?;
+    Ok(())
+}
+
+const U32_BLOCK_LEN: usize = 32;
+
+fn bitpacked_encode_u32<W: Write, I: Iterator<Item = u32>>(
+    writer: &mut W,
+    mut iterator: I,
+    num_bits: usize,
+) -> std::io::Result<()> {
+    // the length of the iterator.
+    let length = iterator.size_hint().1.unwrap();
+
+    let chunks = length / U32_BLOCK_LEN;
+    let remainder = length - chunks * U32_BLOCK_LEN;
+    let mut buffer = [0u32; U32_BLOCK_LEN];
+
+    let compressed_chunk_size = ceil8(U32_BLOCK_LEN * num_bits);
+
+    for _ in 0..chunks {
+        iterator
+            .by_ref()
+            .take(U32_BLOCK_LEN)

----------------------------------------

File: core/rust/qdbr/parquet2/src/encoding/hybrid_rle/mod.rs
Status: added
Changes: +264 -0
Diff:
@@ -0,0 +1,264 @@
+// See https://github.com/apache/parquet-format/blob/master/Encodings.md#run-length-encoding--bit-packing-hybrid-rle--3
+mod bitmap;
+mod decoder;
+mod encoder;
+pub use bitmap::{encode_bool as bitpacked_encode, BitmapIter};
+pub use decoder::Decoder;
+pub use encoder::{encode_bool, encode_u32};
+
+use crate::error::Error;
+
+use super::bitpacked;
+
+/// The two possible states of an RLE-encoded run.
+#[derive(Debug, Clone, Copy, PartialEq, Eq)]
+pub enum HybridEncoded<'a> {
+    /// A bitpacked slice. The consumer must know its bit-width to unpack it.
+    Bitpacked(&'a [u8]),
+    /// A RLE-encoded slice. The first attribute corresponds to the slice (that can be interpreted)
+    /// the second attribute corresponds to the number of repetitions.
+    Rle(&'a [u8], usize),
+}
+
+#[derive(Debug, Clone)]
+enum State<'a> {
+    None,
+    Bitpacked(bitpacked::Decoder<'a, u32>),
+    Rle(std::iter::Take<std::iter::Repeat<u32>>),
+    // Add a special branch for a single value to
+    // adhere to the strong law of small numbers.
+    Single(Option<u32>),
+}
+
+/// [`Iterator`] of [`u32`] from a byte slice of Hybrid-RLE encoded values
+#[derive(Debug, Clone)]
+pub struct HybridRleDecoder<'a> {
+    decoder: Decoder<'a>,
+    state: State<'a>,
+    remaining: usize,
+}
+
+#[inline]
+fn read_next<'a>(decoder: &mut Decoder<'a>, remaining: usize) -> Result<State<'a>, Error> {
+    Ok(match decoder.next().transpose()? {
+        Some(HybridEncoded::Bitpacked(packed)) => {
+            let num_bits = decoder.num_bits();
+            let length = std::cmp::min(packed.len() * 8 / num_bits, remaining);
+            let decoder = bitpacked::Decoder::<u32>::try_new(packed, num_bits, length)?;
+            State::Bitpacked(decoder)
+        }
+        Some(HybridEncoded::Rle(pack, additional)) => {
+            let mut bytes = [0u8; std::mem::size_of::<u32>()];
+            pack.iter().zip(bytes.iter_mut()).for_each(|(src, dst)| {
+                *dst = *src;
+            });
+            let value = u32::from_le_bytes(bytes);
+            if additional == 1 {
+                State::Single(Some(value))
+            } else {
+                State::Rle(std::iter::repeat(value).take(additional))
+            }
+        }
+        None => State::None,
+    })
+}
+
+impl<'a> HybridRleDecoder<'a> {
+    /// Returns a new [`HybridRleDecoder`]
+    pub fn try_new(data: &'a [u8], num_bits: u32, num_values: usize) -> Result<Self, Error> {
+        let num_bits = num_bits as usize;
+        let mut decoder = Decoder::new(data, num_bits);
+        let state = read_next(&mut decoder, num_values)?;
+        Ok(Self {
+            decoder,
+            state,
+            remaining: num_values,
+        })
+    }
+}
+
+impl<'a> Iterator for HybridRleDecoder<'a> {
+    type Item = Result<u32, Error>;
+
+    fn next(&mut self) -> Option<Self::Item> {
+        if self.remaining == 0 {
+            return None;
+        };
+        let result = match &mut self.state {

----------------------------------------

File: core/rust/qdbr/parquet2/src/encoding/mod.rs
Status: added
Changes: +27 -0
Diff:
@@ -0,0 +1,27 @@
+use std::convert::TryInto;
+
+pub mod bitpacked;
+pub mod delta_bitpacked;
+pub mod delta_byte_array;
+pub mod delta_length_byte_array;
+pub mod hybrid_rle;
+pub mod plain_byte_array;
+pub mod uleb128;

----------------------------------------

File: core/rust/qdbr/parquet2/src/encoding/plain_byte_array.rs
Status: added
Changes: +46 -0
Diff:
@@ -0,0 +1,46 @@
+/// Decodes according to [Plain strings](https://github.com/apache/parquet-format/blob/master/Encodings.md#plain-plain--0),
+/// prefixes, lengths and values
+/// # Implementation
+/// This struct does not allocate on the heap.
+use crate::error::Error;
+
+#[derive(Debug)]
+pub struct BinaryIter<'a> {
+    values: &'a [u8],
+    length: Option<usize>,
+}
+
+impl<'a> BinaryIter<'a> {
+    pub fn new(values: &'a [u8], length: Option<usize>) -> Self {

----------------------------------------

File: core/rust/qdbr/parquet2/src/encoding/uleb128.rs
Status: added
Changes: +97 -0
Diff:
@@ -0,0 +1,97 @@
+use crate::error::Error;
+
+pub fn decode(values: &[u8]) -> Result<(u64, usize), Error> {
+    let mut result = 0;
+    let mut shift = 0;
+
+    let mut consumed = 0;
+    for byte in values {
+        consumed += 1;
+        if shift == 63 && *byte > 1 {
+            panic!()
+        };
+
+        result |= u64::from(byte & 0b01111111) << shift;
+
+        if byte & 0b10000000 == 0 {
+            break;
+        }
+
+        shift += 7;
+    }
+    Ok((result, consumed))
+}
+
+/// Encodes `value` in ULEB128 into `container`. The exact number of bytes written
+/// depends on `value`, and cannot be determined upfront. The maximum number of bytes
+/// required are 10.
+/// # Panic
+/// This function may panic if `container.len() < 10` and `value` requires more bytes.
+pub fn encode(mut value: u64, container: &mut [u8]) -> usize {
+    let mut consumed = 0;

----------------------------------------

File: core/rust/qdbr/parquet2/src/encoding/zigzag_leb128.rs
Status: added
Changes: +70 -0
Diff:
@@ -0,0 +1,70 @@
+use crate::error::Error;
+
+use super::uleb128;
+
+pub fn decode(values: &[u8]) -> Result<(i64, usize), Error> {
+    let (u, consumed) = uleb128::decode(values)?;
+    Ok(((u >> 1) as i64 ^ -((u & 1) as i64), consumed))
+}
+
+pub fn encode(value: i64) -> ([u8; 10], usize) {
+    let value = ((value << 1) ^ (value >> (64 - 1))) as u64;
+    let mut a = [0u8; 10];
+    let produced = uleb128::encode(value, &mut a);
+    (a, produced)
+}
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_decode() {

----------------------------------------

File: core/rust/qdbr/parquet2/src/error.rs
Status: added
Changes: +122 -0
Diff:
@@ -0,0 +1,122 @@
+//! Contains [`Error`]
+
+/// List of features whose non-activation may cause a runtime error.
+/// Used to indicate which lack of feature caused [`Error::FeatureNotActive`].
+#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]
+#[non_exhaustive]
+pub enum Feature {
+    /// Snappy compression and decompression
+    Snappy,
+    /// Brotli compression and decompression
+    Brotli,
+    /// Gzip compression and decompression
+    Gzip,
+    /// Lz4 raw compression and decompression
+    Lz4,
+    /// Zstd compression and decompression
+    Zstd,
+}
+
+/// Errors generated by this crate
+#[derive(Debug, Clone)]
+#[non_exhaustive]
+pub enum Error {
+    /// When the parquet file is known to be out of spec.
+    OutOfSpec(String),
+    /// Error presented when trying to use a code branch that requires activating a feature.
+    FeatureNotActive(Feature, String),
+    /// Error presented when trying to use a feature from parquet that is not yet supported
+    FeatureNotSupported(String),
+    /// When encoding, the user passed an invalid parameter
+    InvalidParameter(String),
+    /// When decoding or decompressing, the page would allocate more memory than allowed
+    WouldOverAllocate,
+}
+
+impl Error {
+    pub fn oos<I: Into<String>>(message: I) -> Self {
+        Self::OutOfSpec(message.into())
+    }
+}

----------------------------------------

File: core/rust/qdbr/parquet2/src/indexes/index.rs
Status: added
Changes: +321 -0
Diff:
@@ -0,0 +1,321 @@
+use std::any::Any;
+
+use parquet_format_safe::ColumnIndex;
+
+use crate::parquet_bridge::BoundaryOrder;
+use crate::schema::types::PrimitiveType;
+use crate::{error::Error, schema::types::PhysicalType, types::NativeType};
+
+/// Trait object representing a [`ColumnIndex`] in Rust's native format.
+///
+/// See [`NativeIndex`], [`ByteIndex`] and [`FixedLenByteIndex`] for concrete implementations.
+pub trait Index: Send + Sync + std::fmt::Debug {
+    fn as_any(&self) -> &dyn Any;
+
+    fn physical_type(&self) -> &PhysicalType;
+}
+
+impl PartialEq for dyn Index + '_ {
+    fn eq(&self, that: &dyn Index) -> bool {
+        equal(self, that)
+    }
+}
+
+impl Eq for dyn Index + '_ {}
+
+fn equal(lhs: &dyn Index, rhs: &dyn Index) -> bool {
+    if lhs.physical_type() != rhs.physical_type() {
+        return false;
+    }
+
+    match lhs.physical_type() {
+        PhysicalType::Boolean => {
+            lhs.as_any().downcast_ref::<BooleanIndex>().unwrap()
+                == rhs.as_any().downcast_ref::<BooleanIndex>().unwrap()
+        }
+        PhysicalType::Int32 => {
+            lhs.as_any().downcast_ref::<NativeIndex<i32>>().unwrap()
+                == rhs.as_any().downcast_ref::<NativeIndex<i32>>().unwrap()
+        }
+        PhysicalType::Int64 => {
+            lhs.as_any().downcast_ref::<NativeIndex<i64>>().unwrap()
+                == rhs.as_any().downcast_ref::<NativeIndex<i64>>().unwrap()
+        }
+        PhysicalType::Int96 => {
+            lhs.as_any()
+                .downcast_ref::<NativeIndex<[u32; 3]>>()
+                .unwrap()
+                == rhs
+                    .as_any()
+                    .downcast_ref::<NativeIndex<[u32; 3]>>()
+                    .unwrap()
+        }
+        PhysicalType::Float => {
+            lhs.as_any().downcast_ref::<NativeIndex<f32>>().unwrap()
+                == rhs.as_any().downcast_ref::<NativeIndex<f32>>().unwrap()
+        }
+        PhysicalType::Double => {
+            lhs.as_any().downcast_ref::<NativeIndex<f64>>().unwrap()
+                == rhs.as_any().downcast_ref::<NativeIndex<f64>>().unwrap()
+        }
+        PhysicalType::ByteArray => {
+            lhs.as_any().downcast_ref::<ByteIndex>().unwrap()
+                == rhs.as_any().downcast_ref::<ByteIndex>().unwrap()
+        }
+        PhysicalType::FixedLenByteArray(_) => {
+            lhs.as_any().downcast_ref::<FixedLenByteIndex>().unwrap()
+                == rhs.as_any().downcast_ref::<FixedLenByteIndex>().unwrap()
+        }
+    }
+}
+
+/// An index of a column of [`NativeType`] physical representation
+#[derive(Debug, Clone, PartialEq, Eq, Hash)]
+pub struct NativeIndex<T: NativeType> {
+    /// The primitive type
+    pub primitive_type: PrimitiveType,
+    /// The indexes, one item per page
+    pub indexes: Vec<PageIndex<T>>,
+    /// the order
+    pub boundary_order: BoundaryOrder,
+}
+
+impl<T: NativeType> NativeIndex<T> {
+    /// Creates a new [`NativeIndex`]
+    pub(crate) fn try_new(
+        index: ColumnIndex,
+        primitive_type: PrimitiveType,
+    ) -> Result<Self, Error> {
+        let len = index.min_values.len();
+
+        let null_counts = index
+            .null_counts
+            .map(|x| x.into_iter().map(Some).collect::<Vec<_>>())
+            .unwrap_or_else(|| vec![None; len]);
+
+        let indexes = index
+            .min_values
+            .iter()
+            .zip(index.max_values.into_iter())
+            .zip(index.null_pages.into_iter())
+            .zip(null_counts.into_iter())
+            .map(|(((min, max), is_null), null_count)| {
+                let (min, max) = if is_null {
+                    (None, None)
+                } else {
+                    let min = min.as_slice().try_into()?;

----------------------------------------

File: core/rust/qdbr/parquet2/src/indexes/intervals.rs
Status: added
Changes: +138 -0
Diff:
@@ -0,0 +1,138 @@
+use parquet_format_safe::PageLocation;
+
+use crate::error::Error;
+
+#[cfg(feature = "serde_types")]
+use serde::{Deserialize, Serialize};
+
+/// An interval
+#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]
+#[cfg_attr(feature = "serde_types", derive(Deserialize, Serialize))]
+pub struct Interval {
+    /// Its start
+    pub start: usize,
+    /// Its length
+    pub length: usize,
+}
+
+impl Interval {
+    /// Create a new interal
+    pub fn new(start: usize, length: usize) -> Self {
+        Self { start, length }
+    }
+}
+
+/// Returns the set of (row) intervals of the pages.
+/// # Errors
+/// This function errors if the locations are not castable to `usize` or such that
+/// their ranges of row are larger than `num_rows`.
+pub fn compute_page_row_intervals(
+    locations: &[PageLocation],
+    num_rows: usize,
+) -> Result<Vec<Interval>, Error> {
+    if locations.is_empty() {
+        return Ok(vec![]);
+    };
+
+    let last = (|| {
+        let start: usize = locations.last().unwrap().first_row_index.try_into()?;
+        let length = num_rows
+            .checked_sub(start)
+            .ok_or_else(|| Error::oos("Page start cannot be smaller than the number of rows"))?;
+        Result::<_, Error>::Ok(Interval::new(start, length))
+    })();
+
+    let pages_lengths = locations

----------------------------------------

File: core/rust/qdbr/parquet2/src/indexes/mod.rs
Status: added
Changes: +235 -0
Diff:
@@ -0,0 +1,235 @@
+mod index;
+mod intervals;
+
+pub use crate::parquet_bridge::BoundaryOrder;
+pub use crate::thrift_format::PageLocation;
+
+pub use self::index::{BooleanIndex, ByteIndex, FixedLenByteIndex, Index, NativeIndex, PageIndex};
+pub use intervals::{compute_rows, select_pages, FilteredPage, Interval};
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    use crate::schema::types::{PhysicalType, PrimitiveType};
+
+    #[test]
+    fn test_basic() {
+        let locations = &[PageLocation {
+            offset: 100,
+            compressed_page_size: 10,
+            first_row_index: 0,
+        }];
+        let num_rows = 10;
+
+        let row_intervals = compute_rows(&[true; 1], locations, num_rows).unwrap();
+        assert_eq!(row_intervals, vec![Interval::new(0, 10)])
+    }
+
+    #[test]
+    fn test_multiple() {
+        // two pages
+        let index = ByteIndex {
+            primitive_type: PrimitiveType::from_physical("c1".to_string(), PhysicalType::ByteArray),
+            indexes: vec![
+                PageIndex {
+                    min: Some(vec![0]),
+                    max: Some(vec![8, 9]),
+                    null_count: Some(0),
+                },
+                PageIndex {
+                    min: Some(vec![20]),
+                    max: Some(vec![98, 99]),
+                    null_count: Some(0),
+                },
+            ],
+            boundary_order: Default::default(),
+        };
+        let locations = &[
+            PageLocation {
+                offset: 100,
+                compressed_page_size: 10,
+                first_row_index: 0,
+            },
+            PageLocation {
+                offset: 110,
+                compressed_page_size: 20,
+                first_row_index: 5,
+            },
+        ];
+        let num_rows = 10;
+
+        // filter of the form `x > "a"`
+        let selector = |page: &PageIndex<Vec<u8>>| {
+            page.max
+                .as_ref()
+                .map(|x| x.as_slice()[0] > 97)
+                .unwrap_or(false) // no max is present => all nulls => not selected
+        };
+        let selected = index.indexes.iter().map(selector).collect::<Vec<_>>();
+
+        let rows = compute_rows(&selected, locations, num_rows).unwrap();
+        assert_eq!(rows, vec![Interval::new(5, 5)]);
+
+        let pages = select_pages(&rows, locations, num_rows).unwrap();
+
+        assert_eq!(
+            pages,

----------------------------------------

File: core/rust/qdbr/parquet2/src/lib.rs
Status: added
Changes: +43 -0
Diff:
@@ -0,0 +1,43 @@
+#![forbid(unsafe_code)]
+#![cfg_attr(docsrs, feature(doc_cfg))]
+/// Unofficial implementation of parquet IO in Rust.
+
+#[macro_use]
+pub mod error;
+#[cfg(feature = "bloom_filter")]
+pub mod bloom_filter;
+pub mod compression;
+pub mod deserialize;
+pub mod encoding;
+pub mod indexes;
+pub mod metadata;

----------------------------------------

File: core/rust/qdbr/parquet2/src/metadata/column_chunk_metadata.rs
Status: added
Changes: +209 -0
Diff:
@@ -0,0 +1,209 @@
+use std::sync::Arc;
+
+use parquet_format_safe::{ColumnChunk, ColumnMetaData, Encoding};
+
+use super::column_descriptor::ColumnDescriptor;
+use crate::compression::Compression;
+use crate::error::{Error, Result};
+use crate::schema::types::PhysicalType;
+use crate::statistics::{deserialize_statistics, Statistics};
+
+#[cfg(feature = "serde_types")]
+mod serde_types {
+    pub use parquet_format_safe::thrift::protocol::{
+        TCompactInputProtocol, TCompactOutputProtocol,
+    };
+    pub use serde::de::Error as DeserializeError;
+    pub use serde::ser::Error as SerializeError;
+    pub use serde::{Deserialize, Deserializer, Serialize, Serializer};
+    pub use std::io::Cursor;
+}
+#[cfg(feature = "serde_types")]
+use serde_types::*;
+
+/// Metadata for a column chunk.
+// This contains the `ColumnDescriptor` associated with the chunk so that deserializers have
+// access to the descriptor (e.g. physical, converted, logical).
+#[derive(Debug, Clone)]
+#[cfg_attr(feature = "serde_types", derive(Deserialize, Serialize))]
+pub struct ColumnChunkMetaData {
+    #[cfg_attr(
+        feature = "serde_types",
+        serde(serialize_with = "serialize_column_chunk")
+    )]
+    #[cfg_attr(
+        feature = "serde_types",
+        serde(deserialize_with = "deserialize_column_chunk")
+    )]
+    column_chunk: ColumnChunk,
+    column_descr: ColumnDescriptor,
+}
+
+#[cfg(feature = "serde_types")]
+fn serialize_column_chunk<S>(
+    column_chunk: &ColumnChunk,
+    serializer: S,
+) -> std::result::Result<S::Ok, S::Error>
+where
+    S: Serializer,
+{
+    let mut buf = vec![];
+    let cursor = Cursor::new(&mut buf[..]);
+    let mut protocol = TCompactOutputProtocol::new(cursor);
+    column_chunk
+        .write_to_out_protocol(&mut protocol)
+        .map_err(S::Error::custom)?;
+    serializer.serialize_bytes(&buf)
+}
+
+#[cfg(feature = "serde_types")]
+fn deserialize_column_chunk<'de, D>(deserializer: D) -> std::result::Result<ColumnChunk, D::Error>
+where
+    D: Deserializer<'de>,
+{
+    let buf = Vec::<u8>::deserialize(deserializer)?;
+    let mut cursor = Cursor::new(&buf[..]);
+    let mut protocol = TCompactInputProtocol::new(&mut cursor, usize::MAX);
+    ColumnChunk::read_from_in_protocol(&mut protocol).map_err(D::Error::custom)
+}
+

----------------------------------------

File: core/rust/qdbr/parquet2/src/metadata/column_descriptor.rs
Status: added
Changes: +49 -0
Diff:
@@ -0,0 +1,49 @@
+use crate::schema::types::{ParquetType, PrimitiveType};
+#[cfg(feature = "serde_types")]
+use serde::{Deserialize, Serialize};
+
+/// A descriptor of a parquet column. It contains the necessary information to deserialize
+/// a parquet column.
+#[derive(Debug, Clone, PartialEq, Eq, Hash)]
+#[cfg_attr(feature = "serde_types", derive(Deserialize, Serialize))]
+pub struct Descriptor {
+    /// The [`PrimitiveType`] of this column
+    pub primitive_type: PrimitiveType,
+
+    /// The maximum definition level
+    pub max_def_level: i16,
+

----------------------------------------

File: core/rust/qdbr/parquet2/src/metadata/column_order.rs
Status: added
Changes: +29 -0
Diff:
@@ -0,0 +1,29 @@
+use super::sort::SortOrder;
+#[cfg(feature = "serde_types")]
+use serde::{Deserialize, Serialize};
+
+/// Column order that specifies what method was used to aggregate min/max values for
+/// statistics.
+///
+/// If column order is undefined, then it is the legacy behaviour and all values should
+/// be compared as signed values/bytes.

----------------------------------------

File: core/rust/qdbr/parquet2/src/metadata/file_metadata.rs
Status: added
Changes: +127 -0
Diff:
@@ -0,0 +1,127 @@
+use crate::{error::Error, metadata::get_sort_order};
+
+use super::{column_order::ColumnOrder, schema_descriptor::SchemaDescriptor, RowGroupMetaData};
+use parquet_format_safe::ColumnOrder as TColumnOrder;
+
+pub use crate::thrift_format::KeyValue;
+
+/// Metadata for a Parquet file.
+// This is almost equal to [`parquet_format_safe::FileMetaData`] but contains the descriptors,
+// which are crucial to deserialize pages.
+#[derive(Debug, Clone)]
+pub struct FileMetaData {
+    /// version of this file.
+    pub version: i32,
+    /// number of rows in the file.
+    pub num_rows: usize,
+    /// String message for application that wrote this file.
+    ///
+    /// This should have the following format:
+    /// `<application> version <application version> (build <application build hash>)`.
+    ///
+    /// ```shell
+    /// parquet-mr version 1.8.0 (build 0fda28af84b9746396014ad6a415b90592a98b3b)
+    /// ```
+    pub created_by: Option<String>,
+    /// The row groups of this file
+    pub row_groups: Vec<RowGroupMetaData>,
+    /// key_value_metadata of this file.
+    pub key_value_metadata: Option<Vec<KeyValue>>,
+    /// schema descriptor.
+    pub schema_descr: SchemaDescriptor,
+    /// Column (sort) order used for `min` and `max` values of each column in this file.
+    ///
+    /// Each column order corresponds to one column, determined by its position in the
+    /// list, matching the position of the column in the schema.
+    ///
+    /// When `None` is returned, there are no column orders available, and each column
+    /// should be assumed to have undefined (legacy) column order.
+    pub column_orders: Option<Vec<ColumnOrder>>,
+}
+

----------------------------------------

File: core/rust/qdbr/parquet2/src/metadata/mod.rs
Status: added
Changes: +18 -0
Diff:
@@ -0,0 +1,18 @@
+mod column_chunk_metadata;
+mod column_descriptor;
+mod column_order;
+mod file_metadata;
+mod row_metadata;
+mod schema_descriptor;
+mod sort;
+
+pub use column_chunk_metadata::ColumnChunkMetaData;

----------------------------------------

File: core/rust/qdbr/parquet2/src/metadata/row_metadata.rs
Status: added
Changes: +126 -0
Diff:
@@ -0,0 +1,126 @@
+use parquet_format_safe::{RowGroup, SortingColumn};
+
+use super::{column_chunk_metadata::ColumnChunkMetaData, schema_descriptor::SchemaDescriptor};
+use crate::{
+    error::{Error, Result},
+    write::ColumnOffsetsMetadata,
+};
+#[cfg(feature = "serde_types")]
+use serde::{Deserialize, Serialize};
+
+/// Metadata for a row group.
+#[derive(Debug, Clone)]
+#[cfg_attr(feature = "serde_types", derive(Deserialize, Serialize))]
+pub struct RowGroupMetaData {
+    columns: Vec<ColumnChunkMetaData>,
+    num_rows: usize,
+    sorting_columns: Option<Vec<SortingColumn>>,
+    total_byte_size: usize,
+}
+
+impl RowGroupMetaData {
+    /// Create a new [`RowGroupMetaData`]
+    pub fn _new(
+        columns: Vec<ColumnChunkMetaData>,
+        num_rows: usize,
+        total_byte_size: usize,
+    ) -> RowGroupMetaData {
+        Self {
+            columns,
+            num_rows,
+            sorting_columns: None,
+            total_byte_size,
+        }
+    }
+
+    pub fn with_sorting_columns(
+        columns: Vec<ColumnChunkMetaData>,
+        num_rows: usize,
+        sorting_columns: Option<Vec<SortingColumn>>,
+        total_byte_size: usize,
+    ) -> RowGroupMetaData {

----------------------------------------

File: core/rust/qdbr/parquet2/src/metadata/schema_descriptor.rs
Status: added
Changes: +144 -0
Diff:
@@ -0,0 +1,144 @@
+use parquet_format_safe::SchemaElement;
+
+use crate::{
+    error::Error,
+    schema::{io_message::from_message, types::ParquetType, Repetition},
+};
+use crate::{error::Result, schema::types::FieldInfo};
+
+use super::column_descriptor::{ColumnDescriptor, Descriptor};
+
+#[cfg(feature = "serde_types")]
+use serde::{Deserialize, Serialize};
+
+/// A schema descriptor. This encapsulates the top-level schemas for all the columns,
+/// as well as all descriptors for all the primitive columns.
+#[derive(Debug, Clone)]
+#[cfg_attr(feature = "serde_types", derive(Deserialize, Serialize))]
+pub struct SchemaDescriptor {
+    name: String,
+    // The top-level schema (the "message" type).
+    fields: Vec<ParquetType>,
+
+    // All the descriptors for primitive columns in this schema, constructed from
+    // `schema` in DFS order.
+    leaves: Vec<ColumnDescriptor>,
+}
+
+impl SchemaDescriptor {
+    /// Creates new schema descriptor from Parquet schema.
+    pub fn new(name: String, fields: Vec<ParquetType>) -> Self {
+        let mut leaves = vec![];
+        for f in &fields {
+            let mut path = vec![];
+            build_tree(f, f, 0, 0, &mut leaves, &mut path);
+        }
+
+        Self {
+            name,
+            fields,
+            leaves,
+        }
+    }
+
+    /// The [`ColumnDescriptor`] (leafs) of this schema.
+    ///
+    /// Note that, for nested fields, this may contain more entries than the number of fields
+    /// in the file - e.g. a struct field may have two columns.

----------------------------------------

File: core/rust/qdbr/parquet2/src/metadata/sort.rs
Status: added
Changes: +94 -0
Diff:
@@ -0,0 +1,94 @@
+use crate::schema::types::{
+    IntegerType, PhysicalType, PrimitiveConvertedType, PrimitiveLogicalType,
+};
+
+#[cfg(feature = "serde_types")]
+use serde::{Deserialize, Serialize};
+
+/// Sort order for page and column statistics.
+///
+/// Types are associated with sort orders and column stats are aggregated using a sort
+/// order, and a sort order should be considered when comparing values with statistics
+/// min/max.
+///
+/// See reference in
+/// <https://github.com/apache/parquet-cpp/blob/master/src/parquet/types.h>
+#[derive(Debug, Clone, Copy, PartialEq, Eq)]
+#[cfg_attr(feature = "serde_types", derive(Deserialize, Serialize))]
+pub enum SortOrder {
+    /// Signed (either value or legacy byte-wise) comparison.
+    Signed,
+    /// Unsigned (depending on physical type either value or byte-wise) comparison.
+    Unsigned,
+    /// Comparison is undefined.
+    Undefined,
+}
+
+/// Returns sort order for a physical/logical type.
+pub fn get_sort_order(
+    logical_type: &Option<PrimitiveLogicalType>,
+    converted_type: &Option<PrimitiveConvertedType>,

----------------------------------------

File: core/rust/qdbr/parquet2/src/page/mod.rs
Status: added
Changes: +431 -0
Diff:
@@ -0,0 +1,431 @@
+use std::sync::Arc;
+
+pub use crate::thrift_format::{
+    DataPageHeader as DataPageHeaderV1, DataPageHeaderV2, PageHeader as ParquetPageHeader,
+};
+
+use crate::indexes::Interval;
+pub use crate::parquet_bridge::{DataPageHeaderExt, PageType};
+
+use crate::compression::Compression;
+use crate::encoding::{get_length, Encoding};
+use crate::error::{Error, Result};
+use crate::metadata::Descriptor;
+
+use crate::statistics::{deserialize_statistics, Statistics};
+
+/// A [`CompressedDataPage`] is compressed, encoded representation of a Parquet data page.
+/// It holds actual data and thus cloning it is expensive.
+#[derive(Debug)]
+pub struct CompressedDataPage {
+    pub(crate) header: DataPageHeader,
+    pub(crate) buffer: Vec<u8>,
+    pub(crate) compression: Compression,
+    uncompressed_page_size: usize,
+    pub(crate) descriptor: Descriptor,
+
+    // The offset and length in rows
+    pub(crate) selected_rows: Option<Vec<Interval>>,
+}
+
+impl CompressedDataPage {
+    /// Returns a new [`CompressedDataPage`].
+    pub fn new(
+        header: DataPageHeader,
+        buffer: Vec<u8>,
+        compression: Compression,
+        uncompressed_page_size: usize,
+        descriptor: Descriptor,
+        rows: Option<usize>,
+    ) -> Self {
+        Self::new_read(
+            header,
+            buffer,
+            compression,
+            uncompressed_page_size,
+            descriptor,
+            rows.map(|x| vec![Interval::new(0, x)]),
+        )
+    }
+
+    /// Returns a new [`CompressedDataPage`].
+    pub(crate) fn new_read(
+        header: DataPageHeader,
+        buffer: Vec<u8>,
+        compression: Compression,
+        uncompressed_page_size: usize,
+        descriptor: Descriptor,
+        selected_rows: Option<Vec<Interval>>,
+    ) -> Self {
+        Self {
+            header,
+            buffer,
+            compression,
+            uncompressed_page_size,
+            descriptor,
+            selected_rows,
+        }
+    }
+
+    pub fn header(&self) -> &DataPageHeader {
+        &self.header
+    }
+
+    pub fn uncompressed_size(&self) -> usize {
+        self.uncompressed_page_size
+    }
+
+    pub fn compressed_size(&self) -> usize {
+        self.buffer.len()
+    }
+
+    /// The compression of the data in this page.
+    /// Note that what is compressed in a page depends on its version:
+    /// in V1, the whole data (`[repetition levels][definition levels][values]`) is compressed; in V2 only the values are compressed.
+    pub fn compression(&self) -> Compression {
+        self.compression
+    }
+
+    /// the rows to be selected by this page.
+    /// When `None`, all rows are to be considered.
+    pub fn selected_rows(&self) -> Option<&[Interval]> {
+        self.selected_rows.as_deref()
+    }
+
+    pub fn num_values(&self) -> usize {
+        self.header.num_values()
+    }
+
+    /// Decodes the raw statistics into a statistics
+    pub fn statistics(&self) -> Option<Result<Arc<dyn Statistics>>> {
+        match &self.header {
+            DataPageHeader::V1(d) => d
+                .statistics
+                .as_ref()
+                .map(|x| deserialize_statistics(x, self.descriptor.primitive_type.clone())),
+            DataPageHeader::V2(d) => d
+                .statistics
+                .as_ref()
+                .map(|x| deserialize_statistics(x, self.descriptor.primitive_type.clone())),
+        }
+    }
+
+    #[inline]
+    pub fn select_rows(&mut self, selected_rows: Vec<Interval>) {
+        self.selected_rows = Some(selected_rows);
+    }
+}
+
+#[derive(Debug, Clone)]
+pub enum DataPageHeader {
+    V1(DataPageHeaderV1),
+    V2(DataPageHeaderV2),
+}
+
+impl DataPageHeader {
+    pub fn num_values(&self) -> usize {
+        match &self {
+            DataPageHeader::V1(d) => d.num_values as usize,
+            DataPageHeader::V2(d) => d.num_values as usize,
+        }
+    }
+}
+
+/// A [`DataPage`] is an uncompressed, encoded representation of a Parquet data page. It holds actual data
+/// and thus cloning it is expensive.
+#[derive(Debug, Clone)]
+pub struct DataPage {
+    pub(super) header: DataPageHeader,
+    pub(super) buffer: Vec<u8>,
+    pub descriptor: Descriptor,
+    pub selected_rows: Option<Vec<Interval>>,
+}
+

----------------------------------------

File: core/rust/qdbr/parquet2/src/parquet_bridge.rs
Status: added
Changes: +704 -0
Diff:
@@ -0,0 +1,704 @@
+// Bridges structs from thrift-generated code to rust enums.
+use std::convert::TryFrom;
+
+use super::thrift_format::{
+    BoundaryOrder as ParquetBoundaryOrder, CompressionCodec, DataPageHeader, DataPageHeaderV2,
+    DecimalType, Encoding as ParquetEncoding, FieldRepetitionType, IntType,
+    LogicalType as ParquetLogicalType, PageType as ParquetPageType, TimeType,
+    TimeUnit as ParquetTimeUnit, TimestampType,
+};
+
+use crate::error::Error;
+#[cfg(feature = "serde_types")]
+use serde::{Deserialize, Serialize};
+
+/// The repetition of a parquet field
+#[derive(Debug, Eq, PartialEq, Hash, Clone, Copy)]
+#[cfg_attr(feature = "serde_types", derive(Deserialize, Serialize))]
+pub enum Repetition {
+    /// When the field has no null values
+    Required,
+    /// When the field may have null values
+    Optional,
+    /// When the field may be repeated (list field)
+    Repeated,
+}
+
+impl TryFrom<FieldRepetitionType> for Repetition {
+    type Error = Error;
+
+    fn try_from(repetition: FieldRepetitionType) -> Result<Self, Self::Error> {
+        Ok(match repetition {
+            FieldRepetitionType::REQUIRED => Repetition::Required,
+            FieldRepetitionType::OPTIONAL => Repetition::Optional,
+            FieldRepetitionType::REPEATED => Repetition::Repeated,
+            _ => return Err(Error::oos("Thrift out of range")),
+        })
+    }
+}
+
+impl From<Repetition> for FieldRepetitionType {
+    fn from(repetition: Repetition) -> Self {
+        match repetition {
+            Repetition::Required => FieldRepetitionType::REQUIRED,
+            Repetition::Optional => FieldRepetitionType::OPTIONAL,
+            Repetition::Repeated => FieldRepetitionType::REPEATED,
+        }
+    }
+}
+
+#[derive(Debug, Eq, PartialEq, Hash, Clone, Copy)]
+#[cfg_attr(feature = "serde_types", derive(Deserialize, Serialize))]
+pub enum Compression {
+    Uncompressed,
+    Snappy,
+    Gzip,
+    Lzo,
+    Brotli,
+    Lz4,
+    Zstd,
+    Lz4Raw,
+}
+
+impl TryFrom<CompressionCodec> for Compression {
+    type Error = Error;
+
+    fn try_from(codec: CompressionCodec) -> Result<Self, Self::Error> {
+        Ok(match codec {
+            CompressionCodec::UNCOMPRESSED => Compression::Uncompressed,
+            CompressionCodec::SNAPPY => Compression::Snappy,
+            CompressionCodec::GZIP => Compression::Gzip,
+            CompressionCodec::LZO => Compression::Lzo,
+            CompressionCodec::BROTLI => Compression::Brotli,
+            CompressionCodec::LZ4 => Compression::Lz4,
+            CompressionCodec::ZSTD => Compression::Zstd,
+            CompressionCodec::LZ4_RAW => Compression::Lz4Raw,
+            _ => return Err(Error::oos("Thrift out of range")),
+        })
+    }
+}
+
+impl From<Compression> for CompressionCodec {
+    fn from(codec: Compression) -> Self {
+        match codec {
+            Compression::Uncompressed => CompressionCodec::UNCOMPRESSED,
+            Compression::Snappy => CompressionCodec::SNAPPY,
+            Compression::Gzip => CompressionCodec::GZIP,
+            Compression::Lzo => CompressionCodec::LZO,
+            Compression::Brotli => CompressionCodec::BROTLI,
+            Compression::Lz4 => CompressionCodec::LZ4,
+            Compression::Zstd => CompressionCodec::ZSTD,
+            Compression::Lz4Raw => CompressionCodec::LZ4_RAW,
+        }
+    }
+}
+
+/// Defines the compression settings for writing a parquet file.
+///
+/// If None is provided as a compression setting, then the default compression level is used.
+#[derive(Debug, Eq, PartialEq, Hash, Clone, Copy)]
+pub enum CompressionOptions {
+    Uncompressed,
+    Snappy,
+    Gzip(Option<GzipLevel>),
+    Lzo,
+    Brotli(Option<BrotliLevel>),
+    Lz4,
+    Zstd(Option<ZstdLevel>),
+    Lz4Raw,
+}
+
+impl From<CompressionOptions> for Compression {
+    fn from(value: CompressionOptions) -> Self {
+        match value {
+            CompressionOptions::Uncompressed => Compression::Uncompressed,
+            CompressionOptions::Snappy => Compression::Snappy,
+            CompressionOptions::Gzip(_) => Compression::Gzip,
+            CompressionOptions::Lzo => Compression::Lzo,
+            CompressionOptions::Brotli(_) => Compression::Brotli,
+            CompressionOptions::Lz4 => Compression::Lz4,
+            CompressionOptions::Zstd(_) => Compression::Zstd,
+            CompressionOptions::Lz4Raw => Compression::Lz4Raw,
+        }
+    }
+}
+
+impl From<CompressionOptions> for CompressionCodec {
+    fn from(codec: CompressionOptions) -> Self {
+        match codec {
+            CompressionOptions::Uncompressed => CompressionCodec::UNCOMPRESSED,
+            CompressionOptions::Snappy => CompressionCodec::SNAPPY,
+            CompressionOptions::Gzip(_) => CompressionCodec::GZIP,
+            CompressionOptions::Lzo => CompressionCodec::LZO,
+            CompressionOptions::Brotli(_) => CompressionCodec::BROTLI,
+            CompressionOptions::Lz4 => CompressionCodec::LZ4,
+            CompressionOptions::Zstd(_) => CompressionCodec::ZSTD,
+            CompressionOptions::Lz4Raw => CompressionCodec::LZ4_RAW,
+        }
+    }
+}
+
+/// Defines valid compression levels.
+pub(crate) trait CompressionLevel<T: std::fmt::Display + std::cmp::PartialOrd> {
+    const MINIMUM_LEVEL: T;
+    const MAXIMUM_LEVEL: T;
+
+    /// Tests if the provided compression level is valid.
+    fn is_valid_level(level: T) -> Result<(), Error> {
+        let compression_range = Self::MINIMUM_LEVEL..=Self::MAXIMUM_LEVEL;
+        if compression_range.contains(&level) {
+            Ok(())
+        } else {
+            Err(Error::InvalidParameter(format!(
+                "valid compression range {}..={} exceeded.",
+                compression_range.start(),
+                compression_range.end()
+            )))
+        }
+    }
+}
+
+/// Represents a valid brotli compression level.
+#[derive(Debug, Eq, PartialEq, Hash, Clone, Copy)]
+pub struct BrotliLevel(u32);
+
+impl Default for BrotliLevel {
+    fn default() -> Self {
+        Self(1)
+    }
+}
+
+impl CompressionLevel<u32> for BrotliLevel {
+    const MINIMUM_LEVEL: u32 = 0;
+    const MAXIMUM_LEVEL: u32 = 11;
+}
+
+impl BrotliLevel {
+    /// Attempts to create a brotli compression level.
+    ///
+    /// Compression levels must be valid.
+    pub fn try_new(level: u32) -> Result<Self, Error> {
+        Self::is_valid_level(level).map(|_| Self(level))
+    }
+
+    /// Returns the compression level.
+    pub fn compression_level(&self) -> u32 {
+        self.0
+    }
+}
+
+/// Represents a valid gzip compression level.
+#[derive(Debug, Eq, PartialEq, Hash, Clone, Copy)]
+pub struct GzipLevel(u8);
+
+impl Default for GzipLevel {
+    fn default() -> Self {
+        // The default as of miniz_oxide 0.5.1 is 6 for compression level
+        // (miniz_oxide::deflate::CompressionLevel::DefaultLevel)
+        Self(6)
+    }
+}
+
+impl CompressionLevel<u8> for GzipLevel {
+    const MINIMUM_LEVEL: u8 = 0;
+    const MAXIMUM_LEVEL: u8 = 10;
+}
+
+impl GzipLevel {
+    /// Attempts to create a gzip compression level.
+    ///
+    /// Compression levels must be valid (i.e. be acceptable for [`flate2::Compression`]).
+    pub fn try_new(level: u8) -> Result<Self, Error> {
+        Self::is_valid_level(level).map(|_| Self(level))
+    }
+
+    /// Returns the compression level.
+    pub fn compression_level(&self) -> u8 {
+        self.0
+    }
+}
+
+#[cfg(feature = "gzip")]
+impl From<GzipLevel> for flate2::Compression {
+    fn from(level: GzipLevel) -> Self {
+        Self::new(level.compression_level() as u32)
+    }
+}
+
+/// Represents a valid zstd compression level.
+#[derive(Debug, Eq, PartialEq, Hash, Clone, Copy)]
+pub struct ZstdLevel(i32);
+
+impl CompressionLevel<i32> for ZstdLevel {
+    // zstd binds to C, and hence zstd::compression_level_range() is not const as this calls the
+    // underlying C library.

----------------------------------------

File: core/rust/qdbr/parquet2/src/read/column/mod.rs
Status: added
Changes: +205 -0
Diff:
@@ -0,0 +1,205 @@
+use std::io::{Read, Seek};
+use std::vec::IntoIter;
+
+use crate::error::Error;
+use crate::metadata::{ColumnChunkMetaData, RowGroupMetaData};
+use crate::page::CompressedPage;
+use crate::schema::types::ParquetType;
+
+use super::{get_field_columns, get_page_iterator, PageFilter, PageReader};
+
+#[cfg(feature = "async")]
+#[cfg_attr(docsrs, doc(cfg(feature = "async")))]
+mod stream;
+
+/// Returns a [`ColumnIterator`] of column chunks corresponding to `field`.
+///
+/// Contrarily to [`get_page_iterator`] that returns a single iterator of pages, this iterator
+/// iterates over columns, one by one, and returns a [`PageReader`] per column.
+/// For primitive fields (e.g. `i64`), [`ColumnIterator`] yields exactly one column.
+/// For complex fields, it yields multiple columns.
+/// `max_page_size` is the maximum number of bytes allowed.
+pub fn get_column_iterator<R: Read + Seek>(
+    reader: R,
+    row_group: &RowGroupMetaData,
+    field_name: &str,
+    page_filter: Option<PageFilter>,
+    scratch: Vec<u8>,
+    max_page_size: usize,
+) -> ColumnIterator<R> {
+    let columns = get_field_columns(row_group.columns(), field_name)
+        .cloned()
+        .collect::<Vec<_>>();
+
+    ColumnIterator::new(reader, columns, page_filter, scratch, max_page_size)
+}
+
+/// State of [`MutStreamingIterator`].
+#[derive(Debug)]
+pub enum State<T> {
+    /// Iterator still has elements
+    Some(T),
+    /// Iterator finished
+    Finished(Vec<u8>),
+}
+
+/// A special kind of fallible streaming iterator where `advance` consumes the iterator.
+pub trait MutStreamingIterator: Sized {
+    type Item;
+    type Error;
+
+    fn advance(self) -> std::result::Result<State<Self>, Self::Error>;
+    fn get(&mut self) -> Option<&mut Self::Item>;
+}
+
+/// A [`MutStreamingIterator`] that reads column chunks one by one,
+/// returning a [`PageReader`] per column.
+pub struct ColumnIterator<R: Read + Seek> {
+    reader: Option<R>,
+    columns: Vec<ColumnChunkMetaData>,
+    page_filter: Option<PageFilter>,
+    current: Option<(PageReader<R>, ColumnChunkMetaData)>,
+    scratch: Vec<u8>,
+    max_page_size: usize,
+}
+
+impl<R: Read + Seek> ColumnIterator<R> {
+    /// Returns a new [`ColumnIterator`]

----------------------------------------

File: core/rust/qdbr/parquet2/src/read/column/stream.rs
Status: added
Changes: +51 -0
Diff:
@@ -0,0 +1,51 @@
+use futures::future::{try_join_all, BoxFuture};
+use futures::{AsyncRead, AsyncReadExt, AsyncSeek, AsyncSeekExt};
+
+use crate::error::Error;
+use crate::metadata::ColumnChunkMetaData;
+use crate::read::get_field_columns;
+
+/// Reads a single column chunk into memory asynchronously
+pub async fn read_column_async<'b, R, F>(
+    factory: F,
+    meta: &ColumnChunkMetaData,
+) -> Result<Vec<u8>, Error>
+where
+    R: AsyncRead + AsyncSeek + Send + Unpin,
+    F: Fn() -> BoxFuture<'b, std::io::Result<R>>,
+{

----------------------------------------

File: core/rust/qdbr/parquet2/src/read/compression.rs
Status: added
Changes: +287 -0
Diff:
@@ -0,0 +1,287 @@
+use parquet_format_safe::DataPageHeaderV2;
+use streaming_decompression;
+
+use crate::compression::{self, Compression};
+use crate::error::{Error, Result};
+use crate::page::{CompressedPage, DataPage, DataPageHeader, DictPage, Page};
+use crate::FallibleStreamingIterator;
+
+use super::page::PageIterator;
+
+fn decompress_v1(compressed: &[u8], compression: Compression, buffer: &mut [u8]) -> Result<()> {
+    compression::decompress(compression, compressed, buffer)
+}
+
+fn decompress_v2(
+    compressed: &[u8],
+    page_header: &DataPageHeaderV2,
+    compression: Compression,
+    buffer: &mut [u8],
+) -> Result<()> {
+    // When processing data page v2, depending on enabled compression for the
+    // page, we should account for uncompressed data ('offset') of
+    // repetition and definition levels.
+    //
+    // We always use 0 offset for other pages other than v2, `true` flag means
+    // that compression will be applied if decompressor is defined
+    let offset = (page_header.definition_levels_byte_length
+        + page_header.repetition_levels_byte_length) as usize;
+    // When is_compressed flag is missing the page is considered compressed
+    let can_decompress = page_header.is_compressed.unwrap_or(true);
+
+    if can_decompress {
+        if offset > buffer.len() || offset > compressed.len() {
+            return Err(Error::OutOfSpec(
+                "V2 Page Header reported incorrect offset to compressed data".to_string(),
+            ));
+        }
+
+        (buffer[..offset]).copy_from_slice(&compressed[..offset]);
+
+        compression::decompress(compression, &compressed[offset..], &mut buffer[offset..])?;
+    } else {
+        if buffer.len() != compressed.len() {
+            return Err(Error::OutOfSpec(
+                "V2 Page Header reported incorrect decompressed size".to_string(),
+            ));
+        }
+        buffer.copy_from_slice(compressed);
+    }
+    Ok(())
+}
+
+/// decompresses a [`CompressedDataPage`] into `buffer`.
+/// If the page is un-compressed, `buffer` is swapped instead.
+/// Returns whether the page was decompressed.
+pub fn decompress_buffer(
+    compressed_page: &mut CompressedPage,
+    buffer: &mut Vec<u8>,
+) -> Result<bool> {
+    if compressed_page.compression() != Compression::Uncompressed {
+        // prepare the compression buffer
+        let read_size = compressed_page.uncompressed_size();
+
+        if read_size > buffer.capacity() {
+            // dealloc and ignore region, replacing it by a new region.
+            // This won't reallocate - it frees and calls `alloc_zeroed`
+            *buffer = vec![0; read_size];
+        } else if read_size > buffer.len() {
+            // fill what we need with zeros so that we can use them in `Read`.
+            // This won't reallocate
+            buffer.resize(read_size, 0);
+        } else {
+            buffer.truncate(read_size);
+        }
+        match compressed_page {
+            CompressedPage::Data(compressed_page) => match compressed_page.header() {
+                DataPageHeader::V1(_) => {
+                    decompress_v1(&compressed_page.buffer, compressed_page.compression, buffer)?
+                }
+                DataPageHeader::V2(header) => decompress_v2(
+                    &compressed_page.buffer,
+                    header,
+                    compressed_page.compression,
+                    buffer,
+                )?,
+            },
+            CompressedPage::Dict(page) => decompress_v1(&page.buffer, page.compression(), buffer)?,
+        }
+        Ok(true)
+    } else {
+        // page.buffer is already decompressed => swap it with `buffer`, making `page.buffer` the
+        // decompression buffer and `buffer` the decompressed buffer
+        std::mem::swap(compressed_page.buffer(), buffer);
+        Ok(false)
+    }

----------------------------------------

File: core/rust/qdbr/parquet2/src/read/indexes/deserialize.rs
Status: added
Changes: +27 -0
Diff:
@@ -0,0 +1,27 @@
+use parquet_format_safe::{thrift::protocol::TCompactInputProtocol, ColumnIndex};
+
+use crate::error::Error;
+use crate::schema::types::{PhysicalType, PrimitiveType};
+
+use crate::indexes::{BooleanIndex, ByteIndex, FixedLenByteIndex, Index, NativeIndex};
+
+pub fn deserialize(data: &[u8], primitive_type: PrimitiveType) -> Result<Box<dyn Index>, Error> {
+    let mut prot = TCompactInputProtocol::new(data, data.len() * 2 + 1024);

----------------------------------------

File: core/rust/qdbr/parquet2/src/read/indexes/mod.rs
Status: added
Changes: +4 -0
Diff:
@@ -0,0 +1,4 @@
+mod deserialize;
+mod read;
+
+pub use read::*;

----------------------------------------

File: core/rust/qdbr/parquet2/src/read/indexes/read.rs
Status: added
Changes: +132 -0
Diff:
@@ -0,0 +1,132 @@
+use std::convert::TryInto;
+use std::io::{Cursor, Read, Seek, SeekFrom};
+
+use parquet_format_safe::ColumnChunk;
+use parquet_format_safe::{thrift::protocol::TCompactInputProtocol, OffsetIndex, PageLocation};
+
+use crate::error::Error;
+use crate::indexes::Index;
+use crate::metadata::ColumnChunkMetaData;
+
+use super::deserialize::deserialize;
+
+fn prepare_read<F: Fn(&ColumnChunk) -> Option<i64>, G: Fn(&ColumnChunk) -> Option<i32>>(
+    chunks: &[ColumnChunkMetaData],
+    get_offset: F,
+    get_length: G,
+) -> Result<(u64, Vec<usize>), Error> {
+    // c1: [start, length]
+    // ...
+    // cN: [start, length]
+
+    let first_chunk = if let Some(chunk) = chunks.first() {
+        chunk
+    } else {
+        return Ok((0, vec![]));
+    };
+    let metadata = first_chunk.column_chunk();
+
+    let offset: u64 = if let Some(offset) = get_offset(metadata) {
+        offset.try_into()?
+    } else {
+        return Ok((0, vec![]));
+    };
+
+    let lengths = chunks
+        .iter()
+        .map(|x| get_length(x.column_chunk()))
+        .map(|maybe_length| {
+            let index_length = maybe_length.ok_or_else(|| {
+                Error::oos("The column length must exist if column offset exists")
+            })?;
+
+            Ok(index_length.try_into()?)

----------------------------------------

File: core/rust/qdbr/parquet2/src/read/levels.rs
Status: added
Changes: +27 -0
Diff:
@@ -0,0 +1,27 @@
+/// Returns the number of bits needed to store the given maximum definition or repetition level.
+#[inline]
+pub fn get_bit_width(max_level: i16) -> u32 {
+    16 - max_level.leading_zeros()
+}
+
+#[cfg(test)]
+mod tests {
+    use super::get_bit_width;

----------------------------------------

File: core/rust/qdbr/parquet2/src/read/metadata.rs
Status: added
Changes: +116 -0
Diff:
@@ -0,0 +1,116 @@
+use std::convert::TryInto;
+use std::{
+    cmp::min,
+    io::{Read, Seek, SeekFrom},
+};
+
+use parquet_format_safe::thrift::protocol::TCompactInputProtocol;
+use parquet_format_safe::FileMetaData as TFileMetaData;
+
+use super::super::{
+    metadata::FileMetaData, DEFAULT_FOOTER_READ_SIZE, FOOTER_SIZE, HEADER_SIZE, PARQUET_MAGIC,
+};
+
+use crate::error::{Error, Result};
+
+pub(super) fn metadata_len(buffer: &[u8], len: usize) -> i32 {
+    i32::from_le_bytes(buffer[len - 8..len - 4].try_into().unwrap())
+}
+
+// see (unstable) Seek::stream_len
+fn stream_len(seek: &mut impl Seek) -> std::result::Result<u64, std::io::Error> {
+    let old_pos = seek.stream_position()?;
+    let len = seek.seek(SeekFrom::End(0))?;
+
+    // Avoid seeking a third time when we were already at the end of the
+    // stream. The branch is usually way cheaper than a seek operation.
+    if old_pos != len {
+        seek.seek(SeekFrom::Start(old_pos))?;
+    }
+
+    Ok(len)
+}
+
+/// Reads a [`FileMetaData`] from the reader, located at the end of the file.
+pub fn read_metadata<R: Read + Seek>(reader: &mut R) -> Result<FileMetaData> {
+    // check file is large enough to hold footer
+    let file_size = stream_len(reader)?;
+    read_metadata_with_size(reader, file_size)

----------------------------------------

File: core/rust/qdbr/parquet2/src/read/mod.rs
Status: added
Changes: +243 -0
Diff:
@@ -0,0 +1,243 @@
+mod column;
+mod compression;
+mod indexes;
+pub mod levels;
+mod metadata;
+mod page;
+#[cfg(feature = "async")]
+mod stream;
+
+use std::io::{Read, Seek, SeekFrom};
+use std::sync::Arc;
+
+pub use column::*;
+pub use compression::{decompress, BasicDecompressor, Decompressor};
+pub use metadata::{deserialize_metadata, read_metadata, read_metadata_with_size};
+#[cfg(feature = "async")]
+#[cfg_attr(docsrs, doc(cfg(feature = "async")))]
+pub use page::{get_page_stream, get_page_stream_from_column_start};
+pub use page::{IndexedPageReader, PageFilter, PageIterator, PageMetaData, PageReader};
+
+#[cfg(feature = "async")]
+#[cfg_attr(docsrs, doc(cfg(feature = "async")))]
+pub use stream::read_metadata as read_metadata_async;
+
+use crate::metadata::{ColumnChunkMetaData, RowGroupMetaData};
+use crate::{error::Result, metadata::FileMetaData};
+
+pub use indexes::{read_columns_indexes, read_pages_locations};
+
+/// Filters row group metadata to only those row groups,
+/// for which the predicate function returns true
+pub fn filter_row_groups(
+    metadata: &FileMetaData,
+    predicate: &dyn Fn(&RowGroupMetaData, usize) -> bool,
+) -> FileMetaData {
+    let mut filtered_row_groups = Vec::<RowGroupMetaData>::new();
+    for (i, row_group_metadata) in metadata.row_groups.iter().enumerate() {
+        if predicate(row_group_metadata, i) {
+            filtered_row_groups.push(row_group_metadata.clone());
+        }
+    }
+    let mut metadata = metadata.clone();
+    metadata.row_groups = filtered_row_groups;
+    metadata
+}
+
+/// Returns a new [`PageReader`] by seeking `reader` to the beginning of `column_chunk`.
+pub fn get_page_iterator<R: Read + Seek>(
+    column_chunk: &ColumnChunkMetaData,
+    mut reader: R,
+    pages_filter: Option<PageFilter>,
+    scratch: Vec<u8>,
+    max_page_size: usize,
+) -> Result<PageReader<R>> {
+    let pages_filter = pages_filter.unwrap_or_else(|| Arc::new(|_, _| true));
+
+    let (col_start, _) = column_chunk.byte_range();
+    reader.seek(SeekFrom::Start(col_start))?;
+    Ok(PageReader::new(
+        reader,
+        column_chunk,
+        pages_filter,
+        scratch,
+        max_page_size,
+    ))
+}
+
+/// Returns all [`ColumnChunkMetaData`] associated to `field_name`.
+/// For non-nested types, this returns an iterator with a single column
+pub fn get_field_columns<'a>(
+    columns: &'a [ColumnChunkMetaData],
+    field_name: &'a str,
+) -> impl Iterator<Item = &'a ColumnChunkMetaData> {
+    columns
+        .iter()
+        .filter(move |x| x.descriptor().path_in_schema[0] == field_name)
+}
+
+#[cfg(test)]
+mod tests {

----------------------------------------

File: core/rust/qdbr/parquet2/src/read/page/indexed_reader.rs
Status: added
Changes: +209 -0
Diff:
@@ -0,0 +1,209 @@
+use std::{
+    collections::VecDeque,
+    io::{Cursor, Read, Seek, SeekFrom},
+};
+
+use crate::{
+    error::Error,
+    indexes::{FilteredPage, Interval},
+    metadata::{ColumnChunkMetaData, Descriptor},
+    page::{CompressedDictPage, CompressedPage, ParquetPageHeader},
+    parquet_bridge::Compression,
+};
+
+use super::reader::{finish_page, read_page_header, PageMetaData};
+
+#[derive(Debug, Clone, Copy)]
+enum State {
+    MaybeDict,
+    Data,
+}
+
+/// A fallible [`Iterator`] of [`CompressedPage`]. This iterator leverages page indexes
+/// to skip pages that are not needed. Consequently, the pages from this
+/// iterator always have [`Some`] [`crate::page::CompressedDataPage::selected_rows()`]
+pub struct IndexedPageReader<R: Read + Seek> {
+    // The source
+    reader: R,
+
+    column_start: u64,
+    compression: Compression,
+
+    // used to deserialize dictionary pages and attach the descriptor to every read page
+    descriptor: Descriptor,
+
+    // buffer to read the whole page [header][data] into memory
+    buffer: Vec<u8>,
+
+    // buffer to store the data [data] and re-use across pages
+    data_buffer: Vec<u8>,
+
+    pages: VecDeque<FilteredPage>,
+
+    state: State,
+}
+
+fn read_page<R: Read + Seek>(
+    reader: &mut R,
+    start: u64,
+    length: usize,
+    buffer: &mut Vec<u8>,
+    data: &mut Vec<u8>,
+) -> Result<ParquetPageHeader, Error> {
+    // seek to the page
+    reader.seek(SeekFrom::Start(start))?;
+
+    // read [header][data] to buffer
+    buffer.clear();
+    buffer.try_reserve(length)?;
+    reader.by_ref().take(length as u64).read_to_end(buffer)?;
+
+    // deserialize [header]
+    let mut reader = Cursor::new(buffer);
+    let page_header = read_page_header(&mut reader, 1024 * 1024)?;
+    let header_size = reader.stream_position().unwrap() as usize;
+    let buffer = reader.into_inner();
+
+    // copy [data]
+    data.clear();
+    data.extend_from_slice(&buffer[header_size..]);

----------------------------------------

File: core/rust/qdbr/parquet2/src/read/page/mod.rs
Status: added
Changes: +17 -0
Diff:
@@ -0,0 +1,17 @@
+mod indexed_reader;
+mod reader;
+#[cfg(feature = "async")]
+mod stream;
+
+use crate::{error::Error, page::CompressedPage};
+
+pub use indexed_reader::IndexedPageReader;
+pub use reader::{PageFilter, PageMetaData, PageReader};

----------------------------------------

File: core/rust/qdbr/parquet2/src/read/page/reader.rs
Status: added
Changes: +307 -0
Diff:
@@ -0,0 +1,307 @@
+use std::convert::TryInto;
+use std::{io::Read, sync::Arc};
+
+use parquet_format_safe::thrift::protocol::TCompactInputProtocol;
+
+use crate::compression::Compression;
+use crate::error::{Error, Result};
+use crate::indexes::Interval;
+use crate::metadata::{ColumnChunkMetaData, Descriptor};
+
+use crate::page::{
+    CompressedDataPage, CompressedDictPage, CompressedPage, DataPageHeader, PageType,
+    ParquetPageHeader,
+};
+use crate::parquet_bridge::Encoding;
+
+use super::PageIterator;
+
+/// This meta is a small part of [`ColumnChunkMetaData`].
+#[derive(Debug, Clone, PartialEq, Eq)]
+pub struct PageMetaData {
+    /// The start offset of this column chunk in file.
+    pub column_start: u64,
+    /// The number of values in this column chunk.
+    pub num_values: i64,
+    /// Compression type
+    pub compression: Compression,
+    /// The descriptor of this parquet column
+    pub descriptor: Descriptor,
+}
+
+impl PageMetaData {
+    /// Returns a new [`PageMetaData`].
+    pub fn new(
+        column_start: u64,
+        num_values: i64,
+        compression: Compression,
+        descriptor: Descriptor,
+    ) -> Self {
+        Self {
+            column_start,
+            num_values,
+            compression,
+            descriptor,
+        }
+    }
+}
+
+impl From<&ColumnChunkMetaData> for PageMetaData {
+    fn from(column: &ColumnChunkMetaData) -> Self {
+        Self {
+            column_start: column.byte_range().0,
+            num_values: column.num_values(),
+            compression: column.compression(),
+            descriptor: column.descriptor().descriptor.clone(),
+        }
+    }
+}
+
+/// Type declaration for a page filter
+pub type PageFilter = Arc<dyn Fn(&Descriptor, &DataPageHeader) -> bool + Send + Sync>;
+
+/// A fallible [`Iterator`] of [`CompressedDataPage`]. This iterator reads pages back
+/// to back until all pages have been consumed.
+/// The pages from this iterator always have [`None`] [`crate::page::CompressedDataPage::selected_rows()`] since
+/// filter pushdown is not supported without a
+/// pre-computed [page index](https://github.com/apache/parquet-format/blob/master/PageIndex.md).
+pub struct PageReader<R: Read> {
+    // The source
+    reader: R,
+
+    compression: Compression,
+
+    // The number of values we have seen so far.
+    seen_num_values: i64,
+
+    // The number of total values in this column chunk.
+    total_num_values: i64,
+
+    pages_filter: PageFilter,
+
+    descriptor: Descriptor,
+
+    // The currently allocated buffer.
+    pub(crate) scratch: Vec<u8>,
+
+    // Maximum page size (compressed or uncompressed) to limit allocations
+    max_page_size: usize,
+}
+
+impl<R: Read> PageReader<R> {
+    /// Returns a new [`PageReader`].
+    ///
+    /// It assumes that the reader has been `seeked` to the beginning of `column`.
+    /// The parameter `max_header_size`
+    pub fn new(
+        reader: R,
+        column: &ColumnChunkMetaData,
+        pages_filter: PageFilter,
+        scratch: Vec<u8>,
+        max_page_size: usize,

----------------------------------------

File: core/rust/qdbr/parquet2/src/read/page/stream.rs
Status: added
Changes: +139 -0
Diff:
@@ -0,0 +1,139 @@
+use std::io::SeekFrom;
+
+use async_stream::try_stream;
+use futures::io::{copy, sink};
+use futures::{AsyncRead, AsyncReadExt, AsyncSeek, AsyncSeekExt, Stream};
+use parquet_format_safe::thrift::protocol::TCompactInputStreamProtocol;
+
+use crate::compression::Compression;
+use crate::error::{Error, Result};
+use crate::metadata::{ColumnChunkMetaData, Descriptor};
+use crate::page::{CompressedPage, ParquetPageHeader};
+
+use super::reader::{finish_page, get_page_header, PageMetaData};
+use super::PageFilter;
+
+/// Returns a stream of compressed data pages
+pub async fn get_page_stream<'a, RR: AsyncRead + Unpin + Send + AsyncSeek>(
+    column_metadata: &'a ColumnChunkMetaData,
+    reader: &'a mut RR,
+    scratch: Vec<u8>,
+    pages_filter: PageFilter,
+    max_page_size: usize,
+) -> Result<impl Stream<Item = Result<CompressedPage>> + 'a> {
+    get_page_stream_with_page_meta(
+        column_metadata.into(),
+        reader,
+        scratch,
+        pages_filter,
+        max_page_size,
+    )
+    .await
+}
+
+/// Returns a stream of compressed data pages from a reader that begins at the start of the column
+pub async fn get_page_stream_from_column_start<'a, R: AsyncRead + Unpin + Send>(
+    column_metadata: &'a ColumnChunkMetaData,
+    reader: &'a mut R,
+    scratch: Vec<u8>,
+    pages_filter: PageFilter,
+    max_header_size: usize,
+) -> Result<impl Stream<Item = Result<CompressedPage>> + 'a> {
+    let page_metadata: PageMetaData = column_metadata.into();
+    Ok(_get_page_stream(
+        reader,
+        page_metadata.num_values,

----------------------------------------

File: core/rust/qdbr/parquet2/src/read/stream.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/parquet2/src/schema/io_message/from_message.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/parquet2/src/schema/io_message/mod.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/parquet2/src/schema/io_thrift/from_thrift.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/parquet2/src/schema/io_thrift/mod.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/parquet2/src/schema/io_thrift/to_thrift.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/parquet2/src/schema/mod.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/parquet2/src/schema/types/basic_type.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/parquet2/src/schema/types/converted_type.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/parquet2/src/schema/types/mod.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/parquet2/src/schema/types/parquet_type.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/parquet2/src/schema/types/physical_type.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/parquet2/src/schema/types/spec.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/parquet2/src/statistics/binary.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/parquet2/src/statistics/boolean.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/parquet2/src/statistics/fixed_len_binary.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/parquet2/src/statistics/mod.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/parquet2/src/statistics/primitive.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/parquet2/src/types.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/parquet2/src/write/column_chunk.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/parquet2/src/write/compression.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/parquet2/src/write/dyn_iter.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/parquet2/src/write/file.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/parquet2/src/write/indexes/mod.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/parquet2/src/write/indexes/serialize.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/parquet2/src/write/indexes/write.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/parquet2/src/write/mod.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/parquet2/src/write/page.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/parquet2/src/write/row_group.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/parquet2/src/write/statistics.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/parquet2/src/write/stream.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/src/lib.rs
Status: modified
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/src/parquet_read/column_sink/fixed.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/src/parquet_read/column_sink/mod.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/src/parquet_read/column_sink/var.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/src/parquet_read/decode.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/src/parquet_read/jni.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/src/parquet_read/meta.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/src/parquet_read/mod.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/src/parquet_read/slicer/dict_decoder.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/src/parquet_read/slicer/dict_slicer.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/src/parquet_read/slicer/mod.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/src/parquet_read/slicer/rle.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/src/parquet_write/binary.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/src/parquet_write/boolean.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/src/parquet_write/file.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/src/parquet_write/fixed_len_bytes.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/src/parquet_write/jni.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/src/parquet_write/mod.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/src/parquet_write/primitive.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/src/parquet_write/schema.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/src/parquet_write/string.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/src/parquet_write/symbol.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/src/parquet_write/update.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/src/parquet_write/util.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/src/parquet_write/varchar.rs
Status: added
Changes: +0 -0

----------------------------------------

File: core/rust/qdbr/src/rustfmt.toml
Status: added
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/Bootstrap.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/DefaultBootstrapConfiguration.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/FactoryProviderFactory.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/KqueueFileWatcher.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/PropServerConfiguration.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/ServerMain.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/cairo/AlterTableUtils.java
Status: added
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/cairo/CairoConfiguration.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/cairo/CairoConfigurationWrapper.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/cairo/CairoEngine.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/cairo/CairoException.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/cairo/ColumnType.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/cairo/DatabaseSnapshotAgentImpl.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/cairo/DefaultCairoConfiguration.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/cairo/EmptySymbolMapReader.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/cairo/O3CopyJob.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/cairo/O3OpenColumnJob.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/cairo/O3PartitionJob.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/cairo/O3PartitionPurgeJob.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/cairo/SqlJitMode.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/cairo/SymbolMapReader.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/cairo/SymbolMapReaderImpl.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/cairo/TableReader.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/cairo/TableUtils.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/cairo/TableWriter.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/cairo/TxReader.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/cairo/TxWriter.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/cairo/VarcharTypeDriver.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/cairo/map/OrderedMapVarSizeRecord.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/cairo/map/ShardedMapCursor.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/cairo/map/UnorderedVarcharMap.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/cairo/sql/AsyncWriterCommand.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/cairo/sql/DataFrameCursor.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/cairo/sql/DelegatingRecord.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/cairo/sql/Function.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/cairo/sql/PageFrame.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/cairo/sql/PageFrameCursor.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/cairo/sql/Record.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/cairo/sql/ScalarFunction.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/cairo/sql/VirtualRecord.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/cairo/vm/MemoryCMORImpl.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/cairo/vm/api/MemoryCR.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/cairo/wal/ApplyWal2TableJob.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/cairo/wal/CheckWalTransactionsJob.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/cairo/wal/MetadataService.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/cairo/wal/O3JobParallelismRegulator.java
Status: added
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/cairo/wal/WalWriter.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/cairo/wal/seq/MetadataServiceStub.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/cairo/wal/seq/SeqTxnTracker.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/cairo/wal/seq/SequencerMetadataService.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/cairo/wal/seq/TableSequencerImpl.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/cutlass/http/HttpAuthenticator.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/cutlass/pgwire/PGConnectionContext.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/BasePlanSink.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/ExpressionParser.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/FunctionParser.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/OperatorExpression.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/OperatorRegistry.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/PlanSink.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/SqlCodeGenerator.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/SqlCompilerImpl.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/SqlKeywords.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/SqlOptimiser.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/SqlParser.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/ExplainPlanFactory.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/QueryProgress.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/AbstractGeoHashFunction.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/BinFunction.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/BooleanFunction.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/ByteFunction.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/CharFunction.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/CursorFunction.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/DateFunction.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/DoubleFunction.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/FloatFunction.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/IPv4Function.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/IntFunction.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/Long128Function.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/Long256Function.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/LongFunction.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/RecordFunction.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/ShortFunction.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/StrFunction.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/SymbolFunction.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/TimestampFunction.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/UntypedFunction.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/UuidFunction.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/VarcharFunction.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/bin/Base64FunctionFactory.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/bind/IndexedParameterLinkFunction.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/bind/NamedParameterLinkFunction.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/bind/StrBindVariable.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/bind/VarcharBindVariable.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/cast/CastBooleanToStrFunctionFactory.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/cast/CastByteToStrFunctionFactory.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/cast/CastCharToStrFunctionFactory.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/cast/CastDateToStrFunctionFactory.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/cast/CastDoubleToStrFunctionFactory.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/cast/CastFloatToStrFunctionFactory.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/cast/CastGeoHashToGeoHashFunctionFactory.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/cast/CastIPv4ToStrFunctionFactory.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/cast/CastIntToStrFunctionFactory.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/cast/CastLongToStrFunctionFactory.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/cast/CastShortToStrFunctionFactory.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/cast/CastSymbolToStrFunctionFactory.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/cast/CastTimestampToStrFunctionFactory.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/cast/CastUuidToStrFunctionFactory.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/cast/CastVarcharToStrFunctionFactory.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/catalogue/CurrentSchemasFunctionFactory.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/catalogue/StringToStringArrayFunction.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/catalogue/TypeCatalogueCursor.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/catalogue/WalTableListFunctionFactory.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/columns/StrColumn.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/conditional/NullCaseFunction.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/conditional/StrCaseFunction.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/constants/NullConstant.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/constants/StrTypeConstant.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/date/ToStrDateFunctionFactory.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/date/ToStrTimestampFunctionFactory.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/eq/EqSymLongFunctionFactory.java
Status: added
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/eq/EqSymTimestampFunctionFactory.java
Status: added
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/eq/EqTimestampCursorFunctionFactory.java
Status: added
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/eq/EqVarcharFunctionFactory.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/groupby/CountDistinctStringGroupByFunction.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/groupby/CountDoubleGroupByFunctionFactory.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/groupby/CountFloatGroupByFunctionFactory.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/groupby/CountGeoHashGroupByFunctionFactory.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/groupby/CountIPv4GroupByFunctionFactory.java
Status: modified
Changes: +0 -0

----------------------------------------

File: core/src/main/java/io/questdb/griffin/engine/functions/groupby/CountIntGroupByFunctionFactory.java
Status: modified
Changes: +0 -0

----------------------------------------

