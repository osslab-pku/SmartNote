{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantitative quality evaluation of the generated RNs\n",
    "\n",
    "from config import init_env, settings\n",
    "\n",
    "init_env('evaluation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to attempt to detect AI-generated text [relatively] quickly via compression ratios\n",
    "# (C) 2023 Thinkst Applied Research, PTY\n",
    "# Author: Jacob Torrey <jacob@thinkst.com>\n",
    "\n",
    "import lzma\n",
    "from brotli import compress as brotli_compress, MODE_TEXT\n",
    "from numpy import array_split\n",
    "import re\n",
    "from abc import ABC, abstractmethod\n",
    "from enum import Enum\n",
    "import numpy as np\n",
    "from typing import List, Optional, Tuple, TypeAlias\n",
    "from importlib.resources import files\n",
    "\n",
    "def clean_text(s : str) -> str:\n",
    "    '''\n",
    "    Removes formatting and other non-content data that may skew compression ratios (e.g., duplicate spaces)\n",
    "    '''\n",
    "    # Remove extra spaces and duplicate newlines.\n",
    "    s = re.sub(' +', ' ', s)\n",
    "    s = re.sub('\\t', '', s)\n",
    "    s = re.sub('\\n+', '\\n', s)\n",
    "    s = re.sub('\\n ', '\\n', s)\n",
    "    s = re.sub(' \\n', '\\n', s)\n",
    "\n",
    "    # Remove non-alphanumeric chars\n",
    "    s = re.sub(r'[^0-9A-Za-z,\\.\\(\\) \\n]', '', s)\n",
    "\n",
    "    return s\n",
    "\n",
    "class LlmDetector:\n",
    "    CHUNK_SIZE = 1500\n",
    "    prelude_ratio = 0.0\n",
    "    \n",
    "    def _compress(self, s : str) -> float:\n",
    "        pass\n",
    "    \n",
    "    def score_text(self, sample: str) -> float | None:\n",
    "        '''\n",
    "        Returns a tuple of a string (AI or Human) and a float confidence (higher is more confident) that the sample was generated \n",
    "        by either an AI or human. Returns None if it cannot make a determination\n",
    "        '''\n",
    "        if self.prelude_ratio == 0.0:\n",
    "            return None\n",
    "        # clean sample\n",
    "        sample = clean_text(sample)\n",
    "        sample_score = self._compress(self.prelude_str + sample)\n",
    "        print(self.__class__.__name__ + ': ' + str((self.prelude_ratio, sample_score)))\n",
    "        return (self.prelude_ratio - sample_score) * 100\n",
    "\n",
    "    def score_long_text(self, sample: str) -> float | None:\n",
    "        '''\n",
    "        Returns a tuple of a string (AI or Human) and a float confidence (higher is more confident) that the sample was generated \n",
    "        by either an AI or human. Returns None if it cannot make a determination\n",
    "        '''\n",
    "        if self.prelude_ratio == 0.0:\n",
    "            return None\n",
    "        # clean sample\n",
    "        sample = clean_text(sample)\n",
    "        sample_scores = []\n",
    "        for chunk in [sample[i:i + self.CHUNK_SIZE] for i in range(0, len(sample), self.CHUNK_SIZE)]:\n",
    "            sample_scores.append(self._compress(self.prelude_str + chunk))\n",
    "        print(self.__class__.__name__ + ': ' + str((self.prelude_ratio, np.mean(sample_scores))))\n",
    "        return (self.prelude_ratio - np.mean(sample_scores)) * 100\n",
    "\n",
    "class BrotliLlmDetector(LlmDetector):\n",
    "    '''Class providing functionality to attempt to detect LLM/generative AI generated text using the brotli compression algorithm'''\n",
    "    def __init__(self, prelude_file : Optional[str] = None, prelude_str : Optional[str] = None, \\\n",
    "                 prelude_ratio : Optional[float] = None, preset : int = 8):\n",
    "        self.PRESET = preset\n",
    "        self.WIN_SIZE = 24\n",
    "        self.BLOCK_SIZE = 0\n",
    "        self.CHUNK_SIZE = 1500\n",
    "        self.prelude_ratio = 0.0\n",
    "        if prelude_ratio != None:\n",
    "            self.prelude_ratio = prelude_ratio\n",
    "        \n",
    "        if prelude_file != None:\n",
    "            with open(prelude_file, encoding='utf-8') as fp:\n",
    "                self.prelude_str = clean_text(fp.read())\n",
    "            self.prelude_ratio = self._compress(self.prelude_str)\n",
    "            return\n",
    "    \n",
    "        if prelude_str != None:\n",
    "            self.prelude_str = prelude_str\n",
    "            self.prelude_ratio = self._compress(self.prelude_str)\n",
    "\n",
    "    def _compress(self, s : str) -> float:\n",
    "        orig_len = len(s.encode())\n",
    "        c_len = len(brotli_compress(s.encode(), mode=MODE_TEXT, quality=self.PRESET, lgwin=self.WIN_SIZE, lgblock=self.BLOCK_SIZE))\n",
    "        return c_len / orig_len\n",
    "    \n",
    "class LzmaLlmDetector(LlmDetector):\n",
    "    '''Class providing functionality to attempt to detect LLM/generative AI generated text using the LZMA compression algorithm'''\n",
    "    def __init__(self, prelude_file : Optional[str] = None, prelude_str : Optional[str] = None, prelude_ratio : Optional[float] = None, preset : int = 4, normalize : bool = False) -> None:\n",
    "        '''Initializes a compression with the passed prelude file, and optionally the number of digits to round to compare prelude vs. sample compression'''\n",
    "        self.PRESET : int = preset\n",
    "        self.c_buf : List[bytes] = []\n",
    "        self.in_bytes : int = 0\n",
    "        self.prelude_ratio : float = 0.0\n",
    "        self.nf : float = 0.0\n",
    "\n",
    "        if prelude_ratio != None:\n",
    "            self.prelude_ratio = prelude_ratio\n",
    "\n",
    "        if prelude_file != None:\n",
    "            # Read it once to get the default compression ratio for the prelude\n",
    "            with open(prelude_file, 'r', encoding='utf-8') as fp:\n",
    "                self.prelude_str = fp.read()\n",
    "            self.prelude_ratio = self._compress(self.prelude_str)\n",
    "            return\n",
    "\n",
    "        if prelude_str != None:\n",
    "            self.prelude_str = prelude_str\n",
    "            if self.prelude_ratio == 0.0:\n",
    "                self.prelude_ratio = self._compress(prelude_str)\n",
    "        if normalize:\n",
    "            self.nf : float = self.prelude_ratio / len(self.prelude_str)\n",
    "\n",
    "    def _compress(self, s : str) -> float:\n",
    "        orig_len = len(s.encode())\n",
    "        c = lzma.LZMACompressor(preset=self.PRESET)\n",
    "        bytes = c.compress(s.encode())\n",
    "        bytes += c.flush()\n",
    "        c_len = len(bytes)\n",
    "        return c_len / orig_len\n",
    "\n",
    "\n",
    "det_br = BrotliLlmDetector('./ai-generated.txt')\n",
    "det_br.score_text('This is a test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_br.score_long_text('This is a test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_lzma = LzmaLlmDetector('./ai-generated.txt', normalize=True)\n",
    "det_lzma.score_text('This is a test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "import tiktoken\n",
    "\n",
    "@lru_cache\n",
    "def get_tokenizer(model_name: str) -> tiktoken:\n",
    "    return tiktoken.encoding_for_model(model_name)\n",
    "\n",
    "def count_tokens(text: str, model_name: str = \"gpt-4o\") -> int:\n",
    "    tokenizer = get_tokenizer(model_name)\n",
    "    return len(tokenizer.encode(text, disallowed_special=()))\n",
    "\n",
    "count_tokens('This is a test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log2\n",
    "\n",
    "def count_info_entropy(md: str):\n",
    "    \"\"\"\n",
    "    Count the information entropy of a markdown release note.\n",
    "    \"\"\"\n",
    "    lines = md.split('\\n')\n",
    "    _entries_cnt = []\n",
    "    _curr_cnt = 0\n",
    "    for l in lines:\n",
    "        if l.strip().startswith('#'):\n",
    "            _entries_cnt.append(_curr_cnt)\n",
    "            _curr_cnt = 0\n",
    "        elif l.strip().startswith('-') or l.strip().startswith('*'):\n",
    "            _curr_cnt += 1\n",
    "    _entries_cnt.append(_curr_cnt)\n",
    "    _sum = sum(_entries_cnt)\n",
    "    _ent = 0\n",
    "    for ent in _entries_cnt[1:]:\n",
    "        if ent == 0:\n",
    "            continue\n",
    "        _ent += (ent / _sum) * log2(ent / _sum)\n",
    "\n",
    "    # convert 0.0 to -0.0\n",
    "    if _ent == 0.0:\n",
    "        return 0.0\n",
    "    return -_ent\n",
    "\n",
    "\n",
    "a = \"\"\"# Title\n",
    "- Entry 1\n",
    "- Entry 2\n",
    "- Entry 3\n",
    "- Entry 4\n",
    "## Title 2\n",
    "- Entry 5\n",
    "- Entry 6\n",
    "# Title 3\n",
    "\"\"\"\n",
    "count_info_entropy(a)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from github import Github\n",
    "from tqdm import tqdm\n",
    "\n",
    "gh = Github(settings.GITHUB_TOKEN)\n",
    "\n",
    "@lru_cache\n",
    "def _get_commits_and_prs_between_versions(\n",
    "        name_with_owner: str,\n",
    "        from_version: str,\n",
    "        to_version: str,\n",
    "):\n",
    "    gh_repo = gh.get_repo(name_with_owner)\n",
    "    # get commits list\n",
    "    _commits = list(gh_repo.compare(from_version, to_version).commits)\n",
    "    _pr_to_commits = {}\n",
    "    \n",
    "    for c in tqdm(_commits):\n",
    "        for pr in gh_repo.get_commit(c.sha).get_pulls():\n",
    "            _pr_to_commits[pr.number] = _pr_to_commits.get(pr.number, []) + [c]\n",
    "    \n",
    "    return _pr_to_commits, _commits\n",
    "\n",
    "_get_commits_and_prs_between_versions('stakater/Reloader', 'v1.0.120', 'v1.0.121')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "SHA_REGEX = re.compile(r'([0-9a-f]{7,40})')\n",
    "PR_NUMBER_REGEX = re.compile(r'#(\\d+)')\n",
    "PR_URL_REGEX = re.compile(r'/pull/(\\d+)')\n",
    "\n",
    "def _match_commits_and_prs(\n",
    "        rn_md: str,\n",
    "):\n",
    "    _commits = SHA_REGEX.findall(rn_md)\n",
    "    _prs = PR_NUMBER_REGEX.findall(rn_md)\n",
    "    _pr_urls = PR_URL_REGEX.findall(rn_md)\n",
    "    return (\n",
    "        set(_c[:7] for _c in _commits),\n",
    "        set(int(p) for p in _prs) | set(int(p) for p in _pr_urls),\n",
    "    )\n",
    "\n",
    "_match_commits_and_prs(\n",
    "\"\"\"# Title\n",
    "- Entry 1 #24\n",
    "- Entry 2 https://github.com/stakater/Reloader/pull/25\n",
    "- Entry 3 https://github.com/stakater/Reloader/comit/1234ff67ff90\n",
    "- Entry 4 1234567890\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_commit_coverage(\n",
    "        name_with_owner: str,\n",
    "        from_version: str,\n",
    "        to_version: str,\n",
    "        rn_md: str,\n",
    "):\n",
    "    _pr_to_commits, _commits_to_hit = _get_commits_and_prs_between_versions(name_with_owner, from_version, to_version)\n",
    "    # replace all commits objects with short sha\n",
    "    _commits_to_hit = set(c.sha[:7] for c in _commits_to_hit)\n",
    "    _n_commits = len(_commits_to_hit)\n",
    "    _pr_to_commits = {k: [c.sha[:7] for c in v] for k, v in _pr_to_commits.items()}\n",
    "\n",
    "    print(\"PRs\", _pr_to_commits, \"Commits\", _commits_to_hit)\n",
    "\n",
    "    _commits_in_rn, _prs_in_rn = _match_commits_and_prs(rn_md)\n",
    "    for c in _commits_in_rn:\n",
    "        if c in _commits_to_hit:\n",
    "            _commits_to_hit.remove(c)\n",
    "    for pr in _prs_in_rn:\n",
    "        if pr in _pr_to_commits:\n",
    "            _commits_to_hit -= set(_pr_to_commits[pr])\n",
    "\n",
    "    return 1 - len(_commits_to_hit) / _n_commits\n",
    "\n",
    "calculate_commit_coverage('stakater/Reloader', 'v1.0.120', 'v1.0.121', \"\"\"# v1.0.121\n",
    "## 🔧 chore\n",
    "- Updated artifacts and changed resource field references in environment variables. [70ab566](https://github.com/stakater/Reloader/commit/70ab56606df1f9fd4877b0f615b0b929f8269511) <span style='color:grey;'>(significance=0.61)</span>\n",
    "- Fixed incorrect environment variables when enableHA is true. [#723](https://github.com/stakater/Reloader/pull/723) <span style='color:grey;'>(significance=0.57)</span>\n",
    "\"\"\")\n",
    "                     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "REGEX_NAME = re.compile(r'-- ([\\w-]+)/([\\w-]+)')\n",
    "REGEX_VERSION = re.compile(r'--previous-release\\s+(\\S+) --current-release\\s+(\\S+)')\n",
    "\n",
    "s = 'smartdraft.generator -- stakater/Reloader --previous-release v1.0.120 --current-release v1.0.121 '\n",
    "\n",
    "def parse_cli_args(s: str):\n",
    "    # yields an error if unmatched, should never happen\n",
    "    repo_name = REGEX_NAME.search(s).group(1) + '/' + REGEX_NAME.search(s).group(2)\n",
    "    from_version = REGEX_VERSION.search(s).group(1)\n",
    "    to_version = REGEX_VERSION.search(s).group(2)\n",
    "    return repo_name, from_version, to_version\n",
    "\n",
    "parse_cli_args(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat\n",
    "\n",
    "def calculate_readability(text: str) -> float:\n",
    "    return \n",
    "\n",
    "calculate_readability('This is a test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the tokenizer for token classification\n",
    "# cite: https://github.com/taidnguyen/software_entity_recognition\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"taidng/wikiser-bert-base\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"taidng/wikiser-bert-base\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, device=settings.TORCH_DEVICE)\n",
    "example = \"This release fixes a bug in the previous version.\"\n",
    "\n",
    "# count the number of tokens in the example\n",
    "_tokenized = tokenizer(example)\n",
    "print(len(_tokenized['input_ids']))\n",
    "\n",
    "ner_results = nlp(example)\n",
    "print(list(l['word'] for l in ner_results))\n",
    "def entity_density(text: str) -> int:\n",
    "    return len(nlp(text)) / len(tokenizer(text)['input_ids'])\n",
    "\n",
    "entity_density('This release fixes a bug in the previous version.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "out_path = 'Software_Tools/reloader'\n",
    "\n",
    "def eval_path(out_path):\n",
    "    _cli_file_path = os.path.join(out_path, 'details.txt')\n",
    "\n",
    "    repo_name, from_version, to_version = parse_cli_args(open(_cli_file_path).read())\n",
    "    repo_name, from_version, to_version\n",
    "\n",
    "    # list markdown files\n",
    "    _markdown_files = [f for f in os.listdir(out_path) if f.endswith('.md')]\n",
    "\n",
    "    results = []\n",
    "    for _f in _markdown_files:\n",
    "        print(f'Processing {_f}...')\n",
    "        _rn_md = open(os.path.join(out_path, _f)).read()\n",
    "        results.append({\n",
    "            'repo_name': repo_name,\n",
    "            'rn_name': _f.replace('.md', ''),\n",
    "            # 'commit_coverage': calculate_commit_coverage(repo_name, from_version, to_version, _rn_md),\n",
    "            # 'info_entropy': count_info_entropy(_rn_md),\n",
    "            # 'tokens_count': count_tokens(_rn_md),\n",
    "            # 'llm_brotli': det_br.score_text(_rn_md),\n",
    "            # 'llm_lzma': det_lzma.score_text(_rn_md),\n",
    "            # 'reading_ease': textstat.flesch_reading_ease(clean_text(_rn_md)),\n",
    "            'automated_readability_index': textstat.automated_readability_index(clean_text(_rn_md)),\n",
    "            'dale_chall_readability': textstat.dale_chall_readability_score(clean_text(_rn_md)),\n",
    "            # 'smog_readability': textstat.smog_index(clean_text(_rn_md)),\n",
    "            # 'coleman_liau_index': textstat.coleman_liau_index(clean_text(_rn_md)),\n",
    "            'entity_percent': entity_density(clean_text(_rn_md)) * 100,\n",
    "            'entity_count': len(nlp(clean_text(_rn_md))),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "eval_path(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "_l_ev = []\n",
    "for out_path in glob('*/*'):\n",
    "    if 'node_modules' in out_path:\n",
    "        continue\n",
    "    if not os.path.exists(os.path.join(out_path, 'details.txt')):\n",
    "        continue\n",
    "    print(f'Processing {out_path}...')\n",
    "    df_rn = eval_path(out_path)\n",
    "    df_rn['project_domain'] = out_path.split('/')[0]\n",
    "    df_rn.to_csv(os.path.join(out_path, 'quant_eval.csv'), index=False)\n",
    "    _l_ev.append(df_rn)\n",
    "\n",
    "df_ev = pd.concat(_l_ev)\n",
    "df_ev.to_csv('quant_eval.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's group by rn_name and project_domain and calculate the mean\n",
    "df_ev.groupby(['rn_name']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform to markdown\n",
    "\n",
    "print(df_ev.groupby(['rn_name']).mean().to_markdown())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
